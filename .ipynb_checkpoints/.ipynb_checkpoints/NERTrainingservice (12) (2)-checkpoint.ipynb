{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install --user -r Req_EEF_New.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import HTML\n",
    "# HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost\n"
     ]
    }
   ],
   "source": [
    "# added to use static image in model deployment \n",
    "import os\n",
    "os.environ['PYPI_PACKAGE_REPO'] = 'localhost'\n",
    "print(os.environ['PYPI_PACKAGE_REPO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending required paths\n",
    "import logging\n",
    "logging.basicConfig(filename='NER_Training_Service.log', filemode='w',format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'/notebooks/notebooks/EEF_NER_LSTM_Training')\n",
    "sys.path.insert(0,'/notebooks/notebooks/nltk_data')\n",
    "sys.path.insert(0,'/home/mosaic-ai/.local/bin')\n",
    "\n",
    "logging.info(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting only the application config utility\n",
    "from EEF_NER_LSTM_Training.utility import utility_application as app_util\n",
    "CONFIG = app_util.get_system_config()[1]\n",
    "OnPremiseFlag = CONFIG['OnPremiseFlag']\n",
    "OnPremiseFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying embeddings from s3\n",
    "if not os.path.exists('/notebooks/events'):\n",
    "    os.makedirs('/notebooks/events')\n",
    "    \n",
    "if not os.path.exists('/notebooks/Train_models'):\n",
    "    os.makedirs('/notebooks/Train_models')\n",
    "\n",
    "if not os.path.exists('/notebooks/embeddings'):\n",
    "    os.makedirs('/notebooks/embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "if OnPremiseFlag!=True:\n",
    "    from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "    \n",
    "from EEF_NER_LSTM_Training.OI_Progress_function import OI_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model for faster_rcnn_inception_v2_coco_2018_01_28 alredy present\n"
     ]
    }
   ],
   "source": [
    "# commented below code for on premise deployment and performance improvement\n",
    "# # Downloading faster_rcnn_inception_v2_coco_2018_01_28 from s3\n",
    "\n",
    "# if os.path.isfile('/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/saved_model/saved_model.pb'):\n",
    "#     print(\"Pretrained model for faster_rcnn_inception_v2_coco_2018_01_28 alredy present\")\n",
    "# else:\n",
    "#     print(\"Creating directory Pretrained model for faster_rcnn_inception_v2_coco_2018_01_28\")\n",
    "    \n",
    "#     if not os.path.exists('/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28'):\n",
    "#         os.makedirs('/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28')\n",
    "\n",
    "#     if not os.path.exists('/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/saved_model'):\n",
    "#         os.makedirs('/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/saved_model')\n",
    "\n",
    "#     for key in bucket.list():\n",
    "#         if 'EEF_PretrainedModels/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/' in key.name:\n",
    "#             #s3 = boto3.client('s3',aws_access_key_id='AKIARZWAXCM2OZNL575G',aws_secret_access_key='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')\n",
    "#             s3.download_file(bucket.name, key.name, '/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/'+str((key.name.split('/')[-1])))        \n",
    "            \n",
    "#     shutil.move(\"/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/saved_model.pb\", \"/notebooks/pre-trained-model/faster_rcnn_inception_v2_coco_2018_01_28/saved_model/saved_model.pb\")\n",
    "#     print(\"Created directory Pretrained model for faster_rcnn_inception_v2_coco_2018_01_28\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model for ssd_inception_v2_coco_2018_01_28 alredy present\n"
     ]
    }
   ],
   "source": [
    "# commented below code for on premise deployment and performance improvement\n",
    "# # Downloading ssd_inception_v2_coco_2018_01_28 from s3\n",
    "\n",
    "# if  os.path.isfile('/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28/saved_model/saved_model.pb'):\n",
    "#     print(\"Pretrained model for ssd_inception_v2_coco_2018_01_28 alredy present\")\n",
    "# else:\n",
    "#     print(\"Creating directory Pretrained model for ssd_inception_v2_coco_2018_01_28\")\n",
    "    \n",
    "#     if not os.path.exists('/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28'):\n",
    "#         os.makedirs('/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28')\n",
    "\n",
    "#     if not os.path.exists('/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28/saved_model'):\n",
    "#         os.makedirs('/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28/saved_model')\n",
    "\n",
    "#     for key in bucket.list():\n",
    "#         if 'EEF_PretrainedModels/pre-trained-model/ssd_inception_v2_coco_2018_01_28/' in key.name:\n",
    "#             #s3 = boto3.client('s3',aws_access_key_id='AKIARZWAXCM2OZNL575G',aws_secret_access_key='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')\n",
    "#             s3.download_file(bucket.name, key.name, '/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28/'+str((key.name.split('/')[-1])))        \n",
    "            \n",
    "#     shutil.move(\"/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28/saved_model.pb\", \"/notebooks/pre-trained-model/ssd_inception_v2_coco_2018_01_28/saved_model/saved_model.pb\")\n",
    "#     print(\"Created directory Pretrained model for ssd_inception_v2_coco_2018_01_28\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings fasttext_gensim.model alredy present\n"
     ]
    }
   ],
   "source": [
    "# commented below code for on premise deployment and performance improvement\n",
    "# # Downloading the embeddings from s3 bucket\n",
    "\n",
    "# if  os.path.isfile('/notebooks/embeddings/fasttext_gensim.model'):\n",
    "#     print(\"embeddings fasttext_gensim.model alredy present\")\n",
    "# else:\n",
    "#     print(\"Creating directory for embeddings\")\n",
    "\n",
    "#     if not os.path.exists('/notebooks/embeddings'):\n",
    "#         os.makedirs('/notebooks/embeddings')\n",
    "\n",
    "#     print(\"Downloading embeddings\")\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     s3.download_file(bucket.name, 'EEF_Embedings/fasttext_gensim.model.vectors.npy', '/notebooks/embeddings/fasttext_gensim.model.vectors.npy')\n",
    "#     s3.download_file(bucket.name, 'EEF_Embedings/fasttext_gensim.model', '/notebooks/embeddings/fasttext_gensim.model')\n",
    "#     s3.download_file(bucket.name, 'EEF_Embedings/fasttext_gensim.model.vectors_ngrams.npy', '/notebooks/embeddings/fasttext_gensim.model.vectors_ngrams.npy')\n",
    "\n",
    "#     print(\"Embeddings download complete\")\n",
    "#     print(\"---Time require to download the Embeddings is %s seconds ---\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if face s3 or GCP error try below commands\n",
    "# pip install --user google-compute-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if face s3 or GCP error try below commands\n",
    "# pip install --user --upgrade google-api-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if face s3 or GCP error try below commands\n",
    "# from IPython.core.display import HTML\n",
    "# HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.copytree('/data/NER/nltk_data/','/home/mosaic-ai/nltk_data/')\n",
    "except:\n",
    "    print(\" NLTK data already present in /home/mosaic-ai/nltk_data/ \")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OI IMG_SIZE  (512, 512)\n",
      "  img_rescale_flag  False\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import threading\n",
    "from EEF_NER_LSTM_Training.utility import app_constants, logger_util, http_util\n",
    "from EEF_NER_LSTM_Training.utility import app_constants\n",
    "from EEF_NER_LSTM_Training.manager.model import ModelManager, initialize_model_pool\n",
    "from EEF_NER_LSTM_Training.utility import utility_application as app_util\n",
    "from EEF_NER_LSTM_Training.services.ai.lstm_cnn.train import (\n",
    "    training_in_progress,\n",
    "    create_training_event)\n",
    "from EEF_NER_LSTM_Training.services.ai.lstm_cnn.content_classification import (\n",
    "    get_region_of_interest,\n",
    "    train_content_classification_model)\n",
    "# from nltk_data import *\n",
    "from EEF_NER_LSTM_Training.manager.annotation import Annotation\n",
    "#added for arch2\n",
    "from mosaicml.constants import MLModelFlavours\n",
    "import ast\n",
    "import re\n",
    "from os import path\n",
    "import time\n",
    "from EEF_NER_LSTM_Training.manager.mosaicml_model import update_mosaicml_metadata_info, get_mosaicml_metadata_info\n",
    "# from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_event file not found in /notebooks/events/train.event\n"
     ]
    }
   ],
   "source": [
    "# File cleaning\n",
    "# deleting train.event file \n",
    "\n",
    "try:\n",
    "    Train_evnts = os.remove('/notebooks/events/train.event')\n",
    "    print(\"removed train_event file from /notebooks/events/train.event\") \n",
    "except:\n",
    "    print(\"train_event file not found in /notebooks/events/train.event\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting only the application config utility\n",
    "CONFIG = app_util.get_system_config()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create base model added for arch2\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "\n",
    "inp = Input((64,64,1))\n",
    "out = Lambda(lambda x: K.identity(x))(inp)\n",
    "model = Model(inp,out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=Nadam(lr=0.0001))\n",
    "base_model_location = os.path.join(CONFIG[\"MODEL_BASE_LOCATION\"],CONFIG['MODEL_WEIGHTS_COMPLETE'])\n",
    "model.save(base_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Setting the env variables for Notebook testing\n",
    "# For OI testing on QA\n",
    "# %env files = \"['38576', '38577', '38578', '38579', '38580', '38581', '38582', '38583', '38584','38586', '38587', '38588', '38589', '38590', '38591', '38592', '38593', '38594', '38595', '38596', '38599', '38600', '38601', '38603', '38605', '38607', '38608', '38900', '38901', '38902']\"\n",
    "# For IC testing on QA\n",
    "# %env files = \"['38536', '38537', '38538', '38539', '38540', '38541', '38542', '38543', '38544', '38545', '38546', '38547', '38548', '38549', '38550', '38551', '38552', '38553', '38554', '38555', '38556', '38557', '38558', '38559', '38560', '38561', '38562', '38563', '38564', '38565', '38566', '38567', '38568', '38569', '38570', '38571', '38572', '38573', '38574', '38575','38964']\"\n",
    "# Excel solution used for testing in AIL\n",
    "# %env files = \"['1','2','3','4','5','6','7','8']\"\n",
    "# solution for CV Nelsonhall on QA\n",
    "# %env files = \"['6813', '6814', '6815', '6816', '6817', '6818', '6819', '6820', '6821', '6822', '6823', '6824', '6825', '6826', '6827', '6828', '6829', '6830', '6831', '6832', '6833']\"\n",
    "# mode2 blood report solution\n",
    "# %env files = \"['2601', '2606', '2597', '2598', '2599', '2600', '2603', '2604', '2605', '2607']\"\n",
    "# For testing mode 2 Solution: NelsonHall - Invoice  soln id=222\n",
    "# %env files = \"['4550', '4553', '4557', '4559', '4560', '4564', '4565', '4567', '4570', '4574', '4576', '4577', '4581', '4582', '4584', '4587', '4591', '4593', '4598', '4599', '4601', '4604', '4608', '4610', '4611', '4551', '4552', '4558', '4566', '4568', '4569', '4575', '4583', '4585', '4586', '4592', '4600', '4602', '4603', '4609', '4554', '4561', '4571', '4578', '4588', '4595', '4605', '4612', '4555', '4562', '4572', '4579', '4589', '4606', '4613', '4556', '4563', '4573', '4580', '4590', '4597', '4607']\"\n",
    "# %env files = \"['4550', '4553', '4557', '4559', '4560', '4564']\"\n",
    "# soln 218\n",
    "# %env files = \"['4467', '4472', '4477', '4482', '4487', '4492', '4497', '4502', '4507', '4727', '4729', '4488', '4493', '4498', '4503', '4508', '4464', '4474', '4489', '4494', '4499', '4504', '4732', '4733', '4480', '4495', '4500', '4505', '4728', '4730', '4734', '4476', '4486', '4491', '4501', '4724', '4284', '4725', '4694', '4668', '4680', '4688', '4670', '4677', '4691', '4689']\"\n",
    "# For IC solution 189\n",
    "# %env files = \"['6183','6184','6185','6186','6187','6188','6189','6190','6191','6192']\"\n",
    "# For IC solution 253\n",
    "# %env files = \"['6235', '6236', '6237', '6238','6239', '6240', '6241', '6242', '6243', '6244', '6249', '6250', '6251', '6252', '6253', '6254', '6255', '6256', '6257','6258', '6259', '6260', '6261', '6262', '6263', '6264', '6265', '6266', '6267', '6268', '6269', '6270', '6271']\"\n",
    "# For OI solution 277\n",
    "# %env files = \"['6560','6561','6562','6563','6564','6565','6566','6567','6568','6569']\"\n",
    "# New excel solution\n",
    "#%env files = [\"6397\",\"6398\",\"6399\",\"6400\",\"6401\"] \n",
    "# files for document classifiaction \n",
    "#%env files = [\"6276\",\"6277\",\"6278\",\"6279\",\"6280\",\"6281\",\"6282\",\"6283\"]\n",
    "# Document classification solution 176\n",
    "# %env files = \"['2786','2787','2788','2790','2791','2820','2988','2990','2811','2812','2813','2814','2818','6340','6350','6349','6351','6352','2914']\"\n",
    "# files for CC 158\n",
    "# %env files = \"['2207','3207','6210','3204']\"\n",
    "# %env files = \"['2207','3207']\"\n",
    "# Document classification QA\n",
    "# %env files = \"['8247', '8248', '8249', '8250', '8251', '8252', '8253', '8254', '8255', '8256', '8257', '8258', '8259', '8260', '8261', '8262', '8263', '8264', '8265', '8266', '8267', '8268', '8269', '8270', '8271', '8272', '8273', '8274', '8275', '8276', '8277', '8278', '8279', '8280', '8281', '8282', '8283', '8284', '8285', '8286', '22647', '22648', '22649', '22650', '22651', '22652', '22653', '22654', '22655']\"\n",
    "# DC followed by NER \n",
    "# %env files = \"['2786', '2787', '2788', '2790', '2791', '2820', '2811', '2812', '2813', '2814', '2818']\"\n",
    "# NER CC 352 PC\n",
    "# %env files = \"['38717', '38718', '38719', '38720', '38721', '38722', '38723', '38725', '38726', '38727', '38728', '38729', '38730', '38731', '38732', '38733', '38734']\"\n",
    "# %env created_by = \"EEF\"\n",
    "# %env model_id = \"-1\"\n",
    "# %env model_id = \"7d9f5ae2-916e-41df-b0a6-ba009bd2bf3d__958d5ea6-1894-424e-96cd-ac7d92c74527\"\n",
    "# %env model_id = \"7d9f5ae2-916e-41df-b0a6-ba009bd2bf3d__17df289a-cd5f-46cd-a204-bd5265c8893a\"\n",
    "# %env name = \"NB_DCNERCC_1\"\n",
    "# %env name = \"NB_OI_20_2\"\n",
    "# %env description = \" trained on all files of DC NER Insurance Policy multiple carriers soln \"\n",
    "# %env epochs = '20'\n",
    "# %env current_version = '1'\n",
    "# NER CC 352 PC\n",
    "# %env solution_id = '352' \n",
    "# QA OI\n",
    "# %env solution_id = '350' \n",
    "# QA IC\n",
    "# %env solution_id = '345' \n",
    "#Excel solution usedfor testing in AIL\n",
    "# %env solution_id = '2' \n",
    "#CC solution for CV Nelsonhall on QA\n",
    "# %env solution_id = '265' \n",
    "# mode2 blood report solution\n",
    "# %env solution_id = '171' \n",
    "# For mode 2 testing\n",
    "# %env solution_id = '222'\n",
    "# %env solution_id = '218'\n",
    "# IC Solution\n",
    "# %env solution_id = '189' \n",
    "# IC Solution\n",
    "# %env solution_id = '253' \n",
    "# OI Solution\n",
    "# %env solution_id = '277' \n",
    "# New excel solution \n",
    "#%env solution_id = '258' \n",
    "# Solution for Document classification of Cincinati,Travelers,Pensilivia\n",
    "#%env solution_id = '174'  \n",
    "# Solution for CC+NER\n",
    "# %env solution_id = '158'\n",
    "# Document classification QA\n",
    "# %env solution_id = '289'\n",
    "# QA DC and NER\n",
    "# %env solution_id = '176'\n",
    "# Solution for CC Document classification of Cincinati,Travelers\n",
    "# %env solution_id = '176'\n",
    "# Excel model_type used for testing in AIL and CC as well\n",
    "# %env model_type = [\"NER\"] \n",
    "# model type for DC\n",
    "# %env model_type = [\"DC\"]   \n",
    "# model type for DC and NER\n",
    "# %env model_type = [\"NER\",\"DC\"]   \n",
    "# For IC model_type\n",
    "# %env model_type = [\"IC\"]\n",
    "# For # For IC model_type model_type\n",
    "# %env model_type = [\"Object Identification\"]\n",
    "# %env language = \"english\"\n",
    "# %env mode_type = \"mode_1\"\n",
    "# %env mode_type = \"mode_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for re-training testing\n",
    "# %env files = \"['51815', '51811', '51810', '51809', '51806', '51805', '51804', '51800', '51799', '51797', '52505', '52504', '52501', '52499', '52502', '52497', '52495', '52498', '52496', '52490', '52506', '52508', '52494', '52489', '52491', '52493', '52507', '52492']\"\n",
    "# %env created_by = \"James\"\n",
    "# %env model_id = \"89e4b1c4-2ec2-4a33-be33-c9d974b75ff4\"\n",
    "# %env version_id = \"afa0f985-67de-4583-935b-c9217829ec9a\"\n",
    "# %env name = \"RPA_Reliable_Policy_1\"\n",
    "# %env description = \"Training Test SHould delete\"\n",
    "# %env epochs = '1'\n",
    "# %env current_version = '1'\n",
    "# %env solution_id = '448' \n",
    "# %env model_type = [\"NER\"]\n",
    "# %env language = \"english\"\n",
    "# %env mode_type = \"mode_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mosaic-ai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed for on premise \n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "try:\n",
    "    shutil.copytree('/data/NER/nltk_data/','/home/mosaic-ai/nltk_data/')\n",
    "except:\n",
    "    print(\" NLTK data already present in /home/mosaic-ai/nltk_data/ \")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train module started\n",
      "> \u001b[0;32m<ipython-input-21-ecfc5e69e12e>\u001b[0m(29)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     27 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;31m# For Postmen execusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 29 \u001b[0;31m        \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m\u001b[0;31m#         language = eval(str(os.getenv('language','english')).lower())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language..\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "KeyError: 'language'\n",
      "> \u001b[0;32m<ipython-input-21-ecfc5e69e12e>\u001b[0m(29)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     27 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;31m# For Postmen execusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 29 \u001b[0;31m        \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m\u001b[0;31m#         language = eval(str(os.getenv('language','english')).lower())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language..\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-21-ecfc5e69e12e>\u001b[0m(378)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    376 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mresult_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    377 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 378 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    379 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_EVENT_FILE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    380 \u001b[0;31m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_EVENT_FILE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> l\n",
      "\u001b[1;32m    373 \u001b[0m                        \u001b[0;34m\"CCFlag\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCCFlag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    374 \u001b[0m                       \"Ensemble_ModelDetails\":Ensemble_ModelDetails}\n",
      "\u001b[1;32m    375 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    376 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresult_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    377 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 378 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    379 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_EVENT_FILE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    380 \u001b[0m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_EVENT_FILE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    381 \u001b[0m        \u001b[0;31m#LOGGER.exception(str(exception))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    382 \u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    383 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"failure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete Training Flow\n",
    "\"\"\"Routes for training as a service.\"\"\"\n",
    "\"\"\" This code is taken from service-model.py\"\"\"\n",
    "\n",
    "import ast\n",
    "LOGGER = logger_util.get_logger(\"model_service\")\n",
    "\n",
    "def train():\n",
    "    print(\"Train module started\")\n",
    "    \"\"\"\n",
    "    Route to train Train the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "        NONE\n",
    "    Returns :\n",
    "        Json of all the models present in the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #req_data = request.get_json()\n",
    "        '''\n",
    "        if 'language' in req_data:\n",
    "            language = req_data[\"language\"].lower()\n",
    "        else:\n",
    "            language = 'english'\n",
    "        '''\n",
    "        # For Postmen execusion\n",
    "        language = os.environ[\"language\"]\n",
    "#         language = eval(str(os.getenv('language','english')).lower())\n",
    "        print(\"language..\",language)\n",
    "        created_by = os.environ[\"created_by\"]\n",
    "        print(\"created_by..\",created_by)\n",
    "        files = os.environ['files']\n",
    "        files = files.strip('\"')\n",
    "        files = eval(files)\n",
    "        print(\"files..\",files)\n",
    "#         files = eval(os.environ['files'])\n",
    "#         files = ast.literal_eval(files)\n",
    "#         print(\"files..\",files)\n",
    "        # for arch1 org AI model table\n",
    "#         model_id = os.environ['model_id']\n",
    "#         print(\"model_id..\",model_id,type(model_id))\n",
    "        # updated for retraining flow of arch2\n",
    "        try:\n",
    "            model_id = int(eval(os.environ['model_id']))\n",
    "        except:\n",
    "            print(\"failed to create int model id\")\n",
    "#             model_id = eval(os.environ['model_id'])+\"__\"+eval(os.environ['version_id'])\n",
    "            model_id = os.environ['model_id']+\"__\"+os.environ['version_id']\n",
    "    \n",
    "        # Added for DC NER CC Retraining \n",
    "        try:\n",
    "            label = os.environ[\"label\"]\n",
    "            print(\" label recived for DC NER CC retraining flow \",label , type(label))\n",
    "        except:\n",
    "            label = None\n",
    "            print(\" label not recived setting it None\")\n",
    "            \n",
    "        print('model_id',model_id,type(model_id))\n",
    "        solution_id = int(eval(os.environ['solution_id']))\n",
    "        print(\"solution_id..\",solution_id)\n",
    "        current_version = eval(os.environ['current_version'])\n",
    "        name = os.environ['name']\n",
    "        print(\"model name\" , name, type(name) )\n",
    "        model_type = os.environ['model_type']\n",
    "        print(\"model_type  \", model_type , type(model_type))\n",
    "        mode_type = os.environ['mode_type']\n",
    "        print(\"mode_type...\",mode_type)\n",
    "        print(\"type mode_type...\",type(mode_type))\n",
    "        description =  os.environ['description']\n",
    "        print(\"model_type details\", model_type,eval(model_type),type(model_type),type(eval(model_type)))\n",
    "        \n",
    "        if ((len(eval(model_type)) == 1) and (eval(model_type)[0] == 'DC')):\n",
    "            epochs = 10\n",
    "        else:\n",
    "            epochs =  int(eval(os.environ['epochs']))\n",
    "            \n",
    "        # commented below code as it was not working for DC NER    \n",
    "#         if eval(model_type)[0] != 'DC':\n",
    "#             epochs =  int(eval(os.environ['epochs']))\n",
    "#         else:\n",
    "#             print('Taking default 10 epochs for DC training')\n",
    "#             epochs = 10\n",
    "        print(\"model epochs \",epochs)\n",
    "\n",
    "#         epochs =  int(eval(os.environ['epochs']))\n",
    "#         print(\"model epochs \",epochs)\n",
    "            \n",
    "        parent_id = model_id\n",
    "        print('parent_id',parent_id,type(parent_id))\n",
    "\n",
    "        try:\n",
    "            ail_model_id = model_id.split(\"__\")[0]\n",
    "        except:\n",
    "            ail_model_id = model_id\n",
    "            \n",
    "        print(\"ail_model_id \", ail_model_id)\n",
    "\n",
    "        try:\n",
    "            ail_version_id = model_id.split(\"__\")[1]\n",
    "        except:\n",
    "            ail_version_id = model_id\n",
    "            \n",
    "        print(\"ail_version_id \", ail_version_id)\n",
    "        \n",
    "        # epochs =  os.environ['epochs'].strip('\"')\n",
    "        \n",
    "#         try:\n",
    "#             parent_id = int(eval(model_id)) \n",
    "#         except:\n",
    "#             print(\"failed to create int model id\")\n",
    "#             parent_id = model_id\n",
    "#         print('parent_id',parent_id,type(parent_id))\n",
    "#         try:\n",
    "#             ail_model_id = int(eval(model_id))\n",
    "#         except:\n",
    "#             ail_model_id = eval(model_id).split(\"__\")[0]\n",
    "#         print(\"ail_model_id \", ail_model_id)\n",
    "#         try:\n",
    "#             ail_version_id = int(eval(model_id))\n",
    "#         except:\n",
    "#             ail_version_id = eval(model_id).split(\"__\")[1]\n",
    "#         print(\"ail_version_id \", ail_version_id)\n",
    "        \n",
    "#         parent_id = int(eval(model_id))\n",
    "#         ail_model_id = int(eval(model_id))\n",
    "#         ail_version_id = int(eval(model_id))\n",
    "        print(\"Importing param done\")\n",
    "\n",
    "#         # For Jupytar execusion\n",
    "#         language = eval(str(os.getenv('language','english')).lower())\n",
    "#         print(\"language..\",language)\n",
    "#         created_by = eval(os.getenv('created_by',''))\n",
    "#         files = eval(os.getenv('files',''))\n",
    "#         files = ast.literal_eval(files)\n",
    "#         print(\"files..\",files)\n",
    "#         model_id = os.getenv('model_id','')\n",
    "#         print(\"model_id..\",model_id)\n",
    "#         solution_id = int(eval(os.getenv('solution_id','')))\n",
    "#         print(\"solution_id..\",solution_id)\n",
    "#         current_version = eval(os.getenv('current_version',''))\n",
    "#         name = eval(os.getenv('name',''))\n",
    "#         model_type = os.getenv('model_type','NOT_SPECIFIED')\n",
    "#         print(model_type)\n",
    "#         mode_type = os.getenv('mode_type','')\n",
    "#         description =  os.getenv('description','')\n",
    "#         epochs =  int(eval(os.getenv('epochs',10)))\n",
    "#         print(\"epochs...\",epochs)\n",
    "#         try:\n",
    "#             parent_id = int(eval(model_id))\n",
    "#         except:\n",
    "#             parent_id = eval(model_id)            \n",
    "#         print('parent_id',parent_id)\n",
    "#         try:\n",
    "#             ail_model_id = int(eval(model_id))\n",
    "#         except:\n",
    "#             ail_model_id = eval(model_id).split(\"__\")[0]\n",
    "#         print(\"ail_model_id \", ail_model_id)\n",
    "#         try:\n",
    "#             ail_version_id = int(eval(model_id))\n",
    "#         except:\n",
    "#             ail_version_id = eval(model_id).split(\"__\")[1]\n",
    "#         print(\"ail_version_id \", ail_version_id)\n",
    "        \n",
    "\n",
    "        # Check whether model training already in progress.\n",
    "        if training_in_progress():\n",
    "            return logging.info(\n",
    "                {\"status\": \"failure\",\n",
    "                 \"error\": \"Training already in progress. Try after some time.\"}\n",
    "            )\n",
    "\n",
    "        if (isinstance(parent_id, int) != True) and (OnPremiseFlag != True) :\n",
    "            basemodel_path = '/notebooks/Train_models/' + str(parent_id)+'/'\n",
    "            os.makedirs(basemodel_path)\n",
    "            print(\"model directory created for base model\")\n",
    "            # download the base model from s3 and keep it in above folder\n",
    "            for key in bucket.list():\n",
    "                s3_basemodel = 'EEF_models/'+str(parent_id)+'/'\n",
    "                if s3_basemodel in key.name:\n",
    "                    print(\"............downloading \", s3_basemodel)\n",
    "                    s3.download_file(bucket.name, key.name, '/notebooks/Train_models/' + str(parent_id)+'/'+str((key.name.split('/')[-1])))\n",
    "                    print(\"............download complete \",s3_basemodel)\n",
    "        elif (isinstance(parent_id, int) != True) and (OnPremiseFlag == True):\n",
    "            basemodel_path = '/notebooks/Train_models/' + str(parent_id)+'/'\n",
    "            os.makedirs(basemodel_path)\n",
    "            print(\"model directory created for base model\")\n",
    "            s3_basemodel = '/data/EEF_models/'+str(parent_id)+'/'\n",
    "            try:\n",
    "                shutil.copytree(s3_basemodel,'/notebooks/Train_models/' + str(parent_id))\n",
    "            except:\n",
    "                print(\"fail to copy model from data PV \")\n",
    "        \n",
    "\n",
    "        # Create train event file so that no other training process initiates.\n",
    "        create_training_event(language)\n",
    "        print(\"Created Training Event\")\n",
    "        #print(\"Created Training Event\")\n",
    "\n",
    "        model_manager = ModelManager()\n",
    "        thread_start_flag = True\n",
    "        train_process = None\n",
    "        if len(eval(model_type)) == 1:\n",
    "            print(\"length of model_type is 1\")\n",
    "            thread_start_flag = True\n",
    "            #new_model_id = 1\n",
    "            \n",
    "            # added for arch2\n",
    "            new_model_id = model_manager.add_model(created_by, files, parent_id, name, solution_id, eval(model_type)[0],int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.keras)\n",
    "#             new_model_id = model_manager.add_model(created_by, files, parent_id, name, eval(model_type)[0],\n",
    "#                                                    int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id)\n",
    "                                                       \n",
    "            print(\"new_model_id created\",new_model_id)\n",
    "            Ensemble_ModelDetails = []\n",
    "            # added for cc loop\n",
    "            annotation = Annotation(language)\n",
    "            model_id = new_model_id\n",
    "            print(\"Fetch details on Content Classification\")\n",
    "            #CCFlag,file_data = annotation.Get_CC_Flag_Data(files, solution_id, model_id)\n",
    "            try:\n",
    "                CCFlag,file_data = annotation.Get_CC_Flag_Data(files, solution_id, model_id, created_by)\n",
    "                print(\"CCFlag value \",CCFlag)\n",
    "            except:\n",
    "                print(\"unable to fetch the Content Classification data, keeping CCFlag False\")\n",
    "                CCFlag = \"False\"\n",
    "                print(\"CCFlag value \",CCFlag)\n",
    "            \n",
    "            if CCFlag == \"True\":\n",
    "                print(\"sample file_data...\",file_data[0][0])\n",
    "                train_content_classification_model(model_id, file_data)\n",
    "                print(\"Content Classification Data fetched successfully \")\n",
    "            else:\n",
    "                print(\" CCFlag value is False \")                \n",
    "            \n",
    "            #if eval(model_type)[0] == 'NER' and eval(mode_type) in ['mode_1',\"\"]:\n",
    "            if eval(model_type)[0] == 'NER' and mode_type == 'mode_1':\n",
    "                print(\"in NER and mode_1\")\n",
    "                # train_pr = ModelManager().train(new_model_id, files, parent_id, solution_id, language, epochs)\n",
    "#                 label = None\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train, args=[\n",
    "                        new_model_id,\n",
    "                        files, parent_id, created_by,label, solution_id, language, epochs\n",
    "                    ])\n",
    "                \n",
    "                print(\"finished training\")\n",
    "            #elif eval(model_type)[0] == 'NER' and eval(mode_type) == 'mode_2':\n",
    "            elif eval(model_type)[0] == 'NER' and mode_type == 'mode_2':\n",
    "                print(\"in NER and mode_2\")\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_spacy_ner, args=[\n",
    "                        new_model_id,\n",
    "                        files, parent_id, created_by, solution_id, language, epochs\n",
    "                    ])\n",
    "            elif eval(model_type)[0] == 'DC':\n",
    "                print(\" Training Document classification model\")\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_document_classification, args=[\n",
    "                        new_model_id,\n",
    "                        files, parent_id, created_by, solution_id\n",
    "                    ])\n",
    "            elif eval(model_type)[0] == 'IC':\n",
    "                print(\"In Img classification loop\")\n",
    "                input_img_type =  os.getenv('input_img_type','png')\n",
    "                print(\"input_img_type\",input_img_type)\n",
    "                print(\"epochs...\",epochs)\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_img_classify, args=[\n",
    "                        new_model_id,\n",
    "                        files, created_by, parent_id, solution_id, input_img_type,language,epochs\n",
    "                    ])\n",
    "            elif eval(model_type)[0] == \"Object Identification\":\n",
    "                \n",
    "                #thread_start_flag = False\n",
    "                print(\"OI loop\")\n",
    "                print(files)\n",
    "                print(type(files))\n",
    "                \n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_zoning, args=[\n",
    "                        new_model_id,\n",
    "                        files,parent_id,solution_id,epochs, created_by\n",
    "                    ]\n",
    "                )\n",
    "                print(\"Object Identification progress\")\n",
    "    \n",
    "        \n",
    "        elif len(eval(model_type)) == 2:\n",
    "            thread_start_flag = True\n",
    "            if 'DC' in model_type and 'NER' in model_type:\n",
    "#                 new_model_id = model_manager.add_model(created_by, files, parent_id,\n",
    "#                                                name, 'DC', current_version, language, \"\", description)\n",
    "#                 CCFlag=\"False\"\n",
    "                # added for arch2\n",
    "                new_model_id = model_manager.add_model(created_by, files, parent_id, name, solution_id, 'DC',int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.keras)\n",
    "        \n",
    "                # saving model details for future refrence\n",
    "                Ensemble_ModelDetails = []\n",
    "                DC_details = {\"model_id\":new_model_id,'name':name,\"files\":files,\"model_type\":'DC',\"CCFlag\":\"False\",\"epochs\":10}\n",
    "                Ensemble_ModelDetails.append(DC_details.copy())\n",
    "                \n",
    "                annotation = Annotation(language)\n",
    "                result,result_status = annotation.get_doc_classify_training_data(new_model_id, files, solution_id, created_by)\n",
    "                print(\"Prepared the document classification training data.\")\n",
    "                \n",
    "                \n",
    "                if result_status == app_constants.SUCCESS:\n",
    "                    \n",
    "                    train_process = threading.Thread(target=ModelManager().train_document_classification, args=[\n",
    "                        new_model_id,\n",
    "                        files, parent_id, created_by, solution_id\n",
    "                    ])\n",
    "                                \n",
    "                    training_data = result\n",
    "\n",
    "                    classification_labels = set(list(training_data['carrier_name']))\n",
    "                    print(\"classification_labels ...\",classification_labels)\n",
    "                    classification_labels_files = training_data.drop(\"page_data\", axis=1)\n",
    "                    print(\"classification_labels_files ...\",classification_labels_files)\n",
    "\n",
    "                    for label in classification_labels:\n",
    "                        print(\"Document class \",label)\n",
    "                        files_label_specific1 = list(classification_labels_files.loc[classification_labels_files['carrier_name']==str(label),'file_id'])\n",
    "                        files_label_specific = []\n",
    "                        for i in files_label_specific1:\n",
    "                            files_label_specific.append(str(i))\n",
    "                        print(\"files_label_specific \",files_label_specific,type(files_label_specific))\n",
    "\n",
    "                        import time\n",
    "                        tm = int(round(time.time()))\n",
    "                        name = 'DC_'+label+'_NER_'+str(tm)\n",
    "                        print(\"name of NER model \",name)\n",
    "                        current_version = 1\n",
    "                        parent_id = -1\n",
    "                        ail_model_id = -1\n",
    "                        ail_version_id = -1\n",
    "                        mode_type = \"mode_1\"\n",
    "\n",
    "                        new_model_id = model_manager.add_model(created_by, files_label_specific, parent_id, name,solution_id, 'NER',int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.keras)\n",
    "\n",
    "                        # added for cc loop\n",
    "                        annotation = Annotation(language)\n",
    "                        model_id = new_model_id\n",
    "                        print(\"Fetch details on Content Classification\")\n",
    "                        #CCFlag,file_data = annotation.Get_CC_Flag_Data(files, solution_id, model_id)\n",
    "                        try:\n",
    "                            CCFlag,file_data = annotation.Get_CC_Flag_Data_DC(files_label_specific, solution_id, model_id,label, created_by)\n",
    "                            print(\"CCFlag value \",CCFlag)\n",
    "                        except:\n",
    "                            print(\"unable to fetch the Content Classification data, keeping CCFlag False\")\n",
    "                            CCFlag = \"False\"\n",
    "                            print(\"CCFlag value \",CCFlag)\n",
    "\n",
    "                        if CCFlag == \"True\":\n",
    "                            print(\"sample file_data...\",file_data[0][0])\n",
    "                            train_content_classification_model(model_id, file_data)\n",
    "                            print(\"Content Classification Data fetched successfully \")\n",
    "                        else:\n",
    "                            print(\" CCFlag value is False \")\n",
    "\n",
    "                        NER_details = {\"model_id\":new_model_id,\"name\":name,\"files\":files_label_specific,\"model_type\":'NER',\"CCFlag\":CCFlag,\"epochs\":epochs}\n",
    "                        Ensemble_ModelDetails.append(NER_details.copy())\n",
    "\n",
    "                        print(\"Files for NER+DC traning..\",files)\n",
    "                        print(\"solution id for NER+DC traning..\",solution_id)\n",
    "                        print(\"language for NER+DC traning..\",language)\n",
    "                        print(\"epochs for NER+DC traning..\",epochs)\n",
    "                        print(\"parent_id for NER+DC traning..\",parent_id)\n",
    "\n",
    "#                         ModelManager().train(model_id = new_model_id,files = files_label_specific, parent_id = parent_id, created_by = created_by,solution_id = solution_id, language = language,epochs = epochs)\n",
    "\n",
    "                        # added label below as DC+NER+CC was not working                        \n",
    "                        DCNER_process = threading.Thread(target=ModelManager().train, args=[new_model_id,files_label_specific, parent_id, created_by,label, solution_id,language,epochs])\n",
    "                        DCNER_process.start()\n",
    "                        DCNER_process.join()\n",
    "                        print(\"DCNER model train started \")\n",
    "                        BASE_PATH = '/notebooks/Train_models/'\n",
    "                        model_path = os.path.join(BASE_PATH, new_model_id)\n",
    "                        model_file_path = model_path+'/valid_char_set.pkl'\n",
    "                        print(\"model_file_path...............\",model_file_path)\n",
    "                        # Loop added to wait till model proccess completes\n",
    "                        while not os.path.exists(model_file_path):\n",
    "                            print(\"waiting for valid_char_set.pkl files creation for model \" ,new_model_id)\n",
    "                            time.sleep(10)\n",
    "\n",
    "                        if os.path.isfile(model_file_path):\n",
    "                            print(\"recived new files\")\n",
    "                            time.sleep(30)\n",
    "                        else:\n",
    "                            raise ValueError(\"%s isn't a file!\" % file_path)\n",
    "\n",
    "                        # added for multiproccess\n",
    "#                         m_name = \"model_\"+label\n",
    "#                         print(\"m_name\",m_name)\n",
    "#                         m_name = Process(target=ModelManager().train,args=[new_model_id,files_label_specific,parent_id,solution_id,language,epochs])\n",
    "#                         print(m_name)\n",
    "#                         m_name.start()\n",
    "                        \n",
    "                        print(\"NER followed by DC started\")\n",
    "                        time.sleep(30)\n",
    "                                        \n",
    "                elif result_status == app_constants.FAILED:\n",
    "                    print(\"Failed to get the AI Tutor data for document classification. \")\n",
    "                    print(\"Clearing training event\")\n",
    "                    if clear_training_event():\n",
    "                        print(\"Cleared training event\")\n",
    "                        update_mosaicml_metadata_info(new_model_id,data={'status':_app_constants.STATUS_FAILED})\n",
    "                    return app_constants.FAILED\n",
    "\n",
    "            if \"IC\" in model_type and \"Object Identification\" in model_type:\n",
    "                print(\"In Img classification loop\")\n",
    "                input_img_type =  os.getenv('input_img_type','png')\n",
    "                CCFlag = \"False\"\n",
    "                new_model_id = model_manager.add_model(created_by, files, parent_id, name,solution_id, 'IC',int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.keras)\n",
    "        \n",
    "                # saving model details for future refrence\n",
    "                Ensemble_ModelDetails = []\n",
    "                IC_details = {\"model_id\":new_model_id,'name':name,\"files\":files,\"model_type\":'IC',\"CCFlag\":\"False\",\"epochs\":epochs}\n",
    "                Ensemble_ModelDetails.append(IC_details.copy())\n",
    "                \n",
    "                annotation = Annotation(language)\n",
    "                result,result_status = annotation.get_img_classify_training_data(model_id, files, solution_id, input_img_type, created_by)\n",
    "                print(\"result\",result)\n",
    "                print(\"result_status\",result_status)\n",
    "                print(\"Prepared the image classification training data.\")\n",
    "\n",
    "                if result_status == app_constants.SUCCESS:\n",
    "                    train_process = threading.Thread(target=ModelManager().train_img_classify, args=[\n",
    "                        new_model_id,\n",
    "                        files, created_by, parent_id, solution_id, input_img_type,language,epochs\n",
    "                    ])\n",
    "                    \n",
    "                    file_class = list(set([i.split('/')[-2] for i in result['file_paths']]))\n",
    "                    file_class_list = [i.split('/')[-2] for i in result['file_paths']]\n",
    "                    \n",
    "                    \n",
    "                    print(\"classification_labels ...\",file_class)\n",
    "                    print(\"list of files class \",file_class_list)\n",
    "                    \n",
    "                    for label in file_class:\n",
    "                        import logging\n",
    "                        file_nm = \"OI_progress_\"+str(label)+\".log\"\n",
    "                        file_nm_path = os.path.join('/home/mosaic-ai/'+file_nm)\n",
    "#                         logging.basicConfig(filename=file_nm_path, filemode='w',format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "                        root_logger = logging.getLogger()\n",
    "                        new_handler = logging.FileHandler(file_nm)\n",
    "                        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "                        new_handler.setFormatter(formatter)\n",
    "                        root_logger.addHandler(new_handler)\n",
    "                        \n",
    "                        print(\"Document class \",label)\n",
    "                        logging.info(label)\n",
    "                        print(os.listdir('/home/mosaic-ai/'))\n",
    "                        temp =[]\n",
    "                        for j in range(len(file_class_list)):\n",
    "                            if file_class_list[j] == label:\n",
    "                                temp.append(files[j])\n",
    "                            else:\n",
    "                                pass\n",
    "                        print(\"temp \",temp )\n",
    "                        import time\n",
    "                        tm = int(round(time.time()))\n",
    "                        name = 'IC_'+label+'_OI_'+str(tm)\n",
    "                        print(\"name of OI model \",name)\n",
    "                        current_version = 1\n",
    "                        parent_id = -1\n",
    "                        ail_model_id = -1\n",
    "                        ail_version_id = -1\n",
    "                        mode_type = \"mode_1\"\n",
    "                        \n",
    "                        files_label_specific=temp\n",
    "                        \n",
    "                        new_model_id = model_manager.add_model(created_by, files_label_specific, parent_id, name,solution_id, 'Object Identification',int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.keras)\n",
    "\n",
    "                        OI_details = {\"model_id\":new_model_id,\"name\":name,\"files\":temp,\"model_type\":'Object Identification',\"epochs\":epochs}\n",
    "                        Ensemble_ModelDetails.append(OI_details.copy())\n",
    "\n",
    "                        print(\"Files for IC+OI traning..\",files)\n",
    "                        print(\"solution id for IC+OI traning..\",solution_id)\n",
    "                        print(\"language for IC+OI traning..\",language)\n",
    "                        print(\"epochs for IC+OI traning..\",epochs)\n",
    "                        print(\"parent_id for IC+OI traning..\",parent_id)\n",
    "\n",
    "                        # ModelManager().train_zoning(model_id = new_model_id,files = temp, parent_id = parent_id, solution_id =solution_id,epochs = epochs)\n",
    "                        ICOI_proccess = threading.Thread(target=ModelManager().train_zoning, args=[new_model_id,temp,parent_id,solution_id,epochs,created_by])\n",
    "                        ICOI_PROGRESS = threading.Thread(target=OI_progress,args =[file_nm_path,new_model_id,epochs])\n",
    "                        ICOI_proccess.start()\n",
    "                        ICOI_PROGRESS.start()\n",
    "                        ICOI_proccess.join()\n",
    "                        ICOI_PROGRESS.join()\n",
    "                        print(\"ICOI model train started \")\n",
    "                        BASE_PATH = '/notebooks/Train_models/'\n",
    "                        model_path = os.path.join(BASE_PATH, new_model_id)\n",
    "                        model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "                        print(\"model_file_path...............\",model_file_path)\n",
    "                        # Loop added to wait till model proccess completes\n",
    "                        while not os.path.exists(model_file_path):\n",
    "                            print(\"waiting for frozen_inference_graph.pb files creation for model \" ,new_model_id)\n",
    "                            time.sleep(10)\n",
    "\n",
    "                        if os.path.isfile(model_file_path):\n",
    "                            print(\"recived new files\")\n",
    "                            time.sleep(30)\n",
    "                        else:\n",
    "                            raise ValueError(\"%s isn't a file!\" % file_path)\n",
    "\n",
    "                        print(\"OI followed by IC started\")\n",
    "                        # OI_progress(Log_file=file_nm_path,model_id=new_model_id,epoch=epochs)\n",
    "                        time.sleep(30)\n",
    "\n",
    "                elif result_status == app_constants.FAILED:\n",
    "                    print(\"Failed to get the AI Tutor data for Object identification. \")\n",
    "                    print(\"Clearing training event\")\n",
    "                    if clear_training_event():\n",
    "                        print(\"Cleared training event\")\n",
    "                        update_mosaicml_metadata_info(new_model_id,data={'status':_app_constants.STATUS_FAILED})\n",
    "                    return app_constants.FAILED                \n",
    "\n",
    "        if thread_start_flag:\n",
    "            print(\"...........Model traing started................\")\n",
    "            model_id = str(new_model_id)\n",
    "            if train_process and eval(model_type)[0] == \"Object Identification\" and len(eval(model_type)) == 1:\n",
    "                file_nm_path = '/home/mosaic-ai/NER_Training_Service.log'\n",
    "                OI_PROGRESS = threading.Thread(target=OI_progress,args =[file_nm_path,new_model_id,epochs])\n",
    "                train_process.start()\n",
    "                OI_PROGRESS.start()\n",
    "                train_process.join()\n",
    "                OI_PROGRESS.join()\n",
    "            else:\n",
    "                train_process.start()\n",
    "                train_process.join()            \n",
    "#             if train_process:\n",
    "#                 train_process.start()\n",
    "#                 train_process.join()\n",
    "        result_model = {\"status\": \"success\",\n",
    "                        \"data\": app_constants.STATUS_INPROGRESS,\n",
    "                        \"model_id\": str(new_model_id),\n",
    "                        \"CCFlag\": CCFlag,\n",
    "                       \"Ensemble_ModelDetails\":Ensemble_ModelDetails}\n",
    "        \n",
    "        return result_model\n",
    "\n",
    "    except Exception as exception:\n",
    "        if os.path.exists(CONFIG[\"TRAIN_EVENT_FILE\"]):\n",
    "            os.remove(CONFIG[\"TRAIN_EVENT_FILE\"])\n",
    "        #LOGGER.exception(str(exception))\n",
    "        print(str(exception))\n",
    "        return {\"status\": \"failure\", \"error\": str(exception)}\n",
    "\n",
    "   \n",
    "TrainModel = train()\n",
    "\n",
    "print(\"out from TrainModel\")\n",
    "\n",
    "time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'failure', 'error': \"'language'\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ensemble_ModelDetails'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3c8f35810f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEnsemble_ModelDetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ensemble_ModelDetails'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Ensemble_ModelDetails'"
     ]
    }
   ],
   "source": [
    "Ensemble_ModelDetails = TrainModel['Ensemble_ModelDetails']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble_ModelDetails......\",Ensemble_ModelDetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)==0:\n",
    "    time.sleep(30)\n",
    "    # for jupy execution\n",
    "    # model_type = os.getenv('model_type','NOT_SPECIFIED')\n",
    "    # print(model_type)\n",
    "    # epochs =  int(eval(os.getenv('epochs',10)))\n",
    "    # print(\"epochs...\",epochs)\n",
    "    # new_model_id = TrainModel['model_id']\n",
    "\n",
    "    # for UI execusion\n",
    "\n",
    "    model_type = os.environ['model_type']\n",
    "    print(model_type)\n",
    "\n",
    "    if eval(model_type)[0] != 'DC':\n",
    "        epochs =  int(eval(os.environ['epochs']))\n",
    "    else:\n",
    "        print(\"DC model taking default value for epoch\")\n",
    "        epochs =  '10'\n",
    "\n",
    "    new_model_id = TrainModel['model_id']\n",
    "\n",
    "    if eval(model_type)[0] == \"Object Identification\":\n",
    "        # added for progress\n",
    "        OI_progress(Log_file='/home/mosaic-ai/NER_Training_Service.log',model_id=new_model_id,epoch=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Newly created model\n",
    "time.sleep(30)\n",
    "print(\"TrainModel['model_id']\",TrainModel['model_id'],type(TrainModel['model_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TrainModel['CCFlag']\",TrainModel['CCFlag'])\n",
    "CCFlag = TrainModel['CCFlag']\n",
    "CCFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param\n",
    "time.sleep(30)\n",
    "\n",
    "#BASE_PATH = '/notebooks/notebooks/EEF_NER_LSTM_Training/Train_models/'\n",
    "BASE_PATH = '/notebooks/Train_models/'\n",
    "\n",
    "PREDICTION_FORMAT_DETAILED = 1\n",
    "# model_path = os.path.join(BASE_PATH, TrainModel['model_id'])\n",
    "# print(' model_path ...',model_path)\n",
    "# print('files under model_path ...',os.listdir(model_path))\n",
    "#model_type = eval(os.getenv('model_type','NOT_SPECIFIED'))[0]\n",
    "model_type = eval(os.getenv('model_type','NOT_SPECIFIED'))\n",
    "print(\"model_type\",model_type)\n",
    "\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    model_path = os.path.join(BASE_PATH, TrainModel['model_id'])\n",
    "    if 'NER' in model_type:\n",
    "        model_file_path = model_path+'/valid_char_set.pkl'\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "\n",
    "    if 'DC' in model_type:\n",
    "        file_path = '%s_document_classification_svc_model.pkl' % TrainModel['model_id']\n",
    "        model_file_path = model_path+'/'+file_path\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "\n",
    "    if 'IC' in model_type:\n",
    "        model_file_path = model_path+'/model_complete.h5'\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "        \n",
    "    print(\"BASE_PATH......\",BASE_PATH)\n",
    "    print(\"PREDICTION_FORMAT_DETAILED......\",PREDICTION_FORMAT_DETAILED)\n",
    "    print(\"model_id......\",TrainModel['model_id'])\n",
    "    print(\"model_path......\",model_path)\n",
    "\n",
    "    \n",
    "    # Loop added to wait till model proccess completes\n",
    "    while not os.path.exists(model_file_path):\n",
    "        time.sleep(1)\n",
    "\n",
    "    if os.path.isfile(model_file_path):\n",
    "        print(\"recived new files\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        raise ValueError(\"%s isn't a file!\" % file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model register and deployment\n",
    "\n",
    "# Import modules\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "from mosaicml import *\n",
    "from mosaicml import list_models, describe_model, register_model, load_model, deploy_model, scoring_func, generate_schema, stop_model\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.utils import Progbar\n",
    "from keras.models import load_model as keras_load_model\n",
    "import networkx as nx\n",
    "from collections import OrderedDict\n",
    "from mosaicml.constants import MLModelFlavours\n",
    "from EEF_NER_LSTM_Training.services.ai.image_classify import preprocessing as img_prepro\n",
    "from EEF_NER_LSTM_Training.services.ai.object_detection.utils import label_map_util\n",
    "from EEF_NER_LSTM_Training.services.ai.object_detection import inference as infer_zoning\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from mosaicml import update_existing_model\n",
    "from mosaicml import *\n",
    "image_config = app_util.get_system_config()[3]\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CCFlag',CCFlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suporting files\n",
    "# if len(model_type)==1:\n",
    "\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    print(\"single model\")\n",
    "    if 'NER' in model_type and CCFlag == 'False':\n",
    "        word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "        label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "        case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "        char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "        valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "        print(\"NER model path...\",model_path)\n",
    "\n",
    "    if 'NER' in model_type and CCFlag == 'True':\n",
    "        word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "        label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "        case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "        char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "        valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "        svc_clf_file_name = '%s_content_classification_model.pkl' % TrainModel['model_id']\n",
    "        tf_idf_model_file_name = '%s_content_classification_tf_idf_model.pkl' % TrainModel['model_id']\n",
    "        svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "        tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "        print(\"svc_clf...\",model_path+'/'+svc_clf_file_name)\n",
    "        print(\"tf_idf_model...\",model_path+'/'+tf_idf_model_file_name)\n",
    "\n",
    "    if 'DC' in model_type:\n",
    "        svc_clf_file_name = '%s_document_classification_svc_model.pkl' % TrainModel['model_id']\n",
    "        tf_idf_model_file_name = '%s_document_classification_tf_idf_model.pkl' % TrainModel['model_id']\n",
    "        svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "        tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "        print(\"svc_clf...\",model_path+'/'+svc_clf_file_name)\n",
    "        print(\"tf_idf_model...\",model_path+'/'+tf_idf_model_file_name)\n",
    "\n",
    "    if 'IC' in model_type:\n",
    "        class_encoder = pkl.load(open(model_path+'/'+'image_classes_encoder.pkl','rb'))\n",
    "        print(\"class_encoder...\",model_path+'/'+'image_classes_encoder.pkl')\n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        zoning_file_label_path = model_path+'/'+'label_map.pbtxt'\n",
    "        label_map = label_map_util.load_labelmap(zoning_file_label_path)\n",
    "        num_classes = len(label_map.item)\n",
    "        categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "        category_index = label_map_util.create_category_index(categories)\n",
    "        print(\"zoning_file_label_path...\",zoning_file_label_path)\n",
    "        print(\"category_index...\",category_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents_and_words(text):\n",
    "    \"\"\"\n",
    "    Split the text into sentences containing list of tokens.\n",
    "\n",
    "    Arguments :\n",
    "    text    - input text as a string.\n",
    "\n",
    "    Returns :\n",
    "    List of sentences where each sentence is further a list of tokens.\n",
    "    \"\"\"\n",
    "    content = text\n",
    "    content = re.sub(r\"(?<= [.(a-zA-z]{3})\\.(?!=(\\n))\", ' ', content)\n",
    "    content = re.sub(r\"(?<= [a-zA-z]{2})\\.(?!=(\\n))\", ' ', content)\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for sent in sent_tokenize(content):\n",
    "        for word in word_tokenize(sent):\n",
    "            # Fix nltk word tokenize issue for double quotes.\n",
    "            word = word.replace(\"''\", '\"').replace('``', '\"')\n",
    "            sentence.append([word, word])\n",
    "\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def add_char_informatioin_evaluate(sentences_in):\n",
    "    \"\"\"Add character level features to words of sentences.\"\"\"\n",
    "    sentences = sentences_in.copy()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            sentences[i][j] = [data[0], chars]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def padding(sentences_in, maxlen=52):\n",
    "    \"\"\"Pad sequences upto max length specified or the max length in data.\"\"\"\n",
    "    sentences = sentences_in.copy()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentences[i][2] = pad_sequences(sentences[i][2],\n",
    "                                        maxlen,\n",
    "                                        padding='post')\n",
    "    return sentences\n",
    "\n",
    "def create_data_matrices(sentences, word2idx, case2idx, char2idx):\n",
    "    \"\"\"\n",
    "    Create numerical arrays for textual data.\n",
    "\n",
    "    Arguments :\n",
    "    sentences   - list of sentences containing word tokens along with\n",
    "                  word and characterfs level features.\n",
    "    word2idx    - dictionary containing word to id mapping.\n",
    "    case2idx    - dictionary containing case to id mapping.\n",
    "    char2idx    - dictionary containing character to id mapping.\n",
    "\n",
    "    Returns :\n",
    "    Dataset with text replaced with numerical ids.\n",
    "    \"\"\"\n",
    "    unknown_idx = word2idx['UNKNOWN_TOKEN']\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    word_count = 0\n",
    "    unknown_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_indices = []\n",
    "        case_indices = []\n",
    "        char_indices = []\n",
    "\n",
    "        for word, char in sentence:\n",
    "            word_count += 1\n",
    "            if word in word2idx:\n",
    "                word_idx = word2idx[word]\n",
    "            elif word.lower() in word2idx:\n",
    "                word_idx = word2idx[word.lower()]\n",
    "            else:\n",
    "                word_idx = unknown_idx\n",
    "                unknown_word_count += 1\n",
    "            char_idx = []\n",
    "            for x in char:\n",
    "                char_idx.append(char2idx[x])\n",
    "            # Get the label and map to int\n",
    "            word_indices.append(word_idx)\n",
    "            case_indices.append(get_casing(word, case2idx))\n",
    "            char_indices.append(char_idx)\n",
    "\n",
    "        dataset.append([word_indices, case_indices, char_indices])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_casing(word, case_lookup):\n",
    "    \"\"\"Return the case id for given word from case lookup provided.\"\"\"\n",
    "    casing = 'other'\n",
    "    if len(word) == 0:\n",
    "        return case_lookup[casing]\n",
    "    num_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_digits += 1\n",
    "\n",
    "    digit_fraction = num_digits / float(len(word))\n",
    "\n",
    "    if word.isdigit():  # Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digit_fraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower():  # All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper():  # All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif num_digits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    return case_lookup[casing]\n",
    "\n",
    "def tag_dataset_evaluate(dataset, model):\n",
    "    \"\"\"\n",
    "    Get the inference of the given model on the given dataset.\n",
    "\n",
    "    Arguments :\n",
    "    dataset  - list/tuple containing data the format (tokens, casing, char).\n",
    "    model    - model to be used for prediction.\n",
    "\n",
    "    Returns :\n",
    "        Tuple of predicted labels and respective scores\n",
    "    \"\"\"\n",
    "    pred_labels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    score = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, casing, char = data\n",
    "        tokens = np.asarray([tokens])\n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "        score = score + list(pred.max(axis=-1))\n",
    "        pred_index = pred.argmax(axis=-1)\n",
    "        pred_labels.append(pred_index)\n",
    "        b.update(i)\n",
    "    return pred_labels, score\n",
    "\n",
    "def get_inference_run(input_data_dict, prediction_format=0, model={}, k_graph=None, entity_order_map=None, root_level_ents=None, use_roi=False):\n",
    "    \"\"\"\n",
    "    Run the inference and return the predictions.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model    - dictionary with keras model available as key \"model\"\n",
    "    text     - input to the model\n",
    "    prediction_format - if set to 1, returns detailed format.\n",
    "                        By default it is set to 0.\n",
    "    use_roi  - boolean specifying the use of content classification\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations_result = []\n",
    "    # Added for Content Classification\n",
    "#     roi_input = []\n",
    "#     for page_number, value in input_data_dict.items():\n",
    "#         page_text = \"\"\n",
    "#         for sentence_num in sorted(value.keys(),\n",
    "#                                    key=lambda x: int(x)):\n",
    "#             page_text = page_text + \" \" + value[sentence_num]\n",
    "#         roi_input.append([page_number, page_text])\n",
    "#     print('roi input', roi_input)\n",
    "#     roi_page_numbers = get_region_of_interest(\n",
    "#         model['model'], roi_input)\n",
    "#     print('valid page numbers', roi_page_numbers)\n",
    "    \n",
    "    for page_no in sorted(input_data_dict.keys(),\n",
    "                          key=lambda x: int(x)):\n",
    "#         if page_no not in valid_page_numbers:\n",
    "#             continue\n",
    "        for sent_no in sorted(input_data_dict[page_no].keys(),\n",
    "                              key=lambda x: int(x)):\n",
    "            temp_array = []\n",
    "            text = input_data_dict[page_no][sent_no][\"sentence\"]\n",
    "            unique_id = input_data_dict[page_no][sent_no][\"unique_id\"]\n",
    "            text = remove_non_ascii_2(text)\n",
    "            copied_variable_text = text\n",
    "            text = tokenize_sents_and_words(text)\n",
    "            text = add_char_informatioin_evaluate(text)\n",
    "            test_text = text\n",
    "            text = padding(create_data_matrices(text,word2idx, case2idx,char2idx))\n",
    "            test_batch, test_batch_len = create_batches(text)\n",
    "            predLabels, sentences_score = tag_dataset_evaluate(test_batch,model)\n",
    "            entities = []\n",
    "            for i in predLabels:\n",
    "                for v in i:\n",
    "                    entities.append(list(label2idx.keys())[\n",
    "                        list(label2idx.values()).index(v)])\n",
    "            keyindex = 0\n",
    "            for sent in test_text:\n",
    "                word_index = 0\n",
    "                for word in sent:\n",
    "                    temp = {}\n",
    "                    temp[word[0]] = re.sub('\\n', '', entities[keyindex])\n",
    "                    temp_array.append((temp, sentences_score[word_index]))\n",
    "                    keyindex = keyindex + 1\n",
    "                    word_index = word_index + 1\n",
    "\n",
    "            annotations = []\n",
    "            if prediction_format == PREDICTION_FORMAT_DETAILED:\n",
    "                annotations = prediction_index_format(temp_array,\n",
    "                                                      copied_variable_text,\n",
    "                                                      page_no,\n",
    "                                                      sent_no,\n",
    "                                                      unique_id)\n",
    "            else:\n",
    "                annotations = temp_array\n",
    "\n",
    "            if len(annotations) > 0:\n",
    "                annotations_result.extend(annotations)\n",
    "    print(\"annotations_result \",annotations_result,type(annotations_result))\n",
    "    print(\"k_graph \",k_graph,type(k_graph))\n",
    "    print(\"entity_order_map \",entity_order_map,type(entity_order_map))\n",
    "    print(\"root_level_ents \",root_level_ents,type(root_level_ents))\n",
    "    annotations_result = extract_entity_relation(\n",
    "        annotations_result, k_graph, entity_order_map, root_level_ents)\n",
    "    return annotations_result\n",
    "\n",
    "def get_region_of_interest_AIL(tf_idf_model,svc_clf, file_data):\n",
    "    \n",
    "    file_data_df = pd.DataFrame(file_data, columns=['page_number', 'page_data'])\n",
    "    transformed_vector_text = tf_idf_model.transform(file_data_df['page_data'])\n",
    "    final_pred = svc_clf.predict(transformed_vector_text)\n",
    "    pred_probability = svc_clf.predict_proba(transformed_vector_text)\n",
    "\n",
    "    file_data_df['classification_flag'] = final_pred\n",
    "    file_data_df['zero_probability'] = pred_probability[:, 0]\n",
    "    file_data_df['final_classification_flag'] = np.where(\n",
    "        ((file_data_df.zero_probability < CONFIG['CC_ZERO_PROB_THRESHOLD'])\n",
    "         & (file_data_df.classification_flag == 0)),\n",
    "        1,\n",
    "        file_data_df.classification_flag)\n",
    "    #print(file_data_df)\n",
    "    classified_pages = file_data_df[\n",
    "        file_data_df['final_classification_flag'] == 1].page_number.tolist()\n",
    "\n",
    "    return classified_pages\n",
    "\n",
    "def get_inference_run_CC(input_data_dict,tf_idf_model,svc_clf,prediction_format=0, model={}, k_graph=None, entity_order_map=None, root_level_ents=None, use_roi=False):\n",
    "    \"\"\"\n",
    "    Run the inference and return the predictions.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model    - dictionary with keras model available as key \"model\"\n",
    "    text     - input to the model\n",
    "    prediction_format - if set to 1, returns detailed format.\n",
    "                        By default it is set to 0.\n",
    "    use_roi  - boolean specifying the use of content classification\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations_result = []\n",
    "    # Added for Content Classification\n",
    "    roi_input = []\n",
    "    for page_number, value in input_data_dict.items():\n",
    "        page_text = \"\"\n",
    "        for sentence_num in sorted(value.keys(),\n",
    "                                   key=lambda x: int(x)):\n",
    "            page_text = page_text + \" \" + value[sentence_num]['sentence']\n",
    "            # page_text_without_asc = remove_non_ascii_2(value[sentence_num]['sentence'])\n",
    "            # page_text = page_text + \" \" + page_text_without_asc\n",
    "        roi_input.append([page_number, page_text])\n",
    "    # print('roi input', roi_input)\n",
    "    roi_page_numbers = get_region_of_interest_AIL(tf_idf_model,svc_clf, roi_input)\n",
    "    # print('valid page numbers', roi_page_numbers)\n",
    "    \n",
    "    for page_no in sorted(input_data_dict.keys(),\n",
    "                          key=lambda x: int(x)):\n",
    "        if page_no not in roi_page_numbers:\n",
    "            continue\n",
    "        for sent_no in sorted(input_data_dict[page_no].keys(),\n",
    "                              key=lambda x: int(x)):\n",
    "            temp_array = []\n",
    "            text = input_data_dict[page_no][sent_no][\"sentence\"]\n",
    "            unique_id = input_data_dict[page_no][sent_no][\"unique_id\"]\n",
    "            text = remove_non_ascii_2(text)\n",
    "            copied_variable_text = text\n",
    "            text = tokenize_sents_and_words(text)\n",
    "            text = add_char_informatioin_evaluate(text)\n",
    "            test_text = text\n",
    "            text = padding(create_data_matrices(text,word2idx, case2idx,char2idx))\n",
    "            test_batch, test_batch_len = create_batches(text)\n",
    "            predLabels, sentences_score = tag_dataset_evaluate(test_batch,model)\n",
    "            entities = []\n",
    "            for i in predLabels:\n",
    "                for v in i:\n",
    "                    entities.append(list(label2idx.keys())[\n",
    "                        list(label2idx.values()).index(v)])\n",
    "            keyindex = 0\n",
    "            for sent in test_text:\n",
    "                word_index = 0\n",
    "                for word in sent:\n",
    "                    temp = {}\n",
    "                    temp[word[0]] = re.sub('\\n', '', entities[keyindex])\n",
    "                    temp_array.append((temp, sentences_score[word_index]))\n",
    "                    keyindex = keyindex + 1\n",
    "                    word_index = word_index + 1\n",
    "\n",
    "            annotations = []\n",
    "            if prediction_format == PREDICTION_FORMAT_DETAILED:\n",
    "                annotations = prediction_index_format(temp_array,\n",
    "                                                      copied_variable_text,\n",
    "                                                      page_no,\n",
    "                                                      sent_no,\n",
    "                                                      unique_id)\n",
    "            else:\n",
    "                annotations = temp_array\n",
    "\n",
    "            if len(annotations) > 0:\n",
    "                annotations_result.extend(annotations)\n",
    "    annotations_result = extract_entity_relation(\n",
    "        annotations_result, k_graph, entity_order_map, root_level_ents)\n",
    "    return annotations_result\n",
    "\n",
    "def remove_non_ascii_2(text):\n",
    "    \"\"\"Return ASCII version of the text.\"\"\"\n",
    "    return re.sub(r'[^\\x1F-\\x7F]|[`]', ' ', text)\n",
    "\n",
    "\n",
    "def create_batches(data):\n",
    "    \"\"\"\n",
    "    Create batches by groping together same length records.\n",
    "\n",
    "    Arguments :\n",
    "    data   - List of data in the format format (tokens, casing, char)\n",
    "\n",
    "    Returns :\n",
    "    data along with list containing running batch lengths of different batches.\n",
    "    \"\"\"\n",
    "    unique_len_of_data = []\n",
    "    for i in data:\n",
    "        unique_len_of_data.append(len(i[0]))\n",
    "    unique_len_of_data = set(unique_len_of_data)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in unique_len_of_data:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches, batch_len\n",
    "\n",
    "# New code to fix index out of list issue - Pranav\n",
    "\n",
    "def prediction_index_format(standard_prediction, plain_text, page_no, sent_no, unique_id):\n",
    "    \"\"\"\n",
    "    Add start and end index to every word in model prediction.\n",
    "\n",
    " \n",
    "\n",
    "    Arguments:\n",
    "        standard_prediction -  standard output of the model\n",
    "        plain text:  input text given to the model\n",
    "    Returns :\n",
    "        Json of words with starting,ending index and confidence score\n",
    "    \"\"\"\n",
    "    # merge IOB data\n",
    "    print(\"standard Prediction ----------\",standard_prediction)\n",
    "    iob_data = []\n",
    "    for prediction_with_score in standard_prediction:\n",
    "        prediction = prediction_with_score[0]\n",
    "        word, annotation = list(prediction.items())[0]\n",
    "        iob_data.append((word, annotation, annotation))\n",
    "\n",
    " \n",
    "\n",
    "    ne_tree = conlltags2tree(iob_data)\n",
    "    print(\"-------------ne_tree--------\",ne_tree)\n",
    "    annotations = []\n",
    "    plain_text_length = len(plain_text)\n",
    "    starting_index = 0\n",
    "    character_count = 0\n",
    "    found_length = 0\n",
    "    annotations = []\n",
    "\n",
    " \n",
    "\n",
    "    standard_prediction_index = 0\n",
    "    for x in ne_tree:\n",
    "        if isinstance(x, nltk.tree.Tree):\n",
    "            temp_dict = {}\n",
    "\n",
    " \n",
    "\n",
    "            temp_dict['annotation'] = x.label()\n",
    "            temp_dict['page_no'] = page_no\n",
    "            temp_dict['sentence_no'] = sent_no\n",
    "            temp_dict['unique_id'] = unique_id\n",
    "            temp_word_str = ''\n",
    "            temp_starting_index = []\n",
    "            temp_end_position = []\n",
    "            temp_score = []\n",
    "            temp_word_list = []\n",
    "            for token, pos in x.leaves():\n",
    "                standard_prediction_index += 1\n",
    "                temp_word = ''\n",
    "                space = ''\n",
    "\n",
    " \n",
    "\n",
    "                for index in range(starting_index, plain_text_length):\n",
    "                    temp_word = temp_word + plain_text[index]\n",
    "                    if plain_text[index] == \" \":\n",
    "                        temp_word = ''\n",
    "                        space = ' '\n",
    "                        found_length = index - 1\n",
    "                        starting_index = index + 1\n",
    "                    if (temp_word == token):\n",
    "                        word_length = len(token)\n",
    "                        found_length = index - word_length\n",
    "\n",
    " \n",
    "\n",
    "                        temp_word_str = temp_word_str + space + token\n",
    "                        space = ''\n",
    "\n",
    " \n",
    "\n",
    "                        temp_word_list.append(token)\n",
    "                        temp_starting_index.append(found_length + 1)\n",
    "                        temp_end_position.append(found_length + word_length)\n",
    "                        try: #str(standard_prediction[standard_prediction_index][1]):\n",
    "                            temp_score.append(str(standard_prediction[standard_prediction_index][1]))\n",
    "                        except:\n",
    "                            temp_score.append(\"0\")\n",
    "\n",
    " \n",
    "\n",
    "                        temp_word = ''\n",
    "                        starting_index = index + 1\n",
    "                        break\n",
    "                    character_count = character_count + 1\n",
    "            temp_dict['word'] = temp_word_str.strip()\n",
    "            temp_dict['word_list'] = temp_word_list\n",
    "            temp_dict['starting_index'] = temp_starting_index\n",
    "            temp_dict['end_position'] = temp_end_position\n",
    "            temp_dict['score'] = temp_score\n",
    "            temp_dict['word_index'] = []\n",
    "            annotations.append(temp_dict)\n",
    "\n",
    " \n",
    "\n",
    "    # word index for entities\n",
    "    sent_word_token = custom_word_tokinezer(plain_text)\n",
    "\n",
    " \n",
    "\n",
    "    pad_word_index = 0\n",
    "    print(\"Pred_ind_format annotations\",annotations,type(annotations))\n",
    "    for annotation in annotations:\n",
    "        tmp_word_list = annotation['word_list']\n",
    "\n",
    " \n",
    "\n",
    "        for word_token in tmp_word_list:\n",
    "            temp_word_index = find_element_in_list(word_token, sent_word_token)\n",
    "\n",
    " \n",
    "\n",
    "            if temp_word_index is not None:\n",
    "                sent_word_token = sent_word_token[temp_word_index:]\n",
    "                pad_word_index += temp_word_index\n",
    "                annotation['word_index'].append(pad_word_index)\n",
    "\n",
    " \n",
    "\n",
    "    return annotations\n",
    "\n",
    "# old code with index out of list issue\n",
    "# def prediction_index_format(standard_prediction, plain_text, page_no, sent_no, unique_id):\n",
    "#     \"\"\"\n",
    "#     Add start and end index to every word in model prediction.\n",
    "\n",
    "#     Arguments:\n",
    "#         standard_prediction -  standard output of the model\n",
    "#         plain text:  input text given to the model\n",
    "#     Returns :\n",
    "#         Json of words with starting,ending index and confidence score\n",
    "#     \"\"\"\n",
    "#     # merge IOB data\n",
    "#     iob_data = []\n",
    "#     for prediction_with_score in standard_prediction:\n",
    "#         prediction = prediction_with_score[0]\n",
    "#         word, annotation = list(prediction.items())[0]\n",
    "#         iob_data.append((word, annotation, annotation))\n",
    "\n",
    "#     ne_tree = conlltags2tree(iob_data)\n",
    "\n",
    "#     annotations = []\n",
    "#     plain_text_length = len(plain_text)\n",
    "#     starting_index = 0\n",
    "#     character_count = 0\n",
    "#     found_length = 0\n",
    "#     annotations = []\n",
    "\n",
    "#     standard_prediction_index = 0\n",
    "#     for x in ne_tree:\n",
    "#         if isinstance(x, nltk.tree.Tree):\n",
    "#             temp_dict = {}\n",
    "\n",
    "#             temp_dict['annotation'] = x.label()\n",
    "#             temp_dict['page_no'] = page_no\n",
    "#             temp_dict['sentence_no'] = sent_no\n",
    "#             temp_dict['unique_id'] = unique_id\n",
    "#             temp_word_str = ''\n",
    "#             temp_starting_index = []\n",
    "#             temp_end_position = []\n",
    "#             temp_score = []\n",
    "#             temp_word_list = []\n",
    "#             for token, pos in x.leaves():\n",
    "#                 standard_prediction_index += 1\n",
    "#                 temp_word = ''\n",
    "#                 space = ''\n",
    "\n",
    "#                 for index in range(starting_index, plain_text_length):\n",
    "#                     temp_word = temp_word + plain_text[index]\n",
    "#                     if plain_text[index] == \" \":\n",
    "#                         temp_word = ''\n",
    "#                         space = ' '\n",
    "#                         found_length = index - 1\n",
    "#                         starting_index = index + 1\n",
    "#                     if (temp_word == token):\n",
    "#                         word_length = len(token)\n",
    "#                         found_length = index - word_length\n",
    "\n",
    "#                         temp_word_str = temp_word_str + space + token\n",
    "#                         space = ''\n",
    "\n",
    "#                         temp_word_list.append(token)\n",
    "#                         temp_starting_index.append(found_length + 1)\n",
    "#                         temp_end_position.append(found_length + word_length)\n",
    "#                         temp_score.append(str(standard_prediction[\n",
    "#                             standard_prediction_index][1]))\n",
    "\n",
    "#                         temp_word = ''\n",
    "#                         starting_index = index + 1\n",
    "#                         break\n",
    "#                     character_count = character_count + 1\n",
    "#             temp_dict['word'] = temp_word_str.strip()\n",
    "#             temp_dict['word_list'] = temp_word_list\n",
    "#             temp_dict['starting_index'] = temp_starting_index\n",
    "#             temp_dict['end_position'] = temp_end_position\n",
    "#             temp_dict['score'] = temp_score\n",
    "#             temp_dict['word_index'] = []\n",
    "#             annotations.append(temp_dict)\n",
    "\n",
    "#     # word index for entities\n",
    "#     sent_word_token = custom_word_tokinezer(plain_text)\n",
    "\n",
    "#     pad_word_index = 0\n",
    "#     for annotation in annotations:\n",
    "#         tmp_word_list = annotation['word_list']\n",
    "\n",
    "#         for word_token in tmp_word_list:\n",
    "#             temp_word_index = find_element_in_list(word_token, sent_word_token)\n",
    "\n",
    "#             if temp_word_index is not None:\n",
    "#                 sent_word_token = sent_word_token[temp_word_index:]\n",
    "#                 pad_word_index += temp_word_index\n",
    "#                 annotation['word_index'].append(pad_word_index)\n",
    "\n",
    "#     return annotations\n",
    "\n",
    "\n",
    "def custom_word_tokinezer(plain_text):\n",
    "    \"\"\"Tokenize text while preserving the white spaces.\"\"\"\n",
    "    tokens = word_tokenize(plain_text)\n",
    "\n",
    "    # Handle nltk tokenizer issue with double quotes.\n",
    "    tokens = [x.replace(\"''\", '\"').replace('``', '\"') for x in tokens]\n",
    "    plain_text_length = len(plain_text)\n",
    "    starting_index = 0\n",
    "\n",
    "    word_token_list = []\n",
    "\n",
    "    for token in tokens:\n",
    "        temp_word = ''\n",
    "        for index in range(starting_index, plain_text_length):\n",
    "            temp_word = temp_word + plain_text[index]\n",
    "            if re.match(r\"[\\s]+\", plain_text[index]):\n",
    "                word_token_list.append(plain_text[index])\n",
    "                temp_word = ''\n",
    "            if (temp_word == token):\n",
    "                word_token_list.append(temp_word)\n",
    "                temp_word = ''\n",
    "                starting_index = index + 1\n",
    "                break\n",
    "\n",
    "    return word_token_list\n",
    "\n",
    "\n",
    "\n",
    "def find_element_in_list(element, list_element):\n",
    "    \"\"\"Find element in list of elements.\"\"\"\n",
    "    try:\n",
    "        index_element = list_element.index(element)\n",
    "        return index_element\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_entity_relation(ner_model_output, knowledge_base,\n",
    "                            entity_order_map, root_level_ents=None):\n",
    "    \"\"\"Format the output of model as a knowledge graph.\"\"\"\n",
    "    di_rel_graph = nx.from_edgelist(knowledge_base, create_using=nx.DiGraph())\n",
    "    valid_entities = set(entity_order_map.keys())\n",
    "    entity_level_map = get_group_mapping(di_rel_graph)\n",
    "    model_output_queue = ner_model_output[::-1]\n",
    "    '''\n",
    "    Commented out this for bug and fix added in loop below\n",
    "    if len(ner_model_output)>0 and root_level_ents==None:\n",
    "        found_valid_entity = False\n",
    "        while not found_valid_entity:\n",
    "            first_element = model_output_queue.pop()\n",
    "            if first_element['annotation'] in valid_entities:\n",
    "                found_valid_entity = True\n",
    "    '''\n",
    "    if len(ner_model_output)>0 and root_level_ents != None:\n",
    "        found_valid_entity = False\n",
    "        while not found_valid_entity:\n",
    "            for element in model_output_queue:\n",
    "                if element['annotation'] in root_level_ents:\n",
    "                    first_element = element\n",
    "                    model_output_queue.remove(element)\n",
    "                    found_valid_entity = True\n",
    "                    break\n",
    "    \n",
    "        global_ents_inverse = set(y for x in knowledge_base for y in x)\n",
    "        output = OrderedDict()\n",
    "        entity_type_vs_key_mapping = []\n",
    "        output[first_element['word']] = {'attribute_name': first_element[\n",
    "            'annotation']}\n",
    "        output[first_element['word']]['annotation'] = first_element\n",
    "        output[first_element['word']]['attribute_type'] = entity_level_map.get(\n",
    "            first_element['annotation'], 'global')\n",
    "        output[first_element['word']]['value'] = first_element['word']\n",
    "        output[first_element['word']]['order'] = entity_order_map.get(\n",
    "            first_element['annotation'])\n",
    "        entity_type_vs_key_mapping.append(\n",
    "            (first_element['annotation'], (first_element['word'],)))\n",
    "\n",
    "        dummy_var_count = 1\n",
    "        safety_counter = 0\n",
    "        threshold = 10000\n",
    "\n",
    "        while len(model_output_queue) > 0:\n",
    "            if safety_counter >= threshold:\n",
    "                break\n",
    "            safety_counter += 1\n",
    "            entity = model_output_queue.pop()\n",
    "            curr_entity_value = entity['word']\n",
    "            curr_entity_type = entity['annotation']\n",
    "\n",
    "            if curr_entity_type not in valid_entities:\n",
    "                continue\n",
    "\n",
    "            curr_key = curr_entity_value\n",
    "            if curr_entity_type not in global_ents_inverse:\n",
    "                output[curr_key] = {'attribute_name': curr_entity_type}\n",
    "                output[curr_key]['annotation'] = entity\n",
    "                output[curr_key]['attribute_type'] = \"global\"\n",
    "                output[curr_key]['value'] = curr_entity_value\n",
    "                output[curr_key]['order'] = entity_order_map.get(curr_entity_type)\n",
    "                continue\n",
    "\n",
    "            mapping_loop_over_copy = copy.deepcopy(entity_type_vs_key_mapping)\n",
    "            for i in range(len(entity_type_vs_key_mapping)):\n",
    "                prev_entity_type, prev_entity_keys = entity_type_vs_key_mapping[-1]\n",
    "                #added below line to resolve single quote issue\n",
    "                prev_entity_keys = tuple([prev_entity_keys[0].replace(\"'\", \"\\\\\\'\")])\n",
    "                print(\"prev_entity_keys  \",prev_entity_keys)\n",
    "                \n",
    "                match_found = False\n",
    "                if di_rel_graph.has_successor(prev_entity_type, curr_entity_type):\n",
    "                    match_found = True\n",
    "                    prev_key = \"['\" + \"']['\".join(prev_entity_keys) + \"']\"\n",
    "                    qry_e_type = '{}{}[\"\"\"{}\"\"\"]={}'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        {'attribute_name': curr_entity_type})\n",
    "                    qry_annotation = '{}{}[\"\"\"{}\"\"\"][\"annotation\"]={}'.format(\n",
    "                        'output', prev_key, curr_key, 'entity')\n",
    "                    qry_attribute_type = '{}{}[\"\"\"{}\"\"\"][\"attribute_type\"]=\"{}\"'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        entity_level_map[curr_entity_type])\n",
    "                    qry_value = '{}{}[\"\"\"{}\"\"\"][\"value\"]=\"\"\"{}\"\"\"'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        \"\" if (int(entity['page_no']) == 0) else curr_entity_value)\n",
    "                    qry_order = '{}{}[\"\"\"{}\"\"\"][\"order\"]=\"{}\"'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        entity_order_map.get(curr_entity_type))\n",
    "                    exec(qry_e_type)\n",
    "                    exec(qry_annotation)\n",
    "                    exec(qry_attribute_type)\n",
    "                    exec(qry_value)\n",
    "                    exec(qry_order)\n",
    "                    entity_type_vs_key_mapping.append(\n",
    "                        (curr_entity_type, prev_entity_keys + (curr_key,)))\n",
    "                    break\n",
    "                entity_type_vs_key_mapping.pop()\n",
    "            if not match_found:\n",
    "                if len(tuple(di_rel_graph.predecessors(curr_entity_type))) == 0:\n",
    "                    entity_type_vs_key_mapping.append(\n",
    "                        (curr_entity_type, (curr_key,)))\n",
    "                    output[curr_key] = {'attribute_name': curr_entity_type}\n",
    "                    output[curr_key]['annotation'] = entity\n",
    "                    output[curr_key]['attribute_type'] = entity_level_map.get(\n",
    "                        output[curr_key]['attribute_name'])\n",
    "                    output[curr_key]['value'] = (\"\" if int(entity[\n",
    "                        'page_no']) == 0 else curr_entity_value)\n",
    "                    output[curr_key]['order'] = entity_order_map.get(\n",
    "                        curr_entity_type)\n",
    "                else:\n",
    "                    dummy_key = 'dummy_' + str(dummy_var_count)\n",
    "                    dummy_entity = create_dummy_entity(\n",
    "                        dummy_entity_name=dummy_key,\n",
    "                        curr_entity_type=curr_entity_type,\n",
    "                        di_rel_graph=di_rel_graph)\n",
    "                    dummy_var_count += 1\n",
    "                    model_output_queue.append(entity)\n",
    "                    model_output_queue.append(dummy_entity)\n",
    "                    entity_type_vs_key_mapping = mapping_loop_over_copy\n",
    "\n",
    "        output_formatted = []\n",
    "        for key, value in output.items():\n",
    "            output_formatted.append(value)\n",
    "        for d in output_formatted:\n",
    "            recursive_dict_iterator(d)\n",
    "        # Fill in the blank entities in the output and sort as per sort order\n",
    "        fill_blank_entities_in_k_graph(\n",
    "            output_formatted, di_rel_graph, entity_level_map,\n",
    "            entity_order_map, root_level_ents)\n",
    "        \n",
    "    else:\n",
    "        output_formatted = []\n",
    "        fill_blank_entities_in_k_graph(\n",
    "            output_formatted, di_rel_graph, entity_level_map,\n",
    "            entity_order_map, root_level_ents)\n",
    "\n",
    "    return output_formatted\n",
    "\n",
    "def create_dummy_entity(dummy_entity_name, curr_entity_type, di_rel_graph):\n",
    "    \"\"\"Create dummy entity for extract_entity_relation function.\"\"\"\n",
    "    dummy_entity_type = tuple(\n",
    "        di_rel_graph.predecessors(curr_entity_type))[0]\n",
    "    dummy_entity = {\n",
    "        \"annotation\": dummy_entity_type,\n",
    "        \"page_no\": \"0\",\n",
    "        \"sentence_no\": \"0\",\n",
    "        \"word\": dummy_entity_name,\n",
    "        \"word_list\": [dummy_entity_name],\n",
    "        \"starting_index\": [0],\n",
    "        \"end_position\": [0],\n",
    "        \"score\": [\"0.00\"],\n",
    "        \"word_index\": [0]\n",
    "    }\n",
    "    return dummy_entity\n",
    "\n",
    "\n",
    "def get_group_mapping(di_rel_graph):\n",
    "    \"\"\"Create node to group/subgroup/local map, given a directed graph.\"\"\"\n",
    "    group_map_from_rel = {}\n",
    "    for node in di_rel_graph.nodes():\n",
    "        predecesspr_count = len(tuple(di_rel_graph.predecessors(node)))\n",
    "        successor_count = len(tuple(di_rel_graph.successors(node)))\n",
    "        if successor_count == 0:\n",
    "            group_map_from_rel[node] = 'local'\n",
    "            continue\n",
    "        if (successor_count > 0) and (predecesspr_count) == 0:\n",
    "            group_map_from_rel[node] = 'group'\n",
    "            continue\n",
    "        group_map_from_rel[node] = 'subgroup'\n",
    "    return group_map_from_rel\n",
    "\n",
    "\n",
    "def recursive_dict_iterator(d_in):\n",
    "    \"\"\"Recursively insert attribute_list in dictionaries having hierarchies.\"\"\"\n",
    "    data_keys = [x for x in d_in.keys() if x not in [\n",
    "        'annotation', 'type', 'value', 'attribute_type',\n",
    "        'attribute_name', 'attribute_list', 'order']]\n",
    "    if len(data_keys) > 0:\n",
    "        attrib_list = []\n",
    "        for key in data_keys:\n",
    "            attrib_list.append(d_in[key])\n",
    "            del d_in[key]\n",
    "        d_in['attribute_list'] = attrib_list\n",
    "        for sub_d in d_in['attribute_list']:\n",
    "            recursive_dict_iterator(sub_d)\n",
    "\n",
    "\n",
    "def create_blank_entity(entity_name, attrib_list_req,\n",
    "                        entity_level_map, entity_order_map):\n",
    "    \"\"\"Create blank entity if not predicted by model.\"\"\"\n",
    "    attribute_type = entity_level_map.get(entity_name, 'global')\n",
    "    ent = {\n",
    "        \"attribute_name\": entity_name,\n",
    "        \"annotation\": {\n",
    "            \"annotation\": entity_name,\n",
    "            \"word\": \"\",\n",
    "            \"word_index\": [],\n",
    "            \"word_list\": [],\n",
    "            \"starting_index\": [],\n",
    "            \"sentence_no\": \"0\",\n",
    "            \"score\": \"0.00\",\n",
    "            \"end_position\": [],\n",
    "            \"page_no\": \"0\"\n",
    "        },\n",
    "        \"attribute_type\": attribute_type,\n",
    "        \"value\": \"\",\n",
    "        \"order\": entity_order_map.get(entity_name)}\n",
    "    if attrib_list_req:\n",
    "        ent['attribute_list'] = []\n",
    "    return ent\n",
    "\n",
    "\n",
    "def create_blank_hierarchy(entity_name, di_rel_graph,\n",
    "                           entity_level_map, entity_order_map):\n",
    "    \"\"\"Create blank hierarchy for knowledge graph for missing entities.\"\"\"\n",
    "    child_nodes = set()\n",
    "    if entity_name in di_rel_graph.node:\n",
    "        child_nodes = set(di_rel_graph.successors(entity_name))\n",
    "    attrib_list_req = True if len(child_nodes) > 0 else False\n",
    "    ent = create_blank_entity(entity_name, attrib_list_req,\n",
    "                              entity_level_map, entity_order_map)\n",
    "    if len(child_nodes) > 0:\n",
    "        for child_node in child_nodes:\n",
    "            ent['attribute_list'].append(\n",
    "                create_blank_hierarchy(child_node, di_rel_graph,\n",
    "                                       entity_level_map, entity_order_map)\n",
    "            )\n",
    "    return ent\n",
    "\n",
    "\n",
    "def fill_blank_entities_in_k_graph(graph_output, di_rel_graph,\n",
    "                                   entity_level_map, entity_order_map,\n",
    "                                   root_level_ents):\n",
    "    \"\"\"Format the knowledge graph output as per the document definition.\"\"\"\n",
    "    ent_types_found = set(x['attribute_name'] for x in graph_output)\n",
    "    for ent in root_level_ents:\n",
    "        if ent not in ent_types_found:\n",
    "            graph_output.append(\n",
    "                create_blank_hierarchy(ent, di_rel_graph,\n",
    "                                       entity_level_map, entity_order_map)\n",
    "            )\n",
    "    graph_output.sort(key=lambda x: int(x['order']))\n",
    "    for entity in graph_output:\n",
    "        check_and_fill_blank_entities(\n",
    "            entity, di_rel_graph, entity_level_map, entity_order_map)\n",
    "\n",
    "\n",
    "def check_and_fill_blank_entities(entity, di_rel_graph,\n",
    "                                  entity_level_map, entity_order_map):\n",
    "    \"\"\"Fill in blank entities as per the hierarchy defined in graph.\"\"\"\n",
    "    entity_type = entity['attribute_name']\n",
    "    if entity['attribute_type'] not in ('group', 'subgroup'):\n",
    "        return\n",
    "\n",
    "    attribute_list = entity.get('attribute_list', [])\n",
    "    child_nodes_found = set((x['attribute_name'] for x in attribute_list))\n",
    "    child_nodes = set(di_rel_graph.successors(entity_type))\n",
    "    missing_nodes = child_nodes - child_nodes_found\n",
    "\n",
    "    for missing_node in missing_nodes:\n",
    "        attribute_list.append(\n",
    "            create_blank_hierarchy(missing_node, di_rel_graph,\n",
    "                                   entity_level_map, entity_order_map))\n",
    "\n",
    "    if len(missing_nodes) > 0:\n",
    "        entity['attribute_list'] = attribute_list\n",
    "\n",
    "    if len(attribute_list) > 0:\n",
    "        attribute_list.sort(key=lambda x: int(x['order']))\n",
    "\n",
    "    # check for existing entities not included in above step\n",
    "    for existing_entity in attribute_list:\n",
    "        existing_ent_type = existing_entity['attribute_name']\n",
    "        if existing_ent_type not in missing_nodes:\n",
    "            check_and_fill_blank_entities(\n",
    "                existing_entity, di_rel_graph,\n",
    "                entity_level_map, entity_order_map)\n",
    "\n",
    "        \n",
    "def get_model_predictions_for_npz(model, npz_image_path, class_encoder):\n",
    "    pixels = np.load(npz_image_path)['pixels']\n",
    "    predicted_labels = model.predict(np.array([pixels]), batch_size=1)\n",
    "    return class_encoder.one_hot_decode(predicted_labels[0].astype(np.float64))\n",
    "\n",
    "    \n",
    "def get_inference_IC_run(model, input_img_path,class_encoder):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes model_id and image path as input and predict class of\n",
    "    the image.\n",
    "    \n",
    "     Args:\n",
    "        input_img_type     - Type of Image i.e. PNG, JPEG, JPEG\n",
    "                                 has token, char and label\n",
    "        input_img_path     - path of image\n",
    "        model_id           - model_id of the trained model\n",
    "        \n",
    "    Returns:\n",
    "        Prediction of the class and its prediction accuracy percentage !   \n",
    "    \"\"\"\n",
    "    #Load Trained Models\n",
    "    try:\n",
    "\n",
    "        for npz_image_path in list(glob.glob(input_img_path)):\n",
    "            class_name = 'unknown'\n",
    "            character_name_to_probability = get_model_predictions_for_npz(model,\n",
    "                                                                          npz_image_path,\n",
    "                                                                          class_encoder)\n",
    "            img_name = os.path.basename(npz_image_path)\n",
    "            for clas, prob in character_name_to_probability.items():\n",
    "                if prob >  image_config['IC_PRED_PROB_THRESHOLD']:\n",
    "                    class_name = clas\n",
    "                    probabolity = round(prob,2)\n",
    "\n",
    "            print (img_name ,\" = \", class_name, '\\n',\"  predi_prob = \", probabolity)\n",
    "            print(\"Inferene Generated for Iamge : '{}'\".format(img_name))\n",
    "            return json.dumps({\"status\": \"success\",\n",
    "                               \"image_name\": str(img_name),\n",
    "                               \"class_name\": class_name,\n",
    "                               \"probabolity_percent\": probabolity})\n",
    "    except Exception as exception:\n",
    "        print(exception)\n",
    "        print(\"Inferene Failed, Try other Image or Contact Admin..!\")\n",
    "        return json.dumps({\"status\": \"failure\", \"result\": str(exception)})\n",
    "    \n",
    "def get_zoning_model(model_file_path):\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.get_default_graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(model_file_path, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "    with graph.as_default():\n",
    "        model= graph\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file_path = '/notebooks/Train_models/588/frozen_inference_graph.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(model_type)==1:\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    \n",
    "    if 'NER' in model_type :\n",
    "        print('loading model file')\n",
    "        print(model_path)\n",
    "        print(os.listdir(model_path))\n",
    "        model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "        print(\"loaded NER model\")\n",
    "\n",
    "    if 'DC' in model_type :\n",
    "        model = tf_idf_model\n",
    "        print(\"loaded DC model\")\n",
    "\n",
    "    if 'IC' in model_type :\n",
    "        model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "        print(\"loaded IC model\")\n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        model = get_zoning_model(model_file_path)\n",
    "        print(\"loaded Object Identification model\")\n",
    "\n",
    "    '''\n",
    "    if model_type == 'Object Identification':\n",
    "        model = get_zoning_model(model_file_path,zoning_file_label_path)\n",
    "        print(\"loaded Object Identification model\")\n",
    "    '''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files transfer to s3\n",
    "if OnPremiseFlag!=True:\n",
    "    import time\n",
    "    import boto3\n",
    "    from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "\n",
    "    def upload_file(file_name, bucket, folder_name=None):\n",
    "        \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "        :param file_name: File to upload\n",
    "        :param bucket: Bucket to upload to\n",
    "        :param folder_name: upload to specific folder\n",
    "        :return: True if file was uploaded, else False\n",
    "        \"\"\"\n",
    "\n",
    "        # Upload the file\n",
    "        if folder_name:\n",
    "            #response = s3_client.upload_file(file_name, bucket, folder_name+'/'+file_name.split('/')[-1])\n",
    "            response = s3.upload_file(file_name, bucket, folder_name+'/'+file_name.split('/')[-1])\n",
    "        else:\n",
    "            #response = s3_client.upload_file(file_name, bucket, file_name)\n",
    "            response = s3.upload_file(file_name, bucket, file_name)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Score function\n",
    "from flask import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def predict_ner_CC(model,request):\n",
    "    global word2idx, case2idx, char2idx, label2idx,tf_idf_model, svc_clf\n",
    "    import nltk\n",
    "    try:\n",
    "        shutil.copytree('/data/NER/nltk_data/','/home/mosaic-ai/nltk_data/')\n",
    "    except:\n",
    "        print(\" NLTK data already present in /home/mosaic-ai/nltk_data/ \")\n",
    "        pass      \n",
    "#     import os\n",
    "#     NLTK_PATH = os.getenv(\"NLTK_DATA\")\n",
    "#     import sys\n",
    "#     sys.path.insert(0,NLTK_PATH)\n",
    "#     nltk.download('punkt')\n",
    "    payload = request.json[\"payload\"]\n",
    "    request_data = payload\n",
    "    #model_id = int(request_data['model_id'])\n",
    "    text = request_data['text']\n",
    "    prediction_format = request_data['prediction_format']\n",
    "    k_graph = request_data.get('knowledge_graph')\n",
    "    entity_order_map = request_data.get('entity_order_map')\n",
    "    root_level_ents = request_data.get('root_level_ents')\n",
    "\n",
    "    result = get_inference_run_CC(text, tf_idf_model,svc_clf, prediction_format, model, k_graph, entity_order_map, root_level_ents)\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict_ner(model,request):\n",
    "    global word2idx, case2idx, char2idx, label2idx\n",
    "    import nltk\n",
    "    try:\n",
    "        shutil.copytree('/data/NER/nltk_data/','/home/mosaic-ai/nltk_data/')\n",
    "    except:\n",
    "        print(\" NLTK data already present in /home/mosaic-ai/nltk_data/ \")\n",
    "        pass       \n",
    "#     import os\n",
    "#     NLTK_PATH = os.getenv(\"NLTK_DATA\")\n",
    "#     import sys\n",
    "#     sys.path.insert(0,NLTK_PATH)\n",
    "#     nltk.download('punkt')\n",
    "    payload = request.json[\"payload\"]\n",
    "    request_data = payload\n",
    "    #model_id = int(request_data['model_id'])\n",
    "    text = request_data['text']\n",
    "    prediction_format = request_data['prediction_format']\n",
    "    k_graph = request_data.get('knowledge_graph')\n",
    "    entity_order_map = request_data.get('entity_order_map')\n",
    "    root_level_ents = request_data.get('root_level_ents')\n",
    "\n",
    "    result = get_inference_run(text, prediction_format, model, k_graph, entity_order_map, root_level_ents)\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict_Doc_classification(model,request):\n",
    "    global tf_idf_model, svc_clf\n",
    "    payload = request.json[\"payload\"]\n",
    "    request_data = payload\n",
    "    #model_id = int(request_data['model_id'])\n",
    "    text = request_data['text']['file_data']\n",
    "    transformed_vector_text = model.transform([text])\n",
    "    predicted_proba = svc_clf.predict_proba(transformed_vector_text)\n",
    "    class_pred = svc_clf.predict(transformed_vector_text)\n",
    "    recommendation = 'unknown'\n",
    "    \n",
    "    prob_score = 0.00\n",
    "    for i, proba_score in enumerate(predicted_proba):\n",
    "        prob_score = max(proba_score)\n",
    "        \n",
    "\n",
    "    if prob_score > float(CONFIG['DC_PRED_PROB_THRESHOLD']):\n",
    "         recommendation = class_pred[0]\n",
    "    \n",
    "    result = {'class':class_pred[0],'probability':str(prob_score), 'recommended_class':recommendation}\n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict_IC(model,request):\n",
    "    # added since POD is not able to get the EEF module\n",
    "    import sys\n",
    "    sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "    sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "#     from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "    global class_encoder\n",
    "    \n",
    "\n",
    "    try:\n",
    "        payload = request.json[\"payload\"]\n",
    "        request_data = payload\n",
    "        #model_id = int(request_data['model_id'])\n",
    "        input_img_path = request_data['input_img_path']\n",
    "        print(\"input_img_path_s3\",input_img_path)\n",
    "        input_img_type = request_data['input_img_type']\n",
    "        \n",
    "        # added for s3 storage\n",
    "        if not os.path.exists('/home/mosaic-ai/notebooks/IC_Data_Inf'):\n",
    "            os.makedirs('/home/mosaic-ai/notebooks/IC_Data_Inf')\n",
    "        curr_dir = os.getcwd()\n",
    "        image_parent_path = os.path.dirname(input_img_path)\n",
    "        \n",
    "        if OnPremiseFlag!=True:\n",
    "            from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "            for key in bucket.list(image_parent_path):\n",
    "                print(key)\n",
    "                try:\n",
    "                    os.chdir('/home/mosaic-ai/notebooks/IC_Data_Inf')\n",
    "                    os.makedirs(image_parent_path)\n",
    "                    print(\"dir created, latest updated\")\n",
    "                    print(\"dir\",os.getcwd())\n",
    "                    res = key.get_contents_to_filename(key.name)\n",
    "                except:\n",
    "                    print(key.name+\":\"+\"FAILED\")\n",
    "        else:\n",
    "            os.chdir('/home/mosaic-ai/notebooks/IC_Data_Inf')\n",
    "            os.makedirs(image_parent_path)\n",
    "            fp = '/home/mosaic-ai/notebooks/IC_Data_Inf'+input_img_path\n",
    "            try:\n",
    "                shutil.copy(input_img_path,fp)\n",
    "            except:\n",
    "                print(\"unable to copy IC files to PV\")\n",
    "                                 \n",
    "        os.chdir(curr_dir)\n",
    "                                    \n",
    "        input_img_path = '/home/mosaic-ai/notebooks/IC_Data_Inf/' + input_img_path\n",
    "    \n",
    "        input_images_data = list(glob.glob(input_img_path))\n",
    "     \n",
    "        input_images_data = list(glob.glob(input_img_path))\n",
    "        \n",
    "        #preprocess image and convert to npz format\n",
    "        img_prepro.image_preprocessing(input_images_data)\n",
    "        image_basepath, _ = os.path.splitext(input_img_path)\n",
    "        input_img_path = image_basepath+'.npz'\n",
    "\n",
    "        result = get_inference_IC_run(model,input_img_path, class_encoder)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "def predict_zoning_inference(model,request):\n",
    "#     from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "    from pathlib import Path\n",
    "    global category_index\n",
    "    \"\"\"\n",
    "    Route to call the inference of the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model_id    - selects the models which will be used to infer\n",
    "    image_path  - path of input image\n",
    "    \"\"\"\n",
    "    # added since POD is not able to get the EEF module\n",
    "    import sys\n",
    "    sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "    sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "#     from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "    \n",
    "    try:\n",
    "        payload = request.json[\"payload\"]\n",
    "        request_data = payload\n",
    "        image_path = request_data['image_path']\n",
    "        print(\"image_path............>\",image_path)\n",
    "        image_parent_path = os.path.dirname(image_path)\n",
    "\n",
    "        # added for s3 storage\n",
    "        if not os.path.exists('/home/mosaic-ai/notebooks/OI_Data_Inf'):\n",
    "            os.makedirs('/home/mosaic-ai/notebooks/OI_Data_Inf')\n",
    "        curr_dir = os.getcwd()\n",
    "        \n",
    "        if OnPremiseFlag!=True:\n",
    "            from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3            \n",
    "            for key in bucket.list(image_parent_path):\n",
    "                os.chdir('/home/mosaic-ai/notebooks/OI_Data_Inf')\n",
    "                if not os.path.exists('/home/mosaic-ai/notebooks/OI_Data_Inf/' + image_parent_path):\n",
    "                    os.makedirs(image_parent_path)\n",
    "                print(\"dir created\")\n",
    "                try:\n",
    "                    print(\"dir\",os.getcwd())\n",
    "                    res = key.get_contents_to_filename(key.name)\n",
    "                except:\n",
    "                    print(key.name+\":\"+\"FAILED\")\n",
    "        else:\n",
    "            os.chdir('/home/mosaic-ai/notebooks/OI_Data_Inf')\n",
    "            if not os.path.exists('/home/mosaic-ai/notebooks/OI_Data_Inf/' + image_parent_path):\n",
    "                os.makedirs(image_parent_path)\n",
    "                print(\"dir created\")\n",
    "            try:\n",
    "                fp =  '/home/mosaic-ai/notebooks/OI_Data_Inf/' + image_path\n",
    "                shutil.copy(image_path,fp)\n",
    "            except:\n",
    "                print(\"fail to copy OI files to PV \")\n",
    "                                 \n",
    "        os.chdir(curr_dir)\n",
    "                                    \n",
    "        image_path = '/home/mosaic-ai/notebooks/OI_Data_Inf/' + image_path\n",
    "        print(\"full image path.................>\",image_path)\n",
    "\n",
    "        \n",
    "        result = infer_zoning.run(image_path, model, category_index)\n",
    "        print(\"result\",result)\n",
    "        \n",
    "        if len (result)>0:\n",
    "            for i in range(len(result)):\n",
    "                file_name = result[i]['path']\n",
    "                print(\"OI File name \",file_name)\n",
    "                folder_path = Path(request_data['image_path'])\n",
    "                folder_name = str(folder_path.parent)\n",
    "                print(\"OI Folder name \",folder_name,type(folder_name))\n",
    "                #upload_file(file_name,bucket.name,Folder_name)\n",
    "                if OnPremiseFlag!=True:\n",
    "                    if folder_name:\n",
    "                        response = s3.upload_file(file_name, bucket.name, folder_name+'/'+file_name.split('/')[-1])\n",
    "                    else:\n",
    "                        response = s3.upload_file(file_name, bucket.name, file_name)\n",
    "                else:\n",
    "                    shutil.copy(file_name, folder_name+'/'+file_name.split('/')[-1])\n",
    "                    \n",
    "#                 return True\n",
    "                                          \n",
    "        return result\n",
    "    except Exception as exception:\n",
    "        return print(str(exception))\n",
    "        \n",
    "\n",
    "# OLD code , below functions was taking time while fetching files from s3 for batch inference\n",
    "# def predict_IC(model,request):\n",
    "#     # added since POD is not able to get the EEF module\n",
    "#     import sys\n",
    "#     sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "#     sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "#     from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "#     global class_encoder\n",
    "    \n",
    "#     try:\n",
    "#         payload = request.json[\"payload\"]\n",
    "#         request_data = payload\n",
    "#         #model_id = int(request_data['model_id'])\n",
    "#         input_img_path = request_data['input_img_path']\n",
    "#         print(\"input_img_path_s3\",input_img_path)\n",
    "#         input_img_type = request_data['input_img_type']\n",
    "        \n",
    "#         # added for s3 storage\n",
    "#         if not os.path.exists('/home/mosaic-ai/notebooks/IC_Data_Inf'):\n",
    "#             os.makedirs('/home/mosaic-ai/notebooks/IC_Data_Inf')\n",
    "#         curr_dir = os.getcwd()\n",
    "        \n",
    "#         for key in bucket.list():\n",
    "#             if input_img_path in key.name:\n",
    "#                 os.chdir('/home/mosaic-ai/notebooks/IC_Data_Inf')\n",
    "#                 os.makedirs(os.path.dirname(key.name))\n",
    "#                 print(\"dir created\")\n",
    "#                 try:\n",
    "#                     print(\"dir\",os.getcwd())\n",
    "#                     res = key.get_contents_to_filename(key.name)\n",
    "#                 except:\n",
    "#                     print(key.name+\":\"+\"FAILED\")\n",
    "\n",
    "                                 \n",
    "#         os.chdir(curr_dir)\n",
    "                                    \n",
    "#         input_img_path = '/home/mosaic-ai/notebooks/IC_Data_Inf/' + request_data['input_img_path']                                                        \n",
    "        \n",
    "#         input_images_data = list(glob.glob(input_img_path))\n",
    "        \n",
    "#         #preprocess image and convert to npz format\n",
    "#         img_prepro.image_preprocessing(input_images_data)\n",
    "#         image_basepath, _ = os.path.splitext(input_img_path)\n",
    "#         input_img_path = image_basepath+'.npz'\n",
    "\n",
    "#         result = get_inference_IC_run(model,input_img_path, class_encoder)\n",
    "#         return result\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "            \n",
    "# def predict_zoning_inference(model,request):\n",
    "#     from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "#     from pathlib import Path\n",
    "#     global category_index\n",
    "#     \"\"\"\n",
    "#     Route to call the inference of the model.\n",
    "\n",
    "#     Keyword arguments:\n",
    "#     model_id    - selects the models which will be used to infer\n",
    "#     image_path  - path of input image\n",
    "#     \"\"\"\n",
    "#     # added since POD is not able to get the EEF module\n",
    "#     import sys\n",
    "#     sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "#     sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "#     from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "    \n",
    "#     try:\n",
    "#         payload = request.json[\"payload\"]\n",
    "#         request_data = payload\n",
    "#         image_path = request_data['image_path']\n",
    "#         print(\"image_path............>\",image_path)\n",
    "        \n",
    "#         # added for s3 storage\n",
    "#         if not os.path.exists('/home/mosaic-ai/notebooks/OI_Data_Inf'):\n",
    "#             os.makedirs('/home/mosaic-ai/notebooks/OI_Data_Inf')\n",
    "#         curr_dir = os.getcwd()\n",
    "                \n",
    "#         for key in bucket.list():\n",
    "#             if image_path in key.name:\n",
    "#                 os.chdir('/home/mosaic-ai/notebooks/OI_Data_Inf')\n",
    "#                 os.makedirs(os.path.dirname(key.name))\n",
    "#                 print(\"dir created\")\n",
    "#                 try:\n",
    "#                     print(\"dir\",os.getcwd())\n",
    "#                     res = key.get_contents_to_filename(key.name)\n",
    "#                 except:\n",
    "#                     print(key.name+\":\"+\"FAILED\")\n",
    "\n",
    "                                 \n",
    "#         os.chdir(curr_dir)\n",
    "                                    \n",
    "#         image_path = '/home/mosaic-ai/notebooks/OI_Data_Inf/' + request_data['image_path']\n",
    "#         print(\"full image path.................>\",image_path)\n",
    "\n",
    "        \n",
    "#         result = infer_zoning.run(image_path, model, category_index)\n",
    "#         print(\"result\",result)\n",
    "        \n",
    "#         if len (result)>0:\n",
    "#             for i in range(len(result)):\n",
    "#                 file_name = result[i]['path']\n",
    "#                 print(\"OI File name \",file_name)\n",
    "#                 folder_path = Path(request_data['image_path'])\n",
    "#                 folder_name = str(folder_path.parent)\n",
    "#                 print(\"OI Folder name \",folder_name,type(folder_name))\n",
    "#                 #upload_file(file_name,bucket.name,Folder_name)\n",
    "#                 if folder_name:\n",
    "#                     response = s3.upload_file(file_name, bucket.name, folder_name+'/'+file_name.split('/')[-1])\n",
    "#                 else:\n",
    "#                     response = s3.upload_file(file_name, bucket.name, file_name)\n",
    "# #                 return True\n",
    "                                          \n",
    "#         return result\n",
    "#     except Exception as exception:\n",
    "#         return print(str(exception))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed local inf code below as sometime this was throwing error because of the dummy data used for model validation \n",
    "# # Global and Local Inference\n",
    "# # Validate the predict_ner function with sample data\n",
    "# if len(model_type)==1:\n",
    "    \n",
    "#     if 'NER' in model_type and CCFlag == 'False':\n",
    "#         print(\"NER and CCFlag False\")\n",
    "#         import requests\n",
    "#         req = requests.Request()\n",
    "#         req.json={\"payload\":{\"prediction_format\": 1, \"text\": {\"1\": {\"1\": {\"sentence\": \"Franchise\\tDescription\\tCode\\nBA\\tBaby Care\\tBC01\\nFC\\tFeminine Care\\tFC01\\nSC\\tSelf Care\\tSC01\\nMC\\tMother Care\\tMC01\", \"unique_id\": 1784250}}, \"2\": {\"1\": {\"sentence\": \"Flavor / Scent\\tBrand\\nA012\\tPears\\nA013\\tLEM\\nA014\\tHimalaya\\nA015\\tHimalaya\\nA016\\tMist\", \"unique_id\": 1784252}}}, \"knowledge_graph\": [[\"Franchise\", \"Description\"],[\"Franchise\", \"Code\"],[\"Flavor\", \"Description\"], [\"Flavor\", \"Brand\"]], \"root_level_ents\": [\"Franchise\", \"Flavor\"], \"entity_order_map\": {\"Franchise\": 1, \"Description\": 5, \"Code\": 3, \"Flavor\": 4, \"Brand\": 6}, \"language\": \"english\", \"mode_type\": \"mode_1\"}}\n",
    "#         result = predict_ner(model,req)\n",
    "#         print(\"result\",result)\n",
    "        \n",
    "\n",
    "#     if 'NER' in model_type and CCFlag == 'True':\n",
    "#         print(\"NER and CCFlag True\")\n",
    "#         import requests\n",
    "#         req = requests.Request()\n",
    "#         req.json={\"payload\":{\"model_id\": 12, \"prediction_format\": 1, \"text\": {\"1\": {\"1\": {\"sentence\": \"                                                               The Cincinnati Insurance Company\\n\\n                                                               The Cincinnati Casualty Company\\n\\n                                                               The Cincinnati Indemnity Company\\n\\n                        ABOUT YOUR CINCINNATI POLICY\\n\\nPolicy Number:     EWC  024  52  06-04\\n\\nEffective Date:    06-15-2017\\n\\nNamed Insured:     BLAKLEY   LAWNS  INC     AND  BLAKLEY           FERTILIZER  INC\\n\\nOur  Cincinnati    Customer         Care    Center             provides        service    on   behalf  of                         your\\n\\nagency.\", \"unique_id\": 581591}, \"2\": {\"sentence\": \"Your  requested        transaction                is  attached.\", \"unique_id\": 581592}, \"3\": {\"sentence\": \"If         you  have  questions\\n\\nregarding this transaction, please contact us.\", \"unique_id\": 581593}, \"4\": {\"sentence\": \"For contact information, please\\n\\nrefer to the My Policies page.\", \"unique_id\": 581594}, \"5\": {\"sentence\": \"The Cincinnati Insurance Companies and Cincinnati refer to one or more companies of the insurer group\\n                                                     The                                           The Cincinnati\\nproviding  property and casualty coverages  through                Cincinnati  Insurance  Company,\\n Casualty Company, or  The Cincinnati Indemnity Company.\", \"unique_id\": 581595}, \"6\": {\"sentence\": \"Each insurer has sole financial responsibility for\\n\\nits own products.\", \"unique_id\": 581596}, \"7\": {\"sentence\": \"Not all subsidiaries operate in all states.\", \"unique_id\": 581597}, \"8\": {\"sentence\": \"Mailing Address: P.O.\", \"unique_id\": 581598}, \"9\": {\"sentence\": \"Box 145496  Cincinnati, Ohio 45250-5496  Headquarters: 6200 S. Gilmore Road Fairfield, Ohio 45014-5141\\n\\n                                                 www.cinfin.com\\n\\nWC 98 06 31\", \"unique_id\": 581599}}, \"2\": {\"1\": {\"sentence\": \"                       WORKERS COMP.\", \"unique_id\": 581601}, \"2\": {\"sentence\": \"COVER SHEET\\n\\nNamed Insured:   BLAKLEY    LAWNS   INC  AND  BLAKLEY  FERTILIZER     INC\\n\\nPolicy Number:   EWC   024  52  06-04\\n\\nEffective Date:  06-15-2017\\n\\nAgency Name:     T.J.  NICOUD    &  COMPANY   12-117\\n\\n** FOR INTERNAL PURPOSES ONLY, PLEASE DISCARD THIS COVER SHEET             AFTER  ATTACHING  ALL\\n\\nAPPLICABLE FORMS **\\n\\nPlease manually insert the following forms and send with the policy:\\n\\nCLAIMS  LETTER   WITH  STICKERS\\n\\nWC 98 06 11\", \"unique_id\": 581602}}, \"3\": {\"1\": {\"sentence\": \"                                                          The Cincinnati Insurance Company\\n\\n                                                              The Cincinnati Casualty Company\\n\\n                                                          The Cincinnati Indemnity Company\\n\\nPolicy Number: EWC        024  52  06-04\\n\\nEffective Date: 06-15-2017\\n\\nNamed Insured:       BLAKLEY   LAWNS  INC   AND      BLAKLEY  FERTILIZER         INC\\n\\nFor professional advice and policy questions or changes, please contact your local independent agency:\\n\\nT.J.  NICOUD      &  COMPANY\\n\\nP.O.\", \"unique_id\": 581604}, \"2\": {\"sentence\": \"BOX    13078\\n\\nSPRINGFIELD,         IL   62791-3078\\n\\n877-242-2544\\n\\nDear Policyholder:\\n\\nThank you\\n\\nThank you for trusting The Cincinnati Insurance Companies with your workers compensation coverage.\", \"unique_id\": 581605}, \"3\": {\"sentence\": \"We\\n\\nrecognize that locally based independent agents have the working knowledge to help you choose the right\\n\\ninsurance company for your needs.\", \"unique_id\": 581606}, \"4\": {\"sentence\": \"Together with your local independent insurance agency, we are committed\\n\\nto providing you with the highest level of service.\", \"unique_id\": 581607}, \"5\": {\"sentence\": \"Please review your enclosed policy information to verify your coverage details, as well as deductibles and\\n\\ncoverage amounts.\", \"unique_id\": 581608}, \"6\": {\"sentence\": \"Should your needs change, your agent is available to review and update your policy.\", \"unique_id\": 581609}, \"7\": {\"sentence\": \"Please promptly report claims\\n\\nTo report a workers compensation claim, please call Cincinnati Insurance directly, any time day or night, using\\n\\nour toll-free Claims Reporting Center line at 877-242-2544.\", \"unique_id\": 581610}, \"8\": {\"sentence\": \"Please have claim-related information ready -\\n\\nincluding   your  policy  number,  injured  employee    information,  type  and  cause  of  injury  and  any  treatment\\n\\nspecifics - so our associates can expedite your claim.\", \"unique_id\": 581611}, \"9\": {\"sentence\": \"For information about minimizing and controlling your workers compensation exposures and costs, you may\\n\\nrequest a Claims Kit from your local independent agency.\", \"unique_id\": 581612}, \"10\": {\"sentence\": \"Sincerely,\\n\\nSteve Spray\\n\\nSenior Vice President - Commercial Lines\\n\\nWC 98 06 46\", \"unique_id\": 581613}}, \"4\": {\"1\": {\"sentence\": \"                                                     The Cincinnati Indemnity Company\\n\\n                                                                     A Stock Insurance Company\\n\\n                                                Headquarters: 6200 S. Gilmore Road, Fairfield, OH 45014-5141\\n\\n                                                     Mailing address: P.O.\", \"unique_id\": 581615}, \"2\": {\"sentence\": \"Box 145496, Cincinnati, OH 45250-5496\\n\\n                                                             www.cinfin.com      n              513-870-2000\\n\\nWORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE\\n\\n                                     POLICY INFORMATION PAGE\\n\\nPolicy No.\", \"unique_id\": 581616}, \"3\": {\"sentence\": \"Policy Period        Previous Policy No.\", \"unique_id\": 581617}, \"4\": {\"sentence\": \"Billing Method\\n\\n                                  From                   To\\n\\nEWC     024  52  06-04        06-15-2017        06-15-2018   EWC0245206-02                          DIRECT  BILL\\n\\nAgency                   Carrier             Risk ID No.\", \"unique_id\": 581618}, \"5\": {\"sentence\": \"Entity\\n\\n12-117                   27197                               CORPORATION\\n\\nAgent\\n\\nCINCINNATI       CUSTOMER      CARE     CENTER\\n\\nT.J.    NICOUD   &     COMPANY\\n\\nP.O.\", \"unique_id\": 581619}, \"6\": {\"sentence\": \"BOX     13078\\n\\nSPRINGFIELD,         IL  62791-3078\\n\\n1.\", \"unique_id\": 581620}, \"7\": {\"sentence\": \"Named Insured and Address\\n\\n    BLAKLEY      LAWNS   INC   AND   BLAKLEY    FERTILIZER   INC\\n\\n    3400     W  GRAND    AVE\\n\\n    SPRINGFIELD,         IL   62711-7351\\n\\n2.\", \"unique_id\": 581621}, \"8\": {\"sentence\": \"The Policy Period is from     06-15-2017         to  06-15-2018  12:01 am.\", \"unique_id\": 581622}, \"9\": {\"sentence\": \"The Standard Time at the insureds\\n\\n    Mailing address.\", \"unique_id\": 581623}, \"10\": {\"sentence\": \"3.\", \"unique_id\": 581624}, \"11\": {\"sentence\": \"A.\", \"unique_id\": 581625}, \"12\": {\"sentence\": \"Workers Compensation Insurance: Part ONE of the policy applies to the Workers Compensation Law of the\\n\\n        states listed here:   IL\\n\\n    B.\", \"unique_id\": 581626}, \"13\": {\"sentence\": \"Employers Liability Insurance: Part TWO of the policy applies to work in each state listed in Item 3A.\", \"unique_id\": 581627}, \"14\": {\"sentence\": \"The\\n\\n        limits of our liability under Part TWO are:\\n\\n                         Bodily Injury by Accident       $ 100,000        each accident\\n\\n                         Bodily Injury by Disease        $ 100,000        each employee\\n\\n                         Bodily Injury by Disease        $ 500,000        policy limit\\n\\n    C. Other States Insurance: Part THREE of the policy applies to all states except\\n\\n        North    Dakota,      Ohio,     Washington,      Wyoming,   and  States  Designated                in              Item  3A  of\\n\\n        the     information       page  and  Alaska\\n\\n    D. This policy includes these endorsements and schedules:\\n\\n        REFER    TO  ENDORSEMENT        SCHEDULE\\n\\n4.\", \"unique_id\": 581628}, \"15\": {\"sentence\": \"The premium for this policy will be determined by our Manual of Rules, Classifications, Rates, and Rating\\n\\n    Plans.\", \"unique_id\": 581629}, \"16\": {\"sentence\": \"All information required below is subject to verification and change by audit.\", \"unique_id\": 581630}, \"17\": {\"sentence\": \"SEE EXTENSION OF INFORMATION PAGE\\n\\n05-12-2017       07:10\\n\\n                 Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581631}, \"18\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581632}, \"19\": {\"sentence\": \"WC 00 00 01 A (01 13)                                                     EWC    024            52  06-04                  Page  1   of  8\", \"unique_id\": 581633}}, \"5\": {\"1\": {\"sentence\": \"Minimum Premium        $  1,250  Estimated Annual Premium                                        $ 4,865\\n\\n                                 Deposit Premium                                                 $ 4,865\\n\\n                                 *See Schedule Attached Taxes / Assessments                      $ 49\\n\\n                                 Total Premium Due                                               $ 4,914\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581635}, \"2\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581636}, \"3\": {\"sentence\": \"WC 00 00 01 A (01 13)            EWC  024                                             52  06-04                       Page  2  of  8\", \"unique_id\": 581637}}, \"6\": {\"1\": {\"sentence\": \"                             EXTENSION OF INFORMATION PAGE\\n\\n                                        Name and Location Schedule\\n\\nLoc.\", \"unique_id\": 581639}, \"2\": {\"sentence\": \"Insured\\n\\nNo.\", \"unique_id\": 581640}, \"3\": {\"sentence\": \"No.\", \"unique_id\": 581641}, \"4\": {\"sentence\": \"Name / Address\\n\\n001         001   BLAKLEY    LAWNS  INC  AND  BLAKLEY   FERTILIZER  INC\\n\\n                  3400    W  GRAND  AVE\\n\\n                  SPRINGFIELD,      IL   62711-7351\\n\\nFederal Employee  ID No.\", \"unique_id\": 581642}, \"5\": {\"sentence\": \"Entity\\n\\n37-1158420                                    CORPORATION\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581643}, \"6\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581644}, \"7\": {\"sentence\": \"WC 00 00 01 A (01 13)                                               EWC  024             52  06-04                    Page  3  of  8\", \"unique_id\": 581645}}, \"7\": {\"1\": {\"sentence\": \"                          EXTENSION OF INFORMATION PAGE\\n\\n                                  Installment Schedule\\n\\nSEE   BILLING  STATEMENT  MAILED  SEPARATELY\\n\\nDate                   Premium                Taxes / Assessments                         Installment\\n\\n05-12-2017  07:10\\n\\n               Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581647}, \"2\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581648}, \"3\": {\"sentence\": \"WC 00 00 01 A (01 13)                                   EWC        024                52  06-04                          Page  4  of  8\", \"unique_id\": 581649}}, \"8\": {\"1\": {\"sentence\": \"                       EXTENSION OF INFORMATION PAGE\\n\\n                                  Taxes / Assessments Schedule\\n\\nTaxes / Assessments               Rate / Percentage                                    Premium\\n\\nIL  OPERATIONS  FUND   SURCHARGE  1.01%                                                $49\\n\\n                                  Total Taxes / Assessments                            $49\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581651}, \"2\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581652}, \"3\": {\"sentence\": \"WC 00 00 01 A (01 13)                                           EWC  024           52  06-04                          Page  5  of  8\", \"unique_id\": 581653}}, \"9\": {\"1\": {\"sentence\": \"                               EXTENSION OF INFORMATION PAGE\\n\\n                                         4.\", \"unique_id\": 581655}, \"2\": {\"sentence\": \"Classification of Operations\\n\\n                                                  State: IL\\n\\n                                                       Premium Basis\\n\\n                                                       Total Estimated\\n\\nLoc.\", \"unique_id\": 581656}, \"3\": {\"sentence\": \"Code                                                   Annual            Rate per $100                           Estimated Annual\\n\\nNo.\", \"unique_id\": 581657}, \"4\": {\"sentence\": \"No.\", \"unique_id\": 581658}, \"5\": {\"sentence\": \"Classification Description                      Remuneration      of Remuneration                         Premium\\n\\n001   8742   SALESPERSONS     OR                             55,513            0.440                                   244\\n\\n             COLLECTORS-OUTSIDE\\n\\n             06-15-2017   TO   06-15-2018\\n\\n001   8810   CLERICAL   OFFICE    EMPLOYEES       NOC        IF  ANY           0.220                                          0\\n\\n             06-15-2017   TO   06-15-2018\\n\\n001   9102   PARK   NOC-ALL   EMPLOYEES        &             74,149            5.380                                   3,989\\n\\n             DRIVERS\\n\\n             06-15-2017   TO   06-15-2018\\n\\n                                                                          Manual Premium            $                  4,233\\n\\n      9889   SCHEDULE   MODIFICATION                                           1.0900                                  381\\n\\n             06-15-2017   TO   06-15-2018\\n\\n                                                                           State Premium            $                  4,614\\n\\n      0900   EXPENSE    CONSTANT                                                                                       160\\n\\n             06-15-2017   TO   06-15-2018\\n\\n      9740   TERRORISM                                                         0.0600                                         78\\n\\n             06-15-2017   TO   06-15-2018\\n\\n      9741   CATASTROPHE     (OTHER      THAN                                  0.0100                                         13\\n\\n             CERTIFIED   ACTS     OF     TERRORISM)\\n\\n             06-15-2017   TO   06-15-2018\\n\\n05-12-2017   07:10\\n\\n             Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581659}, \"6\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581660}, \"7\": {\"sentence\": \"WC 00 00 01  A (01 13)                                                    EWC  024       52  06-04                     Page   6   of     8\", \"unique_id\": 581661}}, \"10\": {\"1\": {\"sentence\": \"                                 State Total Estimated Premium                                          $  4,865\\n\\n05-12-2017   07:10\\n\\n             Includes copyright  material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581663}, \"2\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581664}, \"3\": {\"sentence\": \"WC 00 00 01  A (01 13)           EWC  024                                                    52  06-04                  Page  7  of  8\", \"unique_id\": 581665}}, \"11\": {\"1\": {\"sentence\": \"                                  EXTENSION OF INFORMATION PAGE\\n\\n                                  3D.\", \"unique_id\": 581667}, \"2\": {\"sentence\": \"Endorsement Schedule\\n\\nState       Form No.\", \"unique_id\": 581668}, \"3\": {\"sentence\": \"Form Title\\n\\n            WC  98  06  06     A  WORKERS     COMPENSATION     AND    EMPLOYERS                   LIABILITY           INSURANCE\\n\\n                                  POLICY   QUICK   REFERENCE\\n\\nUS          WC  00  00  00     C  WORKERS     COMPENSATION     AND    EMPLOYERS                   LIABILITY           INSURANCE\\n\\n                                  POLICY\\n\\n            WC  99  06  79     E  NOTICE   OF  ELECTION    TO    ACCEPT    OR                 REJECT  AN  INSURANCE\\n\\n                                  DEDUCTIBLE       FOR  ILLINOIS    WORKERS                  COMPENSATION            MEDICAL\\n\\n                                  BENEFITS\\n\\nUS          WC  00  04  06     A  PREMIUM     DISCOUNT   ENDORSEMENT\\n\\nUS          WC  00  04  14        NOTIFICATION     OF    CHANGE   IN  OWNERSHIP                   ENDORSEMENT\\n\\nUS          WC  00  04  19        PREMIUM     DUE  DATE  ENDORSEMENT\\n\\nUS          WC  00  04  21     D  CATASTROPHE      (OTHER  THAN     CERTIFIED                 ACTS    OF  TERRORISM)\\n\\n                                  PREMIUM     ENDORSEMENT\\n\\nUS          WC  00  04  22     B  TERRORISM    RISK     INSURANCE     PROGRAM                 REAUTHORIZATION               ACT\\n\\n                                  DISCLOSURE       ENDORSEMENT\\n\\nUS          WC  00  04  24        AUDIT   NONCOMPLIANCE     CHARGE    ENDORSEMENT\\n\\nIL          WC  12  06  01     E  ILLINOIS     AMENDATORY   ENDORSEMENT\\n\\n            WC  99  06  00     A  SIGNATURE    ENDORSEMENT\\n\\nCountersigned this     day of     ,\\n\\n                                                        Authorized Representative\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\", \"unique_id\": 581669}, \"4\": {\"sentence\": \"Copyright 1987 National Council on Compensation Insurance.\", \"unique_id\": 581670}, \"5\": {\"sentence\": \"WC 00 00 01 A (01 13)                                                 EWC  024                52  06-04               Page  8    of  8\", \"unique_id\": 581671}}, \"12\": {\"1\": {\"sentence\": \"               NOTICE OF ELECTION TO ACCEPT OR REJECT\\n\\n          AN INSURANCE DEDUCTIBLE FOR ILLINOIS WORKERS\\n\\n                        COMPENSATION MEDICAL BENEFITS\\n\\nIllinois  Law  permits  an   employer      to  buy  WORKERS  compensation  insurance  with  a  $1,000  deductible.\", \"unique_id\": 581673}, \"2\": {\"sentence\": \"In\\n\\nconsideration of a reduced premium charge, the deductible is for medical benefits only and applies to each\\n\\naccident.\", \"unique_id\": 581674}, \"3\": {\"sentence\": \"The premium reduction for the deductible is 1.5 % of the manual premium (modified by increased limits\\n\\ncharges for Employers Liability, if applicable) prior to the application of any other rating factors.\", \"unique_id\": 581675}, \"4\": {\"sentence\": \"A full description of how the deductible works is printed as a sample Endorsement below:\\n\\n                        ILLINOIS MEDICAL BENEFITS DEDUCTIBLE ENDORSEMENT\\n\\nThis endorsement applies only to the insurance provided by Part One (Workers Compensation Insurance)\\n\\nbecause Illinois is shown in Item 3.A.\", \"unique_id\": 581676}, \"5\": {\"sentence\": \"of the Information Page.\", \"unique_id\": 581677}, \"6\": {\"sentence\": \"1.\", \"unique_id\": 581678}, \"7\": {\"sentence\": \"Part One (Workers Compensation Insurance) applies to medical benefits only in excess of a\\n\\n          deductible amount of $1,000.\", \"unique_id\": 581679}, \"8\": {\"sentence\": \"This deductible applies separately to each accident, regardless of the\\n\\n          number of persons injured in the accident.\", \"unique_id\": 581680}, \"9\": {\"sentence\": \"2.\", \"unique_id\": 581681}, \"10\": {\"sentence\": \"We will pay the deductible amount for you, but you must reimburse us within 30 days after we send\\n\\n          you notice that payment is due.\", \"unique_id\": 581682}, \"11\": {\"sentence\": \"If you fail to reimburse us, we may cancel the policy in accordance\\n\\n          with Illinois cancellation law.\", \"unique_id\": 581683}, \"12\": {\"sentence\": \"We may keep the amount of unearned premium that will reimburse us\\n\\n          for the payments we made.\", \"unique_id\": 581684}, \"13\": {\"sentence\": \"These rights are in addition to other rights we have to be reimbursed.\", \"unique_id\": 581685}, \"14\": {\"sentence\": \"Please show whether or not you want the deductible by checking the appropriate choice below.\", \"unique_id\": 581686}, \"15\": {\"sentence\": \"Yes, I want a deductible of $1,000 applied to medical benefits under the Illinois WORKERS\\n\\n          Compensation Law.\", \"unique_id\": 581687}, \"16\": {\"sentence\": \"I understand that the Company shall pay the deductible amount and seek re-\\n\\n          imbursement from the employer shown below.\", \"unique_id\": 581688}, \"17\": {\"sentence\": \"No, I do not want the deductible described in this Notice.\", \"unique_id\": 581689}, \"18\": {\"sentence\": \"EWC     024   52  06-04\\n\\nDate                                                          Policy Number\\n\\nBLAKLEY        LAWNS    INC  AND  BLAKLEY      FERTILIZER     INC\\n\\nInsured\\n\\nBy                                                            Title\\n\\nAgency         T.J.     NICOUD    &  COMPANY        12-117\\n\\nWC 99 06 79 E\", \"unique_id\": 581690}}, \"13\": {\"1\": {\"sentence\": \"WORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE POLICY\\n\\n                              PREMIUM DISCOUNT ENDORSEMENT\\n\\nThe premium for this policy and the policies, if any, listed in Item 3 of the Schedule may be eligible for a dis-\\n\\ncount.\", \"unique_id\": 581692}, \"2\": {\"sentence\": \"This endorsement shows your estimated discount in Items 1 or 2 of the Schedule.\", \"unique_id\": 581693}, \"3\": {\"sentence\": \"The final calculation\\n\\nof premium discount will be determined by our manuals and your premium basis as determined by audit.\", \"unique_id\": 581694}, \"4\": {\"sentence\": \"Pre-\\n\\nmium subject to retrospective rating is not subject to premium discount.\", \"unique_id\": 581695}, \"5\": {\"sentence\": \"Schedule\\n\\n1.\", \"unique_id\": 581696}, \"6\": {\"sentence\": \"State                                        Estimated Eligible Premium\\n\\n                       First              Next                          Next\\n\\n    IL                 $10,000            $190,000                      $1,550,000        Balance\\n\\n2.\", \"unique_id\": 581697}, \"7\": {\"sentence\": \"Average percentage discount:                            %\\n\\n3.\", \"unique_id\": 581698}, \"8\": {\"sentence\": \"Other policies:\\n\\n4.\", \"unique_id\": 581699}, \"9\": {\"sentence\": \"If there are no entries in Items 1, 2, and 3 of the Schedule, see the Premium Discount Endorsement at-\\n\\n    tached to your policy number:\\n\\nThis endorsement changes the policy to which it is attached and is effective on the date issued               unless    other-\\n\\nwise stated.\", \"unique_id\": 581700}, \"10\": {\"sentence\": \"(The information below is required only when this endorsement is issued subsequent to preparation of the  policy.)\", \"unique_id\": 581701}, \"11\": {\"sentence\": \"Endorsement Effective  06-15-2017       Policy No.EWC          024  52  06-04       Endorsement No.\", \"unique_id\": 581702}, \"12\": {\"sentence\": \"Insured    BLAKLEY   LAWNS    INC  AND  BLAKLEY             FERTILIZER  INC\\n\\nInsurance Company      THE    CINCINNATI  INDEMNITY            COMPANY              Premium $INCL\\n\\n                                                            Countersigned by\\n\\nWC 00 04 06 A\\n\\nCopyright 1993 National Council on Compensation Insurance.\", \"unique_id\": 581703}}, \"14\": {\"1\": {\"sentence\": \"WORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE POLICY\\n\\n                       PREMIUM DUE DATE ENDORSEMENT\\n\\nThis endorsement is used to amend:\\n\\nSection D. of Part Five of the policy is replaced by this provision.\", \"unique_id\": 581705}, \"2\": {\"sentence\": \"PART FIVE\\n\\n                                                    PREMIUM\\n\\nD.  Premium is amended to read:\\n\\n    You will pay all premium when due.\", \"unique_id\": 581706}, \"3\": {\"sentence\": \"You will pay the premium even if part or all of a workers compensation\\n\\n    law is not valid.\", \"unique_id\": 581707}, \"4\": {\"sentence\": \"The due date for audit and retrospective premiums is the date of the billing.\", \"unique_id\": 581708}, \"5\": {\"sentence\": \"This endorsement   changes  the  policy  to  which  it  is  attached      and  is  effective  on  the  date  issued    unless\\n\\notherwise stated.\", \"unique_id\": 581709}, \"6\": {\"sentence\": \"(The information below is required only when this endorsement is issued subsequent to preparation of the policy.)\", \"unique_id\": 581710}, \"7\": {\"sentence\": \"Endorsement Effective  06-15-2017        Policy No.EWC      024       52  06-04    Endorsement No.\", \"unique_id\": 581711}, \"8\": {\"sentence\": \"Insured  BLAKLEY   LAWNS    INC  AND     BLAKLEY    FERTILIZER            INC\\n\\nInsurance Company      THE  CINCINNATI       INDEMNITY      COMPANY                Premium $INCL\\n\\n                                                    Countersigned by\\n\\nWC 00 04 19                2000 National Council on Compensation Insurance, Inc.\", \"unique_id\": 581712}}, \"15\": {\"1\": {\"sentence\": \"WORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE POLICY\\n\\n                   CATASTROPHE (OTHER THAN CERTIFIED ACTS\\n\\n                          OF TERRORISM) PREMIUM ENDORSEMENT\\n\\nThis endorsement is notification that your insurance carrier is charging premium to cover the losses that may\\n\\noccur in the event of a Catastrophe (other than Certified Acts of Terrorism) as that term is defined below.\", \"unique_id\": 581714}, \"2\": {\"sentence\": \"Your\\n\\npolicy provides coverage for workers compensation losses caused by a Catastrophe (other than Certified Acts\\n\\nof Terrorism).\", \"unique_id\": 581715}, \"3\": {\"sentence\": \"This premium charge does not provide funding for Certified Acts of Terrorism contemplated\\n\\nunder the Terrorism Risk Insurance Program Reauthorization Act Disclosure Endorsement (WC 00 04 22 B),\\n\\nattached to this policy.\", \"unique_id\": 581716}, \"4\": {\"sentence\": \"For purposes of this endorsement, the following definitions apply:  Catastrophe (other than Certified Acts of Terrorism): Any single event, resulting from an Earthquake,\\n   Noncertified Act of Terrorism, or Catastrophic Industrial Accident, which results in aggregate workers    compensation losses in excess of $50 million.\", \"unique_id\": 581717}, \"5\": {\"sentence\": \"  Earthquake: The shaking and vibration at the surface of the earth resulting from underground movement\\n\\n   along a fault plane or from volcanic activity.\", \"unique_id\": 581718}, \"6\": {\"sentence\": \"  Noncertified Act of Terrorism: An event that is not certified as an Act of Terrorism by the Secretary of\\n\\n   Treasury pursuant to the Terrorism Risk Insurance Act of 2002 (as amended) but that meets all of the\\n\\n   following criteria:\\n\\n   a.\", \"unique_id\": 581719}, \"7\": {\"sentence\": \"It is an act that is violent or dangerous to human life, property, or infrastructure;\\n\\n   b.\", \"unique_id\": 581720}, \"8\": {\"sentence\": \"The act results in damage within the United States, or outside of the United States in the case of the\\n\\n         premises of United States missions or air carriers or vessels as those terms are defined in the\\n\\n         Terrorism Risk Insurance Act of 2002 (as amended); and\\n\\n   c.    It is an act that has been committed by an individual or individuals as part of an effort to coerce the\\n\\n         civilian population of the United States or to influence the policy or affect the conduct of the United\\n\\n         States Government by coercion.\", \"unique_id\": 581721}, \"9\": {\"sentence\": \"  Catastrophic Industrial Accident: A chemical release, large explosion, or small blast that is localized in\\n\\n   nature and affects workers in a small perimeter the size of a building.\", \"unique_id\": 581722}, \"10\": {\"sentence\": \"The premium charge for the coverage your policy provides for workers compensation losses caused by a\\n\\nCatastrophe (other than Certified Acts of Terrorism) is shown in Item 4 of the Information Page or in the\\n\\nSchedule below.\", \"unique_id\": 581723}, \"11\": {\"sentence\": \"Schedule\\n\\n                        State                            Rate                                                   Premium\\n\\n                                                   See  State  Schedule\\n\\nThis endorsement changes the policy to which it is attached and is effective on the date issued unless\\n\\notherwise stated.\", \"unique_id\": 581724}, \"12\": {\"sentence\": \"(The information below is required only when this endorsement is issued subsequent to preparation of the policy.)\", \"unique_id\": 581725}, \"13\": {\"sentence\": \"Endorsement Effective     06-15-2017     Policy No.\", \"unique_id\": 581726}, \"14\": {\"sentence\": \"EWC  024        52  06-04  Endorsement No.\", \"unique_id\": 581727}, \"15\": {\"sentence\": \"Insured  BLAKLEY        LAWNS  INC  AND  BLAKLEY   FERTILIZER       INC\\n\\nInsurance Company         THE  CINCINNATI  INDEMNITY     COMPANY               Premium $INCL\\n\\n                                                   Countersigned by\\n\\nWC 00 04 21 D            Copyright 2015 National Council on Compensation Insurance, Inc. All Rights Reserved.\", \"unique_id\": 581728}}, \"16\": {\"1\": {\"sentence\": \"WORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE POLICY\\n\\n     TERRORISM RISK INSURANCE PROGRAM REAUTHORIZATION ACT\\n\\n                                       DISCLOSURE ENDORSEMENT\\n\\nThis endorsement addresses the requirements of the Terrorism Risk Insurance Act of 2002 as amended and\\n\\nextended by the Terrorism Risk Insurance Program Reauthorization Act of 2015.\", \"unique_id\": 581730}, \"2\": {\"sentence\": \"It serves to notify you of\\n\\ncertain limitations under the Act, and that your insurance carrier is charging premium for losses that may occur\\n\\nin the event of an Act of Terrorism.\", \"unique_id\": 581731}, \"3\": {\"sentence\": \"Your  policy   provides     coverage   for   workers   compensation  losses      caused      by  Acts   of      Terrorism,  including\\n\\nworkers compensation benefit obligations dictated by state law.\", \"unique_id\": 581732}, \"4\": {\"sentence\": \"Coverage for such losses is still subject to all\\n\\nterms, definitions, exclusions, and conditions in your policy, and any applicable federal and/or state laws,\\n\\nrules, or regulations.\", \"unique_id\": 581733}, \"5\": {\"sentence\": \"Definitions\\n\\nThe definitions provided in this endorsement are based on and have the same meaning as the definitions in\\n\\nthe Act .If words or phrases not defined in this endorsement are defined in the Act, the definitions in the Act\\n\\nwill apply.\", \"unique_id\": 581734}, \"6\": {\"sentence\": \"Act means the Terrorism Risk Insurance Act of 2002, which took effect on November 26, 2002, and any amendments     thereto,     including  any   amendments  resulting      from     the  Terrorism  Risk           Insurance   Program Reauthorization Act of 2015.\", \"unique_id\": 581735}, \"7\": {\"sentence\": \"Act of Terrorism means any act that is certified by the Secretary of the Treasury, in consultation with the\\n\\nSecretary of Homeland Security, and the Attorney General of the United States as meeting all of the following\\n\\nrequirements:\\n\\na.\", \"unique_id\": 581736}, \"8\": {\"sentence\": \"The act is an act of terrorism.\", \"unique_id\": 581737}, \"9\": {\"sentence\": \"b.\", \"unique_id\": 581738}, \"10\": {\"sentence\": \"The act is violent or dangerous to human life, property or infrastructure.\", \"unique_id\": 581739}, \"11\": {\"sentence\": \"c.   The act resulted in damage within the United States, or outside of the United States in the case of the\\n\\n     premises of United States missions or certain air carriers or vessels.\", \"unique_id\": 581740}, \"12\": {\"sentence\": \"d.   The act has been committed by an individual or individuals as part of an effort to coerce the civilian\\n\\n     population    of  the  United  States   or  to  influence  the  policy  or  affect    the   conduct        of  the  United  States\\n\\n     Government by coercion.\", \"unique_id\": 581741}, \"13\": {\"sentence\": \"Insured Loss means any loss resulting from an act of terrorism (and, except for Pennsylvania, including an\\n\\nact of war, in the case of workers compensation) that is covered by primary or excess property and casualty\\n\\ninsurance issued by an insurer if the loss occurs in the United States or at the premises of United States\\n\\nmissions or to certain air carriers or vessels.\", \"unique_id\": 581742}, \"14\": {\"sentence\": \"Insurer Deductible means, for the period beginning on January 1, 2015, and ending on December 31, 2020,\\n\\nan amount equal to 20% of our direct earned premiums, during the immediately preceding calendar year.\", \"unique_id\": 581743}, \"15\": {\"sentence\": \"Limitation of Liability\\n\\nThe Act limits our liability to you under this policy.\", \"unique_id\": 581744}, \"16\": {\"sentence\": \"If aggregate Insured Losses exceed $100,000,000,000 in a\\n\\ncalendar year and if we have met our Insurer Deductible, we are not liable for the payment of any portion of\\n\\nthe  amount    of  Insured  Losses     that  exceeds   $100,000,000,000;      and     for    aggregate    Insured        Losses  up  to\\n\\n$100,000,000,000, we will pay only a pro rata share of such Insured Losses as determined by the Secretary of\\n\\nthe Treasury.\", \"unique_id\": 581745}, \"17\": {\"sentence\": \"Policyholder Disclosure Notice\\n\\n1.\", \"unique_id\": 581746}, \"18\": {\"sentence\": \"Insured Losses would be partially reimbursed by the United States Government.\", \"unique_id\": 581747}, \"19\": {\"sentence\": \"If the aggregate industry\\n\\n     Insured Losses exceed:\\n\\n     a.\", \"unique_id\": 581748}, \"20\": {\"sentence\": \"$100,000,000,      with    respect  to  such  Insured  Losses  occurring        in  calendar   year        2015,  the   United\\n\\n         States Government would pay 85% of our Insured Losses that exceed our Insurer Deductible.\", \"unique_id\": 581749}, \"21\": {\"sentence\": \"b.\", \"unique_id\": 581750}, \"22\": {\"sentence\": \"$120,000,000,      with    respect  to  such  Insured  Losses  occurring        in  calendar   year        2016,  the   United\\n\\n         States Government would pay 84% of our Insured Losses that exceed our Insurer Deductible.\", \"unique_id\": 581751}, \"23\": {\"sentence\": \"c.  $140,000,000,      with    respect  to  such  Insured  Losses  occurring        in  calendar   year        2017,  the   United\\n\\n         States Government would pay 83% of our Insured Losses that exceed our Insurer Deductible.\", \"unique_id\": 581752}, \"24\": {\"sentence\": \"WC 00 04 22 B            Copyright 2015 National Council on Compensation Insurance, Inc. All Rights Reserved.\", \"unique_id\": 581753}, \"25\": {\"sentence\": \"Page 1 of 2\", \"unique_id\": 581754}}, \"17\": {\"1\": {\"sentence\": \"    d.   $160,000,000,  with  respect  to  such  Insured  Losses  occurring   in  calendar  year           2018,       the  United\\n\\n         States Government would pay 82% of our Insured Losses that exceed our Insurer Deductible.\", \"unique_id\": 581756}, \"2\": {\"sentence\": \"e.   $180,000,000,  with  respect  to  such  Insured  Losses  occurring   in  calendar  year           2019,       the  United\\n\\n         States Government would pay 81% of our Insured Losses that exceed our Insurer Deductible.\", \"unique_id\": 581757}, \"3\": {\"sentence\": \"f.   $200,000,000,  with  respect  to  such  Insured  Losses  occurring   in  calendar  year           2020,       the  United\\n\\n         States Government would pay 80% of our Insured Losses that exceed our Insurer Deductible.\", \"unique_id\": 581758}, \"4\": {\"sentence\": \"2.\", \"unique_id\": 581759}, \"5\": {\"sentence\": \"Notwithstanding item 1 above, the United States Government will not make any payment under the Act for\\n\\n    any portion of Insured Losses that exceed $100,000,000,000.\", \"unique_id\": 581760}, \"6\": {\"sentence\": \"3.\", \"unique_id\": 581761}, \"7\": {\"sentence\": \"The premium charge for the coverage your policy provides for Insured Losses is included in the amount\\n\\n    shown in Item 4 of the Information Page or in the Schedule below.\", \"unique_id\": 581762}, \"8\": {\"sentence\": \"Schedule\\n\\n                   State                                   Rate                                            Premium\\n\\n                                                 See      State  Schedule\\n\\nThis endorsement changes the policy to which it is attached and is effective on the date issued unless\\n\\notherwise stated.\", \"unique_id\": 581763}, \"9\": {\"sentence\": \"(The information below is required only when this endorsement is issued subsequent to preparation of the policy.)\", \"unique_id\": 581764}, \"10\": {\"sentence\": \"Endorsement Effective     06-15-2017       Policy No.\", \"unique_id\": 581765}, \"11\": {\"sentence\": \"EWC  024    52   06-04      Endorsement No.\", \"unique_id\": 581766}, \"12\": {\"sentence\": \"Insured  BLAKLEY   LAWNS      INC  AND  BLAKLEY  FERTILIZER       INC\\n\\nInsurance Company       THE   CINCINNATI   INDEMNITY       COMPANY                Premium $INCL\\n\\n                                                 Countersigned by\\n\\nWC 00 04 22 B       Copyright 2015 National Council on Compensation Insurance, Inc. All Rights Reserved.\", \"unique_id\": 581767}, \"13\": {\"sentence\": \"Page 2           of  2\", \"unique_id\": 581768}}, \"18\": {\"1\": {\"sentence\": \"WORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE POLICY\\n\\n             AUDIT NONCOMPLIANCE CHARGE ENDORSEMENT\\n\\nPart Five - Premium, Section G. (Audit) of the Workers Compensation and Employers Liability Insurance\\n\\nPolicy is revised by adding the following:\\n\\nIf you do not allow us to examine and audit all of your records that relate to this policy, and/or do not provide\\n\\naudit information as requested, we may apply an Audit Noncompliance Charge.\", \"unique_id\": 581770}, \"2\": {\"sentence\": \"The method for determining\\n\\nthe Audit Noncompliance Charge by state, where applicable, is shown in the Schedule below.\", \"unique_id\": 581771}, \"3\": {\"sentence\": \"If you allow us to examine and audit all of your records after we have applied an Audit Noncompliance Charge,\\n\\nwe will revise your premium in accordance with our manuals and Part 5 - Premium, E. (Final Premium) of this\\n\\npolicy.\", \"unique_id\": 581772}, \"4\": {\"sentence\": \"Failure to cooperate with this policy provision may result in the cancellation of your insurance coverage, as\\n\\nspecified under the policy.\", \"unique_id\": 581773}, \"5\": {\"sentence\": \"Note:\\n\\nFor coverage under state-approved workers compensation assigned risk plans, failure to cooperate with this\\n\\npolicy provision may affect your eligibility for coverage.\", \"unique_id\": 581774}, \"6\": {\"sentence\": \"Schedule\\n\\nState(s)     Basis of Audit Noncompliance Charge               Maximum Audit Noncompliance Charge Multiplier\\n\\nIL           ESTIMATED       ANNUAL         PREMIUM            UP  TO   TWO  TIMES\\n\\nThis endorsement changes the policy to which it is attached and is effective on the date issued unless\\n\\notherwise stated.\", \"unique_id\": 581775}, \"7\": {\"sentence\": \"(The information below is required only when this endorsement is issued subsequent to preparation of the policy.)\", \"unique_id\": 581776}, \"8\": {\"sentence\": \"Endorsement Effective        06-15-2017     Policy No.\", \"unique_id\": 581777}, \"9\": {\"sentence\": \"EWC  024  52  06-04   Endorsement No.\", \"unique_id\": 581778}, \"10\": {\"sentence\": \"Insured  BLAKLEY   LAWNS     INC  AND       BLAKLEY  FERTILIZER    INC\\n\\nInsurance Company      THE   CINCINNATI     INDEMNITY       COMPANY          Premium $INCL\\n\\n                                                     Countersigned by\\n\\nWC 00 04 24         Copyright 2015 National Council on Compensation Insurance, Inc. All Rights Reserved.\", \"unique_id\": 581779}}, \"19\": {\"1\": {\"sentence\": \"WORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE POLICY\\n\\n                             ILLINOIS AMENDATORY ENDORSEMENT\\n\\nThis endorsement applies only to the insurance provided by the policy because Illinois is shown in Item 3.A.\", \"unique_id\": 581781}, \"2\": {\"sentence\": \"of\\n\\nthe Information Page.\", \"unique_id\": 581782}, \"3\": {\"sentence\": \"Part Two-Employers Liability Insurance, Section B.\", \"unique_id\": 581783}, \"4\": {\"sentence\": \"(We Will Pay), Item 3. of the policy is replaced by the\\n\\nfollowing:\\n\\n3.\", \"unique_id\": 581784}, \"5\": {\"sentence\": \"For consequential bodily injury to a party to a civil union, spouse, child, parent, brother or sister of the\\n\\n    injured employee; provided that these damages are the direct consequence of bodily injury that arises out\\n\\n    of and in the course of the injured employees employment by you; and\\n\\nPart Five-Premium, Section G. (Audit) of the policy is replaced by the following:\\n\\nG.  Audit\\n\\n    You will let us examine and audit all your records that relate to this policy.\", \"unique_id\": 581785}, \"6\": {\"sentence\": \"These records include ledgers,\\n\\n    journals, registers, vouchers, contracts, tax reports, payroll and disbursement records, and programs for\\n\\n    storing and retrieving data.\", \"unique_id\": 581786}, \"7\": {\"sentence\": \"We may conduct the audits during regular business hours during the policy\\n\\n    period      and  within  three  years    after  the  policy  ends.\", \"unique_id\": 581787}, \"8\": {\"sentence\": \"Information  developed  by  audit         will  be  used  to\\n\\n    determine final premium.\", \"unique_id\": 581788}, \"9\": {\"sentence\": \"The National Council on Compensation Insurance has the same rights we have\\n\\n    under this provision.\", \"unique_id\": 581789}, \"10\": {\"sentence\": \"Part Six-Conditions, Section A.\", \"unique_id\": 581790}, \"11\": {\"sentence\": \"(Inspection) of the policy is replaced by the following:\\n\\nA.\", \"unique_id\": 581791}, \"12\": {\"sentence\": \"Inspection\\n\\n    We have the right, but are not obliged, to inspect your workplaces at any time.\", \"unique_id\": 581792}, \"13\": {\"sentence\": \"Our inspections are not\\n\\n    safety inspections.\", \"unique_id\": 581793}, \"14\": {\"sentence\": \"They relate only to the insurability of the workplaces and the premiums to be charged.\", \"unique_id\": 581794}, \"15\": {\"sentence\": \"We may give you reports on the conditions we find.\", \"unique_id\": 581795}, \"16\": {\"sentence\": \"We may also recommend changes.\", \"unique_id\": 581796}, \"17\": {\"sentence\": \"While they may\\n\\n    help reduce losses, we do not undertake to perform the duty of any person to provide for the health or\\n\\n    safety of your employees or the public.\", \"unique_id\": 581797}, \"18\": {\"sentence\": \"We do not warrant that your workplaces are safe or healthful or\\n\\n    that they comply with laws, regulations, codes, or standards.\", \"unique_id\": 581798}, \"19\": {\"sentence\": \"The National Council on Compensation\\n\\n    Insurance has the same rights we have under this provision.\", \"unique_id\": 581799}, \"20\": {\"sentence\": \"Part Six-Conditions, Section D. (Cancellation) of the policy is replaced by the following:\\n\\nD.  Cancellation\\n\\n    1.\", \"unique_id\": 581800}, \"21\": {\"sentence\": \"You may cancel this policy.\", \"unique_id\": 581801}, \"22\": {\"sentence\": \"You will mail or deliver advance written notice to us, stating when the\\n\\n            cancellation is to take effect.\", \"unique_id\": 581802}, \"23\": {\"sentence\": \"2.\", \"unique_id\": 581803}, \"24\": {\"sentence\": \"We may cancel this policy.\", \"unique_id\": 581804}, \"25\": {\"sentence\": \"We will mail to each named insured at the last known mailing address\\n\\n            advance written notice stating when the cancellation is to take effect.\", \"unique_id\": 581805}, \"26\": {\"sentence\": \"We will maintain proof of\\n\\n            mailing of the notice of cancellation.\", \"unique_id\": 581806}, \"27\": {\"sentence\": \"A copy of all such notices shall be sent to the broker or agent of\\n\\n            record, if known, at the last known mailing address.\", \"unique_id\": 581807}, \"28\": {\"sentence\": \"The broker or agent of record may opt to accept\\n\\n            notification electronically.\", \"unique_id\": 581808}, \"29\": {\"sentence\": \"3.\", \"unique_id\": 581809}, \"30\": {\"sentence\": \"If we cancel because you do not pay all premium when due, we will mail the notice of cancellation at\\n\\n            least ten days before the cancellation is to take effect.\", \"unique_id\": 581810}, \"31\": {\"sentence\": \"If we cancel for any other reason, we will mail\\n\\n            the notice:\\n\\n            a.\", \"unique_id\": 581811}, \"32\": {\"sentence\": \"At least 30 days before the cancellation is to take effect if the policy has been in force for 60 days\\n\\n                or less;\\n\\n            b.\", \"unique_id\": 581812}, \"33\": {\"sentence\": \"At least 60 days before the cancellation is to take effect if the policy has been in force for 61 days\\n\\n                or more.\", \"unique_id\": 581813}, \"34\": {\"sentence\": \"4.\", \"unique_id\": 581814}, \"35\": {\"sentence\": \"If this policy has been in effect for 60 days or more, we may cancel only for one of the following\\n\\n        reasons:\\n\\n            a.  Nonpayment of premium;\\n\\n            b.\", \"unique_id\": 581815}, \"36\": {\"sentence\": \"The policy was issued because of a material misrepresentation;\\n\\n            c.  You violated any of the terms and conditions of the policy;\\n\\n            d.  The risk originally accepted has measurably increased;\\n\\nWC 12 06 01 E              Copyright 2014 National Council on Compensation Insurance, Inc. All Rights Reserved.\", \"unique_id\": 581816}, \"37\": {\"sentence\": \"Page 1 of 2\", \"unique_id\": 581817}}, \"20\": {\"1\": {\"sentence\": \"         e.  The Director has determined that we no longer have adequate reinsurance to meet our needs; or\\n\\n         f.  The Director has determined that continuation of coverage could place us in violation of the laws\\n\\n             of Illinois.\", \"unique_id\": 581819}, \"2\": {\"sentence\": \"5.\", \"unique_id\": 581820}, \"3\": {\"sentence\": \"Our notice of cancellation will state our reasons for cancelling.\", \"unique_id\": 581821}, \"4\": {\"sentence\": \"6.\", \"unique_id\": 581822}, \"5\": {\"sentence\": \"The policy period will end on the day and hour stated in the cancellation notice.\", \"unique_id\": 581823}, \"6\": {\"sentence\": \"Part Six-Conditions, Section E. (Sole Representative) of the policy is replaced by the following:\\n\\nE.  Sole Representative\\n\\n    The insured first named in Item 1 of the Information Page will act on behalf of all insureds to change this\\n\\n    policy, receive return premium, or give us notice of cancellation.\", \"unique_id\": 581824}, \"7\": {\"sentence\": \"Part Six-Conditions of the policy is changed by adding the following:\\n\\nF.  Nonrenewal\\n\\n    1.\", \"unique_id\": 581825}, \"8\": {\"sentence\": \"We may elect not to renew the policy.\", \"unique_id\": 581826}, \"9\": {\"sentence\": \"If we fail to give at least 60 days notice prior to the expiration\\n\\n         date of the current policy, the policy will automatically be extended for one year.\", \"unique_id\": 581827}, \"10\": {\"sentence\": \"We will mail to each\\n\\n         named insured the nonrenewal notice at the last known mailing address.\", \"unique_id\": 581828}, \"11\": {\"sentence\": \"We will maintain proof of\\n\\n         mailing of the nonrenewal notice.\", \"unique_id\": 581829}, \"12\": {\"sentence\": \"An exact and unaltered copy of such notice will also be sent to the\\n\\n         named insureds producer, if known, or the producer of record at the last known mailing address.\", \"unique_id\": 581830}, \"13\": {\"sentence\": \"The\\n\\n         named     insureds  producer,   if  known,   or  the  producer    of  record  may  opt  to       accept      notification\\n\\n         electronically.\", \"unique_id\": 581831}, \"14\": {\"sentence\": \"2.\", \"unique_id\": 581832}, \"15\": {\"sentence\": \"Our notice of nonrenewal will state our reasons for not renewing.\", \"unique_id\": 581833}, \"16\": {\"sentence\": \"3.\", \"unique_id\": 581834}, \"17\": {\"sentence\": \"If we fail to provide the notice of nonrenewal as required, the policy will still terminate on its expiration\\n\\n         date if:\\n\\n         a.\", \"unique_id\": 581835}, \"18\": {\"sentence\": \"You notify us or the producer who procured this policy that you do not want the policy renewed;\\n\\n             or\\n\\n         b.\", \"unique_id\": 581836}, \"19\": {\"sentence\": \"You fail to pay all premiums when due; or\\n\\n         c.  You obtain other insurance as a replacement of the policy.\", \"unique_id\": 581837}, \"20\": {\"sentence\": \"This endorsement changes the policy to which it is attached and is effective on the date issued unless\\n\\notherwise stated.\", \"unique_id\": 581838}, \"21\": {\"sentence\": \"(The information below is required only when this endorsement is issued subsequent to preparation of the policy.)\", \"unique_id\": 581839}, \"22\": {\"sentence\": \"Endorsement Effective      06-15-2017         Policy No.\", \"unique_id\": 581840}, \"23\": {\"sentence\": \"EWC    024  52     06-04       Endorsement No.\", \"unique_id\": 581841}, \"24\": {\"sentence\": \"Insured  BLAKLEY   LAWNS        INC  AND      BLAKLEY  FERTILIZER       INC\\n\\nInsurance Company          THE  CINCINNATI    INDEMNITY         COMPANY                 Premium $INCL\\n\\n                                                       Countersigned by\\n\\nWC 12 06 01 E       Copyright 2014 National Council on Compensation Insurance, Inc. All Rights Reserved.\", \"unique_id\": 581842}, \"25\": {\"sentence\": \"Page 2       of  2\", \"unique_id\": 581843}}}, \n",
    "#     \"knowledge_graph\":\n",
    "#     [[\"Location_State\",\"Location_Tag\"],[\"Location_State\",\"Location_SIC\"],[\"Location_State\",\"Location_Class_Code\"], [\"Location_Class_Code\",\"Location_Categories_duties_clasification_line1\"],[\"Location_Class_Code\", \"Location_Categories_duties_clasification_line2\"],[\"Location_Class_Code\", \"Location_Rate\"], [\"Location_Class_Code\", \"Location_Esti_Annual_Remuneration_Pyroll\"], [\"Location_Class_Code\", \"Location_Esti_Annual_Manual_Premium\"], [\"Location_State\", \"Location_NAICS\"], [\"Location_State\", \"Tot_Premium_Blank_field_line1\"], [\"Location_State\", \"Tot_Premium_Blank_field_line2\"], [\"Location_State\", \"Tot_Premium_Blank_field_rate\"], [\"Location_State\", \"Tot_Premium_Blank_filed_amount\"], [\"Location_State\", \"Tot_Premium_Increased_Limits_rate\"], [\"Location_State\", \"Tot_Premium_Increased_Limits_amount\"], [\"Location_State\", \"Tot_Premium_Deductible_rate\"], [\"Location_State\", \"Tot_Premium_Deductible_amount\"], [\"Location_State\", \"Tot_Premium_Exp_or_Modifi_rate\"], [\"Location_State\", \"Tot_Premium_Exp_or_Modifi_amount\"], [\"Location_State\", \"Tot_Premium_Expense_Constant_rate\"], [\"Location_State\", \"Tot_Premium_Expense_Constant_amount\"], [\"Location_State\", \"Location_Address_line1\"],[\"Location_State\", \"Location_Address_line2\"],[\"Location_State\", \"Location_Address_line3\"],[\"Location_State\", \"Location_Address_line4\"],[\"Location_State\", \"Tot_Premium_Schedule_Rating-Modifi_rate\"],[\"Location_State\", \"Tot_Premium_Schedule_Rating-Modifi_amount\"]], \n",
    "#     \"root_level_ents\": \n",
    "#     [\"Policy_Number\", \"Policy_Effective_Date\", \"Applicant_Name\", \"Location_State\", \"Line_Issuing_Company\", \"Line_Billing_Type\", \"Policy_Commission\", \"Policy_info_Other_States_Insurance\", \"Policy_info_States_Insurance\", \"Policy_Info_Each_accident\", \"Policy_Info_Disease_each_employee\", \"Policy_Info_Disease_policy_limit\", \"Applicant_Business_Type\", \"Policy_Tot_Minimum_Premium_all_States\", \"Policy_Total_Deposit_Premium_all_States\", \"Applicant_Mailing_Address_line1\", \"Applicant_Mailing_Address_line2\", \"Applicant_Mailing_Address_line3\", \"Applicant_Mailing_Address_line4\", \"Tot_Premium_Taxes_Assessments_rate\", \"Tot_Premium_Taxes_Assessments_amount\", \"Tot_Premium_Taxes_Assessments_field\", \"Applicant_SIC\", \"Policy_Estimated Premium\", \"Line_Commision_percent\"], \"entity_order_map\": \n",
    "#     {\"Policy_Number\": 1, \"Policy_Effective_Date\": 2, \"Applicant_Name\": 3, \"Location_State\": 4, \"Location_Tag\": 5, \"Location_SIC\": 6, \"Location_Class_Code\": 7, \"Location_Categories_duties_clasification_line1\": 8, \"Location_Categories_duties_clasification_line2\": 9, \"Location_Rate\": 10, \"Location_Esti_Annual_Remuneration_Pyroll\": 11, \"Location_Esti_Annual_Manual_Premium\": 12, \"Location_NAICS\": 13, \"Tot_Premium_Blank_field_line1\": 14, \"Tot_Premium_Blank_field_line2\": 15, \"Tot_Premium_Blank_field_rate\": 16, \"Tot_Premium_Blank_filed_amount\": 17, \"Tot_Premium_Increased_Limits_rate\": 18, \"Tot_Premium_Increased_Limits_amount\": 19, \"Tot_Premium_Deductible_rate\": 20, \"Tot_Premium_Deductible_amount\": 21, \"Tot_Premium_Exp_or_Modifi_rate\": 22, \"Tot_Premium_Exp_or_Modifi_amount\": 23, \"Tot_Premium_Expense_Constant_rate\": 24, \"Tot_Premium_Expense_Constant_amount\": 25, \"Location_Address_line1\": 26, \"Location_Address_line2\": 27, \"Location_Address_line3\": 28, \"Location_Address_line4\": 29, \"Tot_Premium_Schedule_Rating-Modifi_rate\": 30, \"Tot_Premium_Schedule_Rating-Modifi_amount\": 31, \"Line_Issuing_Company\": 32, \"Line_Billing_Type\": 33, \"Policy_Commission\": 34, \"Policy_info_Other_States_Insurance\": 35, \"Policy_info_States_Insurance\": 36, \"Policy_Info_Each_accident\": 37, \"Policy_Info_Disease_each_employee\": 38, \"Policy_Info_Disease_policy_limit\": 39, \"Applicant_Business_Type\": 40, \"Policy_Tot_Minimum_Premium_all_States\": 41, \"Policy_Total_Deposit_Premium_all_States\": 42, \"Applicant_Mailing_Address_line1\": 43, \"Applicant_Mailing_Address_line2\": 44, \"Applicant_Mailing_Address_line3\": 45, \"Applicant_Mailing_Address_line4\": 46, \"Tot_Premium_Taxes_Assessments_rate\": 47, \"Tot_Premium_Taxes_Assessments_amount\": 48, \"Tot_Premium_Taxes_Assessments_field\": 49, \"Applicant_SIC\": 50, \"Policy_Estimated Premium\": 51, \"Line_Commision_percent\": 52}, \n",
    "#     \"language\": \"english\", \n",
    "#     \"mode_type\": \"mode_1\"}}\n",
    "#         result = predict_ner_CC(model,req)\n",
    "#         print(result)\n",
    "\n",
    "#     # Validate the predict_Doc_classification function with sample data\n",
    "\n",
    "#     if 'DC' in model_type :\n",
    "#         import requests\n",
    "#         req = requests.Request()\n",
    "#         req.json={\"payload\":{\"model_id\": 191, \"text\": {\"file_data\": \"                                                               The Cincinnati Insurance Company\\n\\n                                                               The Cincinnati Casualty Company\\n\\n                                                               The Cincinnati Indemnity Company\\n\\n                        ABOUT YOUR CINCINNATI POLICY\\n\\nPolicy Number:     EWC  024  52  06-04\\n\\nEffective Date:    06-15-2017\\n\\nNamed Insured:     BLAKLEY   LAWNS  INC     AND  BLAKLEY           FERTILIZER  INC\\n\\nOur  Cincinnati    Customer         Care    Center             provides        service    on   behalf  of                         your\\n\\nagency.      Your  requested        transaction                is  attached.   If         you  have  questions\\n\\nregarding this transaction, please contact us. For contact information, please\\n\\nrefer to the My Policies page.\\n\\n The Cincinnati Insurance Companies and  Cincinnati refer to one or more companies of the insurer group\\n                                                      a8 The                                           a8 The Cincinnati\\nproviding  property and casualty coverages  through                Cincinnati  Insurance  Company,\\nCasualty Company, or  a8X The Cincinnati Indemnity Company. Each insurer has sole financial responsibility for\\n\\nits own products. Not all subsidiaries operate in all states.\\n\\nMailing Address: P.O. Box 145496  b7 Cincinnati, Ohio 45250-5496  b7 Headquarters: 6200 S. Gilmore Road  b7 Fairfield, Ohio 45014-5141\\n\\n                                                 www.cinfin.com\\n\\nWC 98 06 31\\n 0c                       WORKERS\\n COMP. COVER SHEET\\n\\nNamed Insured:   BLAKLEY    LAWNS   INC  AND  BLAKLEY  FERTILIZER     INC\\n\\nPolicy Number:   EWC   024  52  06-04\\n\\nEffective Date:  06-15-2017\\n\\nAgency Name:     T.J.  NICOUD    &  COMPANY   12-117\\n\\n** FOR INTERNAL PURPOSES ONLY, PLEASE DISCARD THIS COVER SHEET             AFTER  ATTACHING  ALL\\n\\nAPPLICABLE FORMS **\\n\\nPlease manually insert the following forms and send with the policy:\\n\\nCLAIMS  LETTER   WITH  STICKERS\\n\\nWC 98 06 11\\n 0c                                                          The Cincinnati Insurance Company\\n\\n                                                              The Cincinnati Casualty Company\\n\\n                                                          The Cincinnati Indemnity Company\\n\\n Policy Number: EWC        024  52  06-04\\n\\nEffective Date: 06-15-2017\\n\\nNamed Insured:       BLAKLEY   LAWNS  INC   AND      BLAKLEY  FERTILIZER         INC\\n\\nFor professional advice and policy questions or changes, please contact your local independent agency:\\n\\n T.J.  NICOUD      &  COMPANY\\n\\nP.O.  BOX    13078\\n\\nSPRINGFIELD,         IL   62791-3078\\n\\n877-242-2544\\n\\nDear Policyholder:\\n\\nThank you\\n\\n Thank you for trusting The Cincinnati Insurance Companies with your workers\\n compensation coverage. We\\n\\nrecognize that locally based independent agents have the working knowledge to help you choose the right\\n\\ninsurance company for your needs. Together with your local independent insurance agency, we are committed\\n\\nto providing you with the highest level of service.\\n\\nPlease review your enclosed policy information to verify your coverage details, as well as deductibles and\\n\\ncoverage amounts. Should your needs change, your agent is available to review and update your policy.\\n\\nPlease promptly report claims\\n\\nTo report a workers\\n' compensation claim, please call Cincinnati Insurance directly, any time day or night, using\\n\\nour toll-free Claims Reporting Center line at 877-242-2544. Please have claim-related information ready -\\n\\nincluding   your  policy  number,  injured  employee    information,  type  and  cause  of  injury  and  any  treatment\\n\\nspecifics - so our associates can expedite your claim.\\n\\nFor information about minimizing and controlling your workers\\n' compensation exposures and costs, you may\\n\\nrequest a Claims Kit from your local independent agency.\\n\\nSincerely,\\n\\nSteve Spray\\n\\nSenior Vice President - Commercial Lines\\n\\nWC 98 06 46\\n 0c                                                     The Cincinnati Indemnity Company\\n\\n                                                                     A Stock Insurance Company\\n\\n                                                Headquarters: 6200 S. Gilmore Road, Fairfield, OH 45014-5141\\n\\n                                                     Mailing address: P.O. Box 145496, Cincinnati, OH 45250-5496\\n\\n                                                             www.cinfin.com      n              513-870-2000\\n\\nWORKERS COMPENSATION AND EMPLOYERS LIABILITY INSURANCE\\n\\n                                     POLICY INFORMATION PAGE\\n\\nPolicy No.                              Policy Period        Previous Policy No.                    Billing Method\\n\\n                                  From                   To\\n\\nEWC     024  52  06-04        06-15-2017        06-15-2018   EWC0245206-02                          DIRECT  BILL\\n\\nAgency                   Carrier             Risk ID No.     Entity\\n\\n12-117                   27197                               CORPORATION\\n\\nAgent\\n\\nCINCINNATI       CUSTOMER      CARE     CENTER\\n\\nT.J.    NICOUD   &     COMPANY\\n\\nP.O.    BOX     13078\\n\\nSPRINGFIELD,         IL  62791-3078\\n\\n1.  Named Insured and Address\\n\\n    BLAKLEY      LAWNS   INC   AND   BLAKLEY    FERTILIZER   INC\\n\\n    3400     W  GRAND    AVE\\n\\n    SPRINGFIELD,         IL   62711-7351\\n\\n2.  The Policy Period is from     06-15-2017         to  06-15-2018  12:01 am. The Standard Time at the insured\\\" s\\n\\n    Mailing address.\\n\\n3.  A.  Workers Compensation Insurance: Part ONE of the policy applies to the Workers Compensation Law of the\\n\\n        states listed here:   IL\\n\\n    B.  Employers Liability Insurance: Part TWO of the policy applies to work in each state listed in Item 3A.                       The\\n\\n        limits of our liability under Part TWO are:\\n\\n                         Bodily Injury by Accident       $ 100,000        each accident\\n\\n                         Bodily Injury by Disease        $ 100,000        each employee\\n\\n                         Bodily Injury by Disease        $ 500,000        policy limit\\n\\n    C. Other States Insurance: Part THREE of the policy applies to all states except\\n\\n        North    Dakota,      Ohio,     Washington,      Wyoming,   and  States  Designated                in              Item  3A  of\\n\\n        the     information       page  and  Alaska\\n\\n    D. This policy includes these endorsements and schedules:\\n\\n        REFER    TO  ENDORSEMENT        SCHEDULE\\n\\n4.  The premium for this policy will be determined by our Manual of Rules, Classifications, Rates, and Rating\\n\\n    Plans. All information required below is subject to verification and change by audit.\\n\\n                                     SEE EXTENSION OF INFORMATION PAGE\\n\\n05-12-2017       07:10\\n\\n                 Includes copyright material of the National Council on Compensation Insurance, used with its permission.\\n\\n                                    Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01 A (01 13)                                                     EWC    024            52  06-04                  Page  1   of  8\\n 0cMinimum Premium        $  1,250  Estimated Annual Premium                                        $ 4,865\\n\\n                                 Deposit Premium                                                 $ 4,865\\n\\n                                 *See Schedule Attached Taxes / Assessments                      $ 49\\n\\n                                 Total Premium Due                                               $ 4,914\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\\n\\n                          Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01 A (01 13)            EWC  024                                             52  06-04                       Page  2  of  8\\n 0c                             EXTENSION OF INFORMATION PAGE\\n\\n                                        Name and Location Schedule\\n\\nLoc.  Insured\\n\\nNo.         No.                         Name / Address\\n\\n001         001   BLAKLEY    LAWNS  INC  AND  BLAKLEY   FERTILIZER  INC\\n\\n                  3400    W  GRAND  AVE\\n\\n                  SPRINGFIELD,      IL   62711-7351\\n\\nFederal Employee  ID No.                      Entity\\n\\n37-1158420                                    CORPORATION\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\\n\\n                             Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01 A (01 13)                                               EWC  024             52  06-04                    Page  3  of  8\\n 0c                          EXTENSION OF INFORMATION PAGE\\n\\n                                  Installment Schedule\\n\\nSEE   BILLING  STATEMENT  MAILED  SEPARATELY\\n\\nDate                   Premium                Taxes / Assessments                         Installment\\n\\n05-12-2017  07:10\\n\\n               Includes copyright material of the National Council on Compensation Insurance, used with its permission.\\n\\n                          Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01 A (01 13)                                   EWC        024                52  06-04                          Page  4  of  8\\n 0c                       EXTENSION OF INFORMATION PAGE\\n\\n                                  Taxes / Assessments Schedule\\n\\nTaxes / Assessments               Rate / Percentage                                    Premium\\n\\nIL  OPERATIONS  FUND   SURCHARGE  1.01%                                                $49\\n\\n                                  Total Taxes / Assessments                            $49\\n\\n05-12-2017  07:10\\n\\n            Includes copyright material of the National Council on Compensation Insurance, used with its permission.\\n\\n                       Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01 A (01 13)                                           EWC  024           52  06-04                          Page  5  of  8\\n 0c                               EXTENSION OF INFORMATION PAGE\\n\\n                                         4. Classification of Operations\\n\\n                                                  State: IL\\n\\n                                                       Premium Basis\\n\\n                                                       Total Estimated\\n\\nLoc.  Code                                                   Annual            Rate per $100                           Estimated Annual\\n\\nNo.   No.    Classification Description                      Remuneration      of Remuneration                         Premium\\n\\n001   8742   SALESPERSONS     OR                             55,513            0.440                                   244\\n\\n             COLLECTORS-OUTSIDE\\n\\n             06-15-2017   TO   06-15-2018\\n\\n001   8810   CLERICAL   OFFICE    EMPLOYEES       NOC        IF  ANY           0.220                                          0\\n\\n             06-15-2017   TO   06-15-2018\\n\\n001   9102   PARK   NOC-ALL   EMPLOYEES        &             74,149            5.380                                   3,989\\n\\n             DRIVERS\\n\\n             06-15-2017   TO   06-15-2018\\n\\n                                                                          Manual Premium            $                  4,233\\n\\n      9889   SCHEDULE   MODIFICATION                                           1.0900                                  381\\n\\n             06-15-2017   TO   06-15-2018\\n\\n                                                                           State Premium            $                  4,614\\n\\n      0900   EXPENSE    CONSTANT                                                                                       160\\n\\n             06-15-2017   TO   06-15-2018\\n\\n      9740   TERRORISM                                                         0.0600                                         78\\n\\n             06-15-2017   TO   06-15-2018\\n\\n      9741   CATASTROPHE     (OTHER      THAN                                  0.0100                                         13\\n\\n             CERTIFIED   ACTS     OF     TERRORISM)\\n\\n             06-15-2017   TO   06-15-2018\\n\\n05-12-2017   07:10\\n\\n             Includes copyright material of the National Council on Compensation Insurance, used with its permission.\\n\\n                             Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01  A (01 13)                                                    EWC  024       52  06-04                     Page   6   of     8\\n 0c                                 State Total Estimated Premium                                          $  4,865\\n\\n05-12-2017   07:10\\n\\n             Includes copyright  material of the National Council on Compensation Insurance, used with its permission.\\n\\n                                 Copyright 1987 National Council on Compensation Insurance.\\n\\nWC 00 00 01  A (01 13)           EWC  024                                                    52  06-04                  Page  7  of  8\\n 0c\" , \" file_id\" : \" 2786\" }}}\n",
    "#         result = predict_Doc_classification(model,req)\n",
    "#         print(result)\n",
    "\n",
    "#     if 'IC' in model_type :\n",
    "#         import requests\n",
    "#         req = requests.Request()\n",
    "#         #req.json={\"payload\":{'model_id': 158, 'input_img_path': '/app/entityextractor/core_backend/data/intermediate_store/data/image_classification/powder_21.jpeg', 'input_img_type': 'jpeg'}}\n",
    "#         req.json={\"payload\":{'model_id': 158, 'input_img_path': 'batch/38651_powder_23/powder_23.jpeg', 'input_img_type': 'jpeg'}}\n",
    "#         result = predict_IC(model,req)\n",
    "#         print(result)\n",
    "\n",
    "#     if 'Object Identification' in model_type :\n",
    "#         import requests\n",
    "#         req = requests.Request()\n",
    "#         req.json={\"payload\":{'model_id': 535, 'image_path': 'batch/38712_NP_I_10/NP_I_10.png'}}\n",
    "#         result = predict_zoning_inference(model,req)\n",
    "#         print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For jupyter execusion\n",
    "# Name = eval(os.getenv('name',''))\n",
    "# Description =  os.getenv('description','')\n",
    "# for UI execustion\n",
    "Name = os.environ['name']\n",
    "\n",
    "Description = os.environ['description']\n",
    "\n",
    "\n",
    "print(\"name......\",Name)\n",
    "print(\"description...\",Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name = \"PC_NB_OI_350_11052020_41\"\n",
    "Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files paths to upload to s3\n",
    "import os,time\n",
    "#FileBasePath = '/notebooks/notebooks/EEF_NER_LSTM_Training/Train_models/'\n",
    "FileBasePath = '/notebooks/Train_models/'\n",
    "Folder_name = TrainModel['model_id']\n",
    "# Folder_name =  Folder_name = model_path.split('/')[-1]\n",
    "files = os.listdir(FileBasePath+Folder_name)\n",
    "# files = os.listdir(model_path)\n",
    "\n",
    "print(\"FileBasePath.............\",FileBasePath)\n",
    "print(\"Folder_name.............\",Folder_name)\n",
    "print(\"files.............\",files)\n",
    "print(\"files_Path.............\",FileBasePath+Folder_name+'/'+files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to s3\n",
    "# if len(model_type)==1:\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    \n",
    "    #FileBasePath = '/notebooks/notebooks/EEF_NER_LSTM_Training/Train_models/'\n",
    "    FileBasePath = '/notebooks/Train_models/'\n",
    "    Folder_name = TrainModel['model_id']\n",
    "    # Folder_name =  Folder_name = model_path.split('/')[-1]\n",
    "    files = os.listdir(FileBasePath+Folder_name)\n",
    "    # files = os.listdir(model_path)\n",
    "\n",
    "    print(\"FileBasePath.............\",FileBasePath)\n",
    "    print(\"Folder_name.............\",Folder_name)\n",
    "    print(\"files.............\",files)\n",
    "    print(\"files_Path.............\",FileBasePath+Folder_name+'/'+files[0])\n",
    "\n",
    "    print(\"Upload files to s3 in progress\")\n",
    "    #uploading log file\n",
    "    if OnPremiseFlag!=True:\n",
    "        upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "    else:\n",
    "        if not os.path.exists('/data/EEF_models/'+Folder_name):\n",
    "            os.makedirs('/data/EEF_models/'+Folder_name)\n",
    "        shutil.copy('/home/mosaic-ai/NER_Training_Service.log','/data/EEF_models/'+Folder_name)\n",
    "\n",
    "    for i in files:\n",
    "        file_chk = FileBasePath+Folder_name+'/'+i\n",
    "        if os.path.isfile(file_chk)==True:\n",
    "            start_time = time.time()\n",
    "            print(\"uploading file \",i)\n",
    "            print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "            if OnPremiseFlag!=True:\n",
    "                upload_file(FileBasePath+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "                print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "            else:\n",
    "                shutil.copy(FileBasePath+Folder_name+'/'+i,'/data/EEF_models/'+Folder_name)\n",
    "\n",
    "    print(\"Upload files to s3 in completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed this loop as OI retraining needs all files of model.ckpt\n",
    "# # Upload files to s3\n",
    "# OD_Files = ['train.csv', 'test.csv', 'label_map.pbtxt', 'train.record', 'test.record', 'pipeline.config', 'graph.pbtxt','frozen_inference_graph.pb']\n",
    "# print(\"Upload files to s3 in progress\")\n",
    "# for i in files:\n",
    "#     start_time = time.time()\n",
    "#     print(\"uploading file \",i)\n",
    "#     if model_type[0] == 'Object Identification':\n",
    "#         if i in OD_Files:\n",
    "#             print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "#             upload_file(FileBasePath+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "#             print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#     if model_type[0] != 'Object Identification':\n",
    "#         print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "#         upload_file(FileBasePath+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "#         print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "\n",
    "# print(\"Upload files to s3 in completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update Trained model\n",
    "\n",
    "# if len(model_type)==1:\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    ml_model_id = TrainModel['model_id'].split(\"__\")[0]\n",
    "    version_id = TrainModel['model_id'].split(\"__\")[1]\n",
    "    print(\"ml_model_id...\",ml_model_id)\n",
    "    print(\"version_id...\",version_id)\n",
    "    model_obj = model\n",
    "    \n",
    "    if 'NER' in model_type and CCFlag == 'True':    \n",
    "#         RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,  description=Description, scoring_func=predict_ner_CC)\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner_CC, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"NER UpdateModel Details..\", UpdateModel)\n",
    "#         model_details=RegisterModel['versions'][0]['object_url'].split('/')\n",
    "#         print(\"NER model details...\", model_details)\n",
    "\n",
    "    if 'NER' in model_type and CCFlag == 'False':    \n",
    "#         RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,  description=Description, scoring_func=predict_ner)\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"NER UpdateModel Details..\", UpdateModel)\n",
    "#         model_details=RegisterModel['versions'][0]['object_url'].split('/')\n",
    "#         print(\"NER model details...\", model_details)\n",
    "\n",
    "    if 'DC' in model_type :\n",
    "#         RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.sklearn,  description=Description, scoring_func=predict_Doc_classification)\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_Doc_classification, flavour = MLModelFlavours.sklearn, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Document Classification UpdateModel Details..\", UpdateModel)\n",
    "#         model_details=RegisterModel['versions'][0]['object_url'].split('/')\n",
    "#         print(\"Document Classification model details...\", model_details)    \n",
    "\n",
    "    if 'IC' in model_type :\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,  description=Description, scoring_func=predict_IC)\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,init_script=\"mkdir temp \\\\n git clone https://mosaic-ai-qa-user:H8Hbdozkypt1TNFFVEit@git.lti-aiq.in/mosaic-ai-qa-user/5ea452e7-975b-4c78-b08a-a5100b5e183a.git/temp \\\\n cd /temp/notebooks \\\\n cp -r EEF_NER_LSTM_Training  /usr/local/lib/python3.7/site-packages/ \\\\n python -c 'import sys; sys.path.insert(0,\\\\\\\"/usr/local/lib/python3.7/site-packages/EEF_NER_LSTM_Training\\\\\\\")' \\\\n\"  ,description=Description, scoring_func=predict_IC)\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,init_script=\"mkdir temp \\\\n git clone https://mosaic-ai-qa-user:H8Hbdozkypt1TNFFVEit@git.lti-aiq.in/mosaic-ai-qa-user/5ea452e7-975b-4c78-b08a-a5100b5e183a.git temp/ \\\\n cd /temp/notebooks \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training  /usr/local/lib/python3.7/site-packages/ \\\\n python -c 'import sys; sys.path.insert(0,\\\\\\\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\\\\\\\");sys.path.insert(1,\\\\\\\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\\\\\\\")' \\\\n\"  ,description=Description, scoring_func=predict_IC)\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,init_script=\"mkdir temp \\\\n git clone https://mosaic-ai-qa-user:H8Hbdozkypt1TNFFVEit@git.lti-aiq.in/mosaic-ai-qa-user/5ea452e7-975b-4c78-b08a-a5100b5e183a.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\"  ,description=Description, scoring_func=predict_IC)\n",
    "#         RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\"  ,description=Description, scoring_func=predict_IC)\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_IC, flavour = MLModelFlavours.keras, init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\"  , schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.keras,init_script=\"pip install --user https://mosaic-ai-qa-user:H8Hbdozkypt1TNFFVEit@git.lti-aiq.in/mosaic-ai-qa-user/5ea452e7-975b-4c78-b08a-a5100b5e183a.git \\\\n\"  ,description=Description, scoring_func=predict_IC)\n",
    "        print(\"Image Classification UpdateModel Details..\", UpdateModel)\n",
    "#         print(type(model))\n",
    "#         model_details=RegisterModel['versions'][0]['object_url'].split('/')\n",
    "#         print(\"Image Classification model details...\", model_details)    \n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.tensorflow, init_script=\"mkdir temp \\\\n git clone --depth 1 https://services-dev-user:8xP5MGdw7HNTBztfANXG@git.lti-aiq.in/services-dev-user/bd4fa05d-ce42-4bee-9928-a3b94c716286.git temp/ \\\\n cd /temp/notebooks \\\\n cp -r EEF_NER_LSTM_Training  /usr/local/lib/python3.7/site-packages/ \\\\n python -c 'import sys; sys.path.insert(0,\\\"/usr/local/lib/python3.7/site-packages/EEF_NER_LSTM_Training\\\")' \\\\n\" , description=Description, scoring_func=predict_zoning_inference)\n",
    "        #RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.tensorflow, init_script=\"mkdir temp \\\\n git clone https://services-dev-user:8xP5MGdw7HNTBztfANXG@git.lti-aiq.in/services-dev-user/bd4fa05d-ce42-4bee-9928-a3b94c716286.git temp/ \\\\n cd /temp/notebooks \\\\n cp -r EEF_NER_LSTM_Training  /usr/local/lib/python3.7/site-packages/ \\\\n python -c 'import sys; sys.path.insert(0,\\\\\\\"/usr/local/lib/python3.7/site-packages/EEF_NER_LSTM_Training\\\\\\\")' \\\\n\" , description=Description, scoring_func=predict_zoning_inference)\n",
    "#         RegisterModel = register_model(model, name=Name, flavour=MLModelFlavours.tensorflow,init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\"  ,description=Description, scoring_func=predict_zoning_inference)\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_zoning_inference, flavour = MLModelFlavours.tensorflow, init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\" , schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Object Identification UpdateModel Details..\", UpdateModel)\n",
    "#         model_details=RegisterModel['versions'][0]['object_url'].split('/')\n",
    "#         print(\"Object Identification model details...\", model_details)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble_ModelDetails......\",Ensemble_ModelDetails,len(Ensemble_ModelDetails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)>0:\n",
    "    for Each_Model in Ensemble_ModelDetails:\n",
    "        if (('NER' in Each_Model['model_type']) and (Each_Model['CCFlag'] == 'False')):\n",
    "            model_path = os.path.join(BASE_PATH, Each_Model['model_id'])\n",
    "            print(\"model_path...\",model_path)\n",
    "            print(\"model_type\",\"NER\")\n",
    "            print(\"CCFlag False loop\")\n",
    "            model_file_path = model_path+'/valid_char_set.pkl'\n",
    "            model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "            print(\"model_file_path...............\",model_file_path)\n",
    "            word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "            label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "            case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "            char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "            valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "            print(\"NER model path...\",model_path)\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Uploading the model files to s3\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(BASE_PATH+Each_Model['model_id'])\n",
    "            print(\"list of files uploading to s3\", files_list)\n",
    "            print(\"Model id folder \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "                except:\n",
    "                    print(\"file NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists('/data/EEF_models/'+Folder_name):\n",
    "                    os.makedirs('/data/EEF_models/'+Folder_name)\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log','/data/EEF_models/'+Folder_name)\n",
    "            \n",
    "            for i in files_list:\n",
    "                file_chk = BASE_PATH+Folder_name+'/'+i\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    print(\"uploading file \",i)\n",
    "                    print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        upload_file(BASE_PATH+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "                        print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(BASE_PATH+Folder_name+'/'+i,'/data/EEF_models/'+Folder_name)\n",
    "                        print(\"model copied to /data PV\")\n",
    "\n",
    "            print(\"Upload files to s3 in completed\")\n",
    "                \n",
    "        if (('NER' in Each_Model['model_type']) and (Each_Model['CCFlag'] == 'True')):\n",
    "            model_path = os.path.join(BASE_PATH, Each_Model['model_id'])\n",
    "            print(\"model_path...\",model_path)\n",
    "            print(\"model_type\",\"NER\")\n",
    "            print(\"CCFlag True loop\")\n",
    "            word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "            label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "            case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "            char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "            valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "            svc_clf_file_name = '%s_content_classification_model.pkl' % Each_Model['model_id']\n",
    "            tf_idf_model_file_name = '%s_content_classification_tf_idf_model.pkl' % Each_Model['model_id']\n",
    "            svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "            tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "            print(\"svc_clf...\",model_path+'/'+svc_clf_file_name)\n",
    "            print(\"tf_idf_model...\",model_path+'/'+tf_idf_model_file_name)\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner_CC, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Uploading the model files to s3\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(BASE_PATH+Each_Model['model_id'])\n",
    "            print(\"list of files uploading to s3\", files_list)\n",
    "            print(\"Model id folder \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:                \n",
    "                try:\n",
    "                    upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "                except:\n",
    "                    print(\"file NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists('/data/EEF_models/'+Folder_name):\n",
    "                    os.makedirs('/data/EEF_models/'+Folder_name)\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log','/data/EEF_models/'+Folder_name)\n",
    "\n",
    "            for i in files_list:\n",
    "                file_chk = BASE_PATH+Folder_name+'/'+i\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    print(\"uploading file \",i)\n",
    "                    print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        upload_file(BASE_PATH+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "                        print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(BASE_PATH+Folder_name+'/'+i,'/data/EEF_models/'+Folder_name)\n",
    "                        print(\"model copied to /data PV\")\n",
    "                        \n",
    "            print(\"Upload files to s3 in completed\")\n",
    "                \n",
    "                \n",
    "        if 'DC' in Each_Model['model_type']:\n",
    "            model_path = os.path.join(BASE_PATH, Each_Model['model_id'])\n",
    "            print(\"model_path...\",model_path)\n",
    "            print(\"model_type\",\"DC\")\n",
    "            svc_clf_file_name = '%s_document_classification_svc_model.pkl' % Each_Model['model_id']\n",
    "            tf_idf_model_file_name = '%s_document_classification_tf_idf_model.pkl' % Each_Model['model_id']\n",
    "            svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "            tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "            print(\"svc_clf...\",model_path+'/'+svc_clf_file_name)\n",
    "            print(\"tf_idf_model...\",model_path+'/'+tf_idf_model_file_name)\n",
    "            model = tf_idf_model\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_Doc_classification, flavour = MLModelFlavours.sklearn, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Uploading the model files to s3\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(BASE_PATH+Each_Model['model_id'])\n",
    "            print(\"list of files uploading to s3\", files_list)\n",
    "            print(\"Model id folder \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "                except:\n",
    "                    print(\"file NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists('/data/EEF_models/'+Folder_name):\n",
    "                    os.makedirs('/data/EEF_models/'+Folder_name)\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log','/data/EEF_models/'+Folder_name)\n",
    "                \n",
    "            for i in files_list:\n",
    "                file_chk = BASE_PATH+Folder_name+'/'+i\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    print(\"uploading file \",i)\n",
    "                    print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        upload_file(BASE_PATH+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "                        print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(BASE_PATH+Folder_name+'/'+i,'/data/EEF_models/'+Folder_name)\n",
    "                        print(\"model copied to /data PV\")                        \n",
    "\n",
    "            print(\"Upload files to s3 in completed\")\n",
    "\n",
    "        if 'IC' in Each_Model['model_type']:\n",
    "            print(\"In IC ensemble loop\")\n",
    "            model_path = os.path.join(BASE_PATH, Each_Model['model_id'])\n",
    "            print(\"model_path...\",model_path)\n",
    "            # print\n",
    "            model_file_path = model_path+'/model_complete.h5'\n",
    "            print(\"model_file_path...............\",model_file_path)\n",
    "            print(\" image_classes_encoder file check \",os.listdir(model_path))\n",
    "            class_encoder = pkl.load(open(model_path+'/'+'image_classes_encoder.pkl','rb'))\n",
    "            print(\"class_encoder...\",model_path+'/'+'image_classes_encoder.pkl')\n",
    "            model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "            print(\"loaded IC model\")\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]            \n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_IC, flavour = MLModelFlavours.keras, init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\"  , schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Image Classification UpdateModel Details..\", UpdateModel)\n",
    "            print(\"Uploading the model files to s3\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(BASE_PATH+Each_Model['model_id'])\n",
    "            print(\"list of files uploading to s3\", files_list)\n",
    "            print(\"Model id folder \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "                except:\n",
    "                    print(\"file NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists('/data/EEF_models/'+Folder_name):\n",
    "                    os.makedirs('/data/EEF_models/'+Folder_name)\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log','/data/EEF_models/'+Folder_name)\n",
    "                \n",
    "            for i in files_list:\n",
    "                file_chk = BASE_PATH+Folder_name+'/'+i\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    print(\"uploading file \",i)\n",
    "                    print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        upload_file(BASE_PATH+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "                        print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(BASE_PATH+Folder_name+'/'+i,'/data/EEF_models/'+Folder_name)\n",
    "                        print(\"model copied to /data PV\")\n",
    "                        \n",
    "\n",
    "            print(\"Upload files to s3 in completed\")\n",
    "            \n",
    "        if 'Object Identification' in Each_Model['model_type']:\n",
    "            model_path = os.path.join(BASE_PATH, Each_Model['model_id'])\n",
    "            print(\"model_path...\",model_path)\n",
    "            model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "            print(\"model_file_path...............\",model_file_path)\n",
    "            zoning_file_label_path = model_path+'/'+'label_map.pbtxt'\n",
    "            label_map = label_map_util.load_labelmap(zoning_file_label_path)\n",
    "            num_classes = len(label_map.item)\n",
    "            categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "            category_index = label_map_util.create_category_index(categories)\n",
    "            print(\"zoning_file_label_path...\",zoning_file_label_path)\n",
    "            print(\"category_index...\",category_index)            \n",
    "            model = get_zoning_model(model_file_path)\n",
    "            print(\"loaded Object Identification model\")\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]            \n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_zoning_inference, flavour = MLModelFlavours.tensorflow, init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\" , schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Object Identification UpdateModel Details..\", UpdateModel)\n",
    "            print(\"Uploading the model files to s3\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(BASE_PATH+Each_Model['model_id'])\n",
    "            print(\"list of files uploading to s3\", files_list)\n",
    "            print(\"Model id folder \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "                except:\n",
    "                    print(\"file NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists('/data/EEF_models/'+Folder_name):\n",
    "                    os.makedirs('/data/EEF_models/'+Folder_name)\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log','/data/EEF_models/'+Folder_name)\n",
    "                \n",
    "            for i in files_list:\n",
    "                file_chk = BASE_PATH+Folder_name+'/'+i\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    print(\"uploading file \",i)\n",
    "                    print(\"Uploading file to S3 in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        upload_file(BASE_PATH+Folder_name+'/'+i,bucket.name,'EEF_models/'+Folder_name)\n",
    "                        print(\"---Time require to upload model files in S3 bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(BASE_PATH+Folder_name+'/'+i,'/data/EEF_models/'+Folder_name)\n",
    "                        print(\"model copied to /data PV\")\n",
    "\n",
    "            print(\"Upload files to s3 in completed\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed for arch2\n",
    "# from base import SQLAlchemyDBConnection\n",
    "# from EEF_NER_LSTM_Training.database.models.models import Model\n",
    "# from sqlalchemy import update, and_\n",
    "\n",
    "# def update_AIL_ModelDetails(model_id, _model_id,_version_id):\n",
    "#     \"\"\"\n",
    "#     Update status of a particular running model.\n",
    "\n",
    "#     Arguments:\n",
    "#         model_id - Model id of the model whose status needs to updated\n",
    "#         status  - status which needs to be updated\n",
    "\n",
    "#     Returns :\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     with SQLAlchemyDBConnection() as db:\n",
    "#         update_statement = update(Model).where(\n",
    "#             Model.id == model_id).values(\n",
    "#             ail_model_id=_model_id,ail_version_id=_version_id)\n",
    "#         db.session.execute(update_statement)\n",
    "#         db.session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model details in model table\n",
    "#update_AIL_ModelDetails('439','a1','b1')\n",
    "# update_AIL_ModelDetails(TrainModel['model_id'],model_details[0],model_details[1]) # removed for arch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model deployement\n",
    "# model_deploy=deploy_model(model_details[0],model_details[1],'07566385-70ee-47c9-8f3b-3bd4bf19384c')\n",
    "# print(model_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_deploy['deployment_info']['host_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from base import SQLAlchemyDBConnection\n",
    "# def get_data():\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Update status of a particular running model.\n",
    "\n",
    "#     Arguments:\n",
    "#         model_id - Model id of the model whose status needs to updated\n",
    "#         status  - status which needs to be updated\n",
    "\n",
    "#     Returns :\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     with SQLAlchemyDBConnection() as db:\n",
    "#         update_statement = \"select * from model order by id desc LIMIT 10\"\n",
    "#         output = db.session.execute(update_statement)\n",
    "#         return output\n",
    "# a= get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to update serving image\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(non_sso_user, non_sso_password):\n",
    "    \"\"\"To get access token\"\"\"\n",
    "    \n",
    "    password = hashlib.md5(non_sso_password.encode('utf-8')).hexdigest()\n",
    "    username = non_sso_user\n",
    "    request_url = \"https://keycloak-qa.lti-mosaic.com/auth/realms/mosaic/protocol/openid-connect/token\"\n",
    "    params = [\n",
    "    (\"grant_type\", \"password\"), (\"client_id\", \"keycloak-gatekeeper\"),\n",
    "        (\"client_secret\", \"62419864-bd9b-4d34-a235-f63fc8bedb95\"),(\"username\", f\"{username}\"),\n",
    "        (\"password\", f\"{password}\")]\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    response = requests.post(request_url, data=params, headers=headers, verify=False)\n",
    "    print(\"Access Response: \",response.status_code)\n",
    "    return response.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_serving_image(env, docker_image_url, project_id, model_id, version_id):\n",
    "    \"\"\"\"send put request to url with image.\"\"\"\n",
    "    \"\"\"\n",
    "    env : can be QA /GA /Cloud\n",
    "    docker_image_url = keras or pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    access_token = get_access_token(non_sso_user, non_sso_password)\n",
    "    \n",
    "    payload = {\"docker_image_url\":docker_image_url}\n",
    "    req_header = {\"X-Project-Id\": project_id, \"Content-Type\": \"application/json\", \n",
    "                  \"Accept\":\"application/json\",\"Authorization\":\"Bearer %s\" %access_token} \n",
    "    \n",
    "    api = \"/registry/api/v1/ml-model/{model_id}/version/{version_id}\"\n",
    "    \n",
    "    \n",
    "    env_url = env.lower() + api\n",
    "    url = env_url.format(model_id=model_id,version_id=version_id)\n",
    "    print(url)\n",
    "\n",
    "    req_response = requests.get(url, data=json.dumps(payload), headers=req_header, verify = False)\n",
    "    \n",
    "    return req_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_serving_image(env, docker_image_url, project_id, model_id, version_id):\n",
    "    \"\"\"\"send put request to url with image.\"\"\"\n",
    "    \"\"\"\n",
    "    env : can be QA /GA /Cloud\n",
    "    docker_image_url = keras or pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    access_token = get_access_token(non_sso_user, non_sso_password)\n",
    "    \n",
    "    payload = {\"docker_image_url\":docker_image_url}\n",
    "    req_header = {\"X-Project-Id\": project_id, \"Content-Type\": \"application/json\", \n",
    "                  \"Accept\":\"application/json\",\"Authorization\":\"Bearer %s\" %access_token} \n",
    "    \n",
    "    api = \"/registry/api/v1/ml-model/{model_id}/version/{version_id}\"\n",
    "    \n",
    "    \n",
    "    env_url = env.lower() + api\n",
    "    url = env_url.format(model_id=model_id,version_id=version_id)\n",
    "    print(url)\n",
    "\n",
    "    req_response = requests.put(url, data=json.dumps(payload), headers=req_header, verify = False)\n",
    "    \n",
    "    return req_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_old_models(env, docker_image_url, project_id, model_id, version_id):\n",
    "    \"\"\"\"send put request to url for old models to update image and removes the init_script\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    env : can be QA /GA /Cloud\n",
    "    docker_image_url = keras or pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    access_token = get_access_token(non_sso_user, non_sso_password)\n",
    "    \n",
    "    payload = {\"docker_image_url\":docker_image_url, \"init_script\": \"\"}\n",
    "    \n",
    "    req_header = {\"X-Project-Id\": project_id, \"Content-Type\": \"application/json\", \n",
    "                  \"Accept\":\"application/json\",\"Authorization\":\"Bearer %s\" %access_token} \n",
    "    \n",
    "    api = \"/registry/api/v1/ml-model/{model_id}/version/{version_id}\"\n",
    "    \n",
    "    \n",
    "    env_url = env.lower() + api\n",
    "    url = env_url.format(model_id=model_id,version_id=version_id)\n",
    "    print(url)\n",
    "\n",
    "    req_response = requests.put(url, data=json.dumps(payload), headers=req_header, verify = False)\n",
    "    \n",
    "    return req_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasks_to_perform(url, image, project_id, model_id, version_id, old_model=False): \n",
    "    \"\"\"\n",
    "    If we have old model which was not trained on localhost, we can use old_model = True. \n",
    "    This will delete the init script table. As we are fetching the current model version by current serving,\n",
    "    the old init script will be printed which can be saved for reference by user.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not old_model:\n",
    "        \n",
    "        \"\"\"Fetching current to have backup of script displayed\"\"\"\n",
    "        current_serving_resp = get_current_serving_image(url, image, project_id, model_id, version_id)\n",
    "        print(\"current_serving_resp\",current_serving_resp, type(current_serving_resp))\n",
    "        current_serving_resp_json =  json.loads(current_serving_resp.text)\n",
    "        for key in current_serving_resp_json:\n",
    "            if key in [\"name\", \"id\", \"ml_model_id\", \"current_version\",\"docker_image_url\",\"init_script\"]:\n",
    "                print(key,\" : \",current_serving_resp_json[key])\n",
    "                print()\n",
    "\n",
    "        resp = update_serving_image(url, image, project_id, model_id, version_id)\n",
    "        print(resp)\n",
    "\n",
    "\n",
    "    elif old_model:\n",
    "\n",
    "        \"\"\"Fetching current to have backup of script displayed\"\"\"\n",
    "        current_serving_resp = get_current_serving_image(url, image, project_id, model_id, version_id)\n",
    "        print(current_serving_resp)\n",
    "        current_serving_resp_json =  json.loads(current_serving_resp.text)\n",
    "        for key in current_serving_resp_json:\n",
    "            if key in [\"name\", \"id\", \"ml_model_id\", \"current_version\",\"docker_image_url\",\"init_script\"]:\n",
    "                print(key,\" : \",current_serving_resp_json[key])\n",
    "                print()\n",
    "\n",
    "        resp = update_old_models(url, image, project_id, model_id, version_id)\n",
    "        print(resp)\n",
    "        \n",
    "    return resp, current_serving_resp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_project_id=\"9968fe47-6cd0-47b1-aab8-bdb4b091c116\"\n",
    "\n",
    "# qa=\"https://mosaic-qa.lti-mosaic.com\"\n",
    "# ga=\"https://livmosaic.lntinfotech.com\"\n",
    "# cloud=\"https://mosaiccloud.lntinfotech.com\"\n",
    "\n",
    "# non_sso_user=\"sanchit\"\n",
    "# non_sso_password=\"123456\"\n",
    "# keras=\"registry.lti-aiq.in:443/mosaic-ai-logistics/mosaic-ai-serving:1.0.0-10112021_uWSGI8_T1V1-EEF-Static\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"46d7edc4-f811-41d7-96f6-61fed463fb4a\"\n",
    "url = \"https://livmosaic.lntinfotech.com\"\n",
    "non_sso_user=\"sanchit\"\n",
    "non_sso_password=\"123456\"\n",
    "keras=\"registry.lti-aiq.in:443/mosaic-ai-logistics/mosaic-ai-serving:1.0.0-10112021_uWSGI8_T1V1-EEF-Static\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_id  46d7edc4-f811-41d7-96f6-61fed463fb4a  url  https://livmosaic.lntinfotech.com\n"
     ]
    }
   ],
   "source": [
    "CONFIG = app_util.get_system_config()[1]\n",
    "project_id = CONFIG['project_id']\n",
    "url = CONFIG['url']\n",
    "non_sso_user = CONFIG['non_sso_user']\n",
    "non_sso_password = CONFIG['non_sso_password']\n",
    "keras = CONFIG['keras']\n",
    "print(\"project_id \",project_id , \" url \", url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainModel={'solution_id': 846, 'files': ['72690', '72689', '72687', '72688'], 'flag': 'ZONING', 'model_id': 'edcb60d6-fed3-4de5-b15f-753f60c0db2b__be8c3091-621f-4a42-b833-0decd37df660'}\n",
    "ml_model_id = TrainModel['model_id'].split(\"__\")[0]\n",
    "version_id = TrainModel['model_id'].split(\"__\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edcb60d6-fed3-4de5-b15f-753f60c0db2b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be8c3091-621f-4a42-b833-0decd37df660'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'keycloak-qa.lti-mosaic.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Response:  200\n",
      "https://livmosaic.lntinfotech.com/registry/api/v1/ml-model/edcb60d6-fed3-4de5-b15f-753f60c0db2b/version/be8c3091-621f-4a42-b833-0decd37df660\n",
      "current_serving_resp <Response [401]> <class 'requests.models.Response'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/urllib3/connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'livmosaic.lntinfotech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-176f23ef573b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_serving_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks_to_perform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-46d37e26092f>\u001b[0m in \u001b[0;36mtasks_to_perform\u001b[0;34m(url, image, project_id, model_id, version_id, old_model)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcurrent_serving_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_serving_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current_serving_resp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_serving_resp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_serving_resp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcurrent_serving_resp_json\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_serving_resp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_serving_resp_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ml_model_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_version\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"docker_image_url\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"init_script\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "update_response, old_serving_json = tasks_to_perform(url, keras,project_id, ml_model_id, version_id, old_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)==0:\n",
    "    ml_model_id = TrainModel['model_id'].split(\"__\")[0]\n",
    "    version_id = TrainModel['model_id'].split(\"__\")[1]\n",
    "    update_response, old_serving_json = tasks_to_perform(url, keras,project_id, ml_model_id, version_id, old_model=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)>0:\n",
    "    for Each_Model in Ensemble_ModelDetails:\n",
    "        ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "        version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "        update_response, old_serving_json = tasks_to_perform(url, keras,project_id, ml_model_id, version_id, old_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for models in list_models:\n",
    "    \n",
    "#     print(models)\n",
    "#     print()\n",
    "    \n",
    "#     update_response, old_serving_json = tasks_to_perform(url, keras, \n",
    "#                                                      project_id, models[\"model_id\"], models[\"version_id\"], old_model=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
